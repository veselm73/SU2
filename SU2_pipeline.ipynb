{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bfb1ea6e",
      "metadata": {},
      "source": [
        "\n",
        "# SU2 Full Pipeline (Synthetic Training + Tracking Sweep)\n",
        "\n",
        "This notebook configures the SU2 workspace, generates synthetic CCP data, trains the U-Net++ model, and evaluates tracking via a HOTA sweep on validation sequences. It is ready for local Jupyter, VS Code, or a remote (e.g., Colab) kernel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a57ecf3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Workspace setup (paths + folders)\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "repo_root_env = os.environ.get('SU2_REPO_ROOT')\n",
        "default_root = Path(repo_root_env).expanduser().resolve() if repo_root_env else Path.cwd().resolve()\n",
        "repo_root = default_root\n",
        "\n",
        "if not (repo_root / 'modules').exists():\n",
        "    if IS_COLAB:\n",
        "        colab_root = Path('/content/SU2').resolve()\n",
        "        if not colab_root.exists():\n",
        "            repo_url = os.environ.get('SU2_REPO_URL', 'https://github.com/veselm73/SU2.git')\n",
        "            print(f\"Cloning repository into {colab_root}...\")\n",
        "            subprocess.run(['git', 'clone', repo_url, str(colab_root)], check=True)\n",
        "        repo_root = colab_root\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not locate the SU2 repository. Set SU2_REPO_ROOT to the repo path or run inside the repo root.\"\n",
        "        )\n",
        "\n",
        "REPO_ROOT = repo_root\n",
        "\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "os.chdir(REPO_ROOT)\n",
        "\n",
        "SAVE_DIR = Path(os.environ.get('SU2_SAVE_DIR', REPO_ROOT / 'artifacts'))\n",
        "CHECKPOINT_DIR = Path(os.environ.get('SU2_CHECKPOINT_DIR', REPO_ROOT / 'checkpoints'))\n",
        "VAL_DATA_DIR = Path(os.environ.get('SU2_VAL_DATA_DIR', REPO_ROOT / 'val_data'))\n",
        "CHAIN_PATH = Path(os.environ.get('SU2_CERT_CHAIN', REPO_ROOT / 'chain-harica-cross.pem'))\n",
        "\n",
        "for path_obj in (SAVE_DIR, CHECKPOINT_DIR, VAL_DATA_DIR):\n",
        "    path_obj.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Repository root: {REPO_ROOT}\")\n",
        "print(f\"Artifacts directory: {SAVE_DIR}\")\n",
        "print(f\"Checkpoints directory: {CHECKPOINT_DIR}\")\n",
        "print(f\"Validation data directory: {VAL_DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e7cf1c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Pipeline configuration (write config.yaml before importing modules)\n",
        "import yaml\n",
        "\n",
        "PIPELINE_CONFIG = {\n",
        "    \"TRAIN_SAMPLES\": 4000,\n",
        "    \"VAL_SAMPLES\": 600,\n",
        "    \"BATCH_SIZE\": 16,\n",
        "    \"LEARNING_RATE\": 5e-4,\n",
        "    \"WEIGHT_DECAY\": 1e-4,\n",
        "    \"DROPOUT_RATE\": 0.15,\n",
        "    \"EPOCHS\": 200,\n",
        "    \"PATIENCE\": 20,\n",
        "    \"SEED\": 73,\n",
        "    \"DETECTION_MODEL\": \"unet\",\n",
        "    \"SAM3_CHECKPOINT\": str(CHECKPOINT_DIR / \"sam3_hiera_large.pt\"),\n",
        "    \"SAM3_MODEL_TYPE\": \"vit_l\",\n",
        "    \"SKIP_TRAINING\": False,\n",
        "    \"MIN_CELLS\": 8,\n",
        "    \"MAX_CELLS\": 24,\n",
        "    \"PATCH_SIZE\": 128,\n",
        "    \"SIM_CONFIG\": {\n",
        "        \"na\": 1.49,\n",
        "        \"wavelength\": 512,\n",
        "        \"px_size\": 0.07,\n",
        "        \"wiener_parameter\": 0.1,\n",
        "        \"apo_cutoff\": 2.0,\n",
        "        \"apo_bend\": 0.9,\n",
        "    },\n",
        "}\n",
        "\n",
        "with open('config.yaml', 'w', encoding='utf-8') as f:\n",
        "    yaml.safe_dump(PIPELINE_CONFIG, f, sort_keys=False)\n",
        "\n",
        "print(\"config.yaml updated. Key parameters:\")\n",
        "for key in (\"TRAIN_SAMPLES\", \"VAL_SAMPLES\", \"BATCH_SIZE\", \"LEARNING_RATE\", \"EPOCHS\", \"PATIENCE\"):\n",
        "    print(f\"  {key}: {PIPELINE_CONFIG[key]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "129e812c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Imports (after config is written)\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from modules.config import *  # noqa: F401,F403\n",
        "from modules.dataset import SyntheticCCPDataset\n",
        "from modules.training import train_unet_pipeline\n",
        "from modules.utils import set_seed, plot_training_history, open_tiff_file, download_and_unzip\n",
        "from modules.sweep import sweep_and_save_gif\n",
        "from modules.tracking import DetectionParams, BTrackParams\n",
        "\n",
        "set_seed(SEED)\n",
        "print(\"Imports ready. Random seed initialized.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc25d9dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Download validation data if missing\n",
        "import requests\n",
        "\n",
        "val_tif_path = VAL_DATA_DIR / 'val.tif'\n",
        "if val_tif_path.exists():\n",
        "    print(f\"Validation TIFF already present at {val_tif_path}\")\n",
        "else:\n",
        "    print(\"Downloading SSL certificate chain...\")\n",
        "    cert_url = 'https://pki.cesnet.cz/_media/certs/chain-harica-rsa-ov-crosssigned-root.pem'\n",
        "    response = requests.get(cert_url, timeout=10, stream=True)\n",
        "    response.raise_for_status()\n",
        "    CHAIN_PATH.write_bytes(response.content)\n",
        "    print(\"Certificate ready. Fetching validation archive...\")\n",
        "    zip_url = 'https://su2.utia.cas.cz/files/labs/final2025/val_and_sota.zip'\n",
        "    download_and_unzip(zip_url, str(VAL_DATA_DIR), str(CHAIN_PATH))\n",
        "    print(\"Validation data downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e92477",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Synthetic vs real validation data preview\n",
        "synthetic_dataset = SyntheticCCPDataset(\n",
        "    min_n=PIPELINE_CONFIG['MIN_CELLS'],\n",
        "    max_n=PIPELINE_CONFIG['MAX_CELLS'],\n",
        "    patch_size=PIPELINE_CONFIG['PATCH_SIZE'],\n",
        "    sim_config=PIPELINE_CONFIG['SIM_CONFIG'],\n",
        ")\n",
        "\n",
        "synth_img, synth_mask = synthetic_dataset.data_sample()\n",
        "val_tif = VAL_DATA_DIR / 'val.tif'\n",
        "val_img = None\n",
        "if val_tif.exists():\n",
        "    val_stack = open_tiff_file(str(val_tif))\n",
        "    val_img = val_stack[0]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "axes[0].imshow(synth_img, cmap='gray')\n",
        "axes[0].set_title('Synthetic CCP Image')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(synth_mask, cmap='gray')\n",
        "axes[1].set_title('Synthetic Mask')\n",
        "axes[1].axis('off')\n",
        "if val_img is not None:\n",
        "    axes[2].imshow(val_img, cmap='gray')\n",
        "    axes[2].set_title('Validation Frame (val.tif)')\n",
        "else:\n",
        "    axes[2].text(0.5, 0.5, 'Validation TIFF missing', ha='center', va='center')\n",
        "    axes[2].set_axis_off()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb686323",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Train U-Net++ on synthetic data\n",
        "training_kwargs = dict(\n",
        "    train_samples=PIPELINE_CONFIG['TRAIN_SAMPLES'],\n",
        "    val_samples=PIPELINE_CONFIG['VAL_SAMPLES'],\n",
        "    epochs=PIPELINE_CONFIG['EPOCHS'],\n",
        "    batch_size=PIPELINE_CONFIG['BATCH_SIZE'],\n",
        "    learning_rate=PIPELINE_CONFIG['LEARNING_RATE'],\n",
        "    weight_decay=PIPELINE_CONFIG['WEIGHT_DECAY'],\n",
        "    patience=PIPELINE_CONFIG['PATIENCE'],\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "model, history = train_unet_pipeline(**training_kwargs)\n",
        "print('Training finished.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "105ca82d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Persist best epoch by validation Dice\n",
        "if history['val_dice']:\n",
        "    best_epoch = int(np.argmax(history['val_dice'])) + 1\n",
        "    checkpoint_path = Path(f'checkpoint_epoch_{best_epoch}.pth')\n",
        "    best_dice_path = SAVE_DIR / 'best_val_dice_model.pth'\n",
        "    if checkpoint_path.exists():\n",
        "        shutil.copy(checkpoint_path, best_dice_path)\n",
        "        print(f'Best validation Dice epoch: {best_epoch} (score={history[\"val_dice\"][best_epoch-1]:.4f})')\n",
        "        print(f'Checkpoint copied to {best_dice_path}')\n",
        "    else:\n",
        "        print(f'Checkpoint {checkpoint_path} not found; cannot copy best Dice weights.')\n",
        "else:\n",
        "    print('History missing val_dice information.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e503e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Plot training curves\n",
        "plot_training_history(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e495bbc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Save final model snapshot\n",
        "if model is not None:\n",
        "    final_path = SAVE_DIR / 'final_model.pth'\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "    print(f'Final model saved to {final_path}')\n",
        "else:\n",
        "    print('Model object is None; skipping save.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09b97966",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define detection/tracking search spaces\n",
        "from modules.tracking import DetectionParams, BTrackParams\n",
        "\n",
        "det_param_grid = {\n",
        "    'threshold': [0.25, 0.3, 0.35],\n",
        "    'min_area': [3, 5],\n",
        "    'nms_min_dist': [3.0, 4.0],\n",
        "}\n",
        "\n",
        "btrack_param_grid = {\n",
        "    'do_optimize': [False],\n",
        "    'max_search_radius': [20.0, 25.0],\n",
        "    'dist_thresh': [12.0, 15.0],\n",
        "    'time_thresh': [4, 6],\n",
        "    'min_track_len': [8, 12],\n",
        "    'segmentation_miss_rate': [0.1],\n",
        "    'apoptosis_rate': [0.001],\n",
        "    'allow_divisions': [False],\n",
        "}\n",
        "print('Parameter grids prepared.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6a0f683",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Run HOTA sweep + GIF export\n",
        "val_tif_path = (VAL_DATA_DIR / 'val.tif').resolve()\n",
        "gif_output_path = SAVE_DIR / 'best_tracking.gif'\n",
        "\n",
        "best_det, best_bt, best_tracks = sweep_and_save_gif(\n",
        "    model,\n",
        "    det_param_grid,\n",
        "    btrack_param_grid,\n",
        "    gif_output=str(gif_output_path),\n",
        "    val_tif_path=str(val_tif_path)\n",
        ")\n",
        "\n",
        "print('Best detection params:', best_det)\n",
        "print('Best tracking params:', best_bt)\n",
        "print(f'GIF stored at: {gif_output_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba5cd688",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Display best tracking GIF\n",
        "from IPython.display import Image\n",
        "\n",
        "gif_path = SAVE_DIR / 'best_tracking.gif'\n",
        "if gif_path.exists():\n",
        "    display(Image(filename=str(gif_path)))\n",
        "else:\n",
        "    print('best_tracking.gif not found.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}