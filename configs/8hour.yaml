# Optimized Configuration for 8-hour T4 GPU Training
# Target: Improve validation Dice score beyond 0.54
# 
# Strategy:
# 1. Increase training samples for better generalization
# 2. Reduce batch size for more gradient updates
# 3. Lower initial learning rate with longer patience
# 4. More epochs to fully converge

TRAIN_SAMPLES: 5000        # Increased from 2000 (more diverse training data)
VAL_SAMPLES: 800           # Increased from 400 (better validation estimate)
BATCH_SIZE: 16             # Reduced from 32 (more frequent updates, fits T4 memory)
LEARNING_RATE: 5e-4        # Reduced from 1e-3 (more stable convergence)
WEIGHT_DECAY: 1e-4         # Keep same (good regularization)
DROPOUT_RATE: 0.15         # Increased from 0.1 (prevent overfitting)
EPOCHS: 300                # Increased from 200 (allow full convergence)
PATIENCE: 15               # Increased from 10 (don't stop too early)
SEED: 73

# Data Generator Configuration
MIN_CELLS: 8
MAX_CELLS: 24
RADIUS: 4.0
PATCH_SIZE: 128

# Detection Model Configuration
DETECTION_MODEL: "unet"  # Options: "unet", "sam3"
SAM3_CHECKPOINT: "checkpoints/sam3_hiera_large.pt" # Example checkpoint name
SAM3_MODEL_TYPE: "vit_l" # Example model type


# SIM Configuration
SIM_CONFIG:
  na: 1.49
  wavelength: 512
  px_size: 0.07
  wiener_parameter: 0.1
  apo_cutoff: 2.0
  apo_bend: 0.9

# ============================================================================
# PERFORMANCE OPTIMIZATION NOTES
# ============================================================================
#
# Time Estimate for 8 hours:
# - ~5000 samples / 16 batch = 313 batches per epoch
# - ~1.5 min per epoch on T4 = ~300 epochs in 7.5 hours
#
# Why these changes help with Dice score:
#
# 1. More Training Samples (5000):
#    - Better coverage of cell configurations
#    - Reduces overfitting to limited patterns
#    - Target: +5-10% Dice improvement
#
# 2. Smaller Batch Size (16):
#    - More gradient updates per epoch
#    - Better escape from local minima
#    - Noisier gradients = better generalization
#
# 3. Lower Learning Rate (5e-4):
#    - More careful optimization
#    - Better fine-tuning of weights
#    - Reduces oscillation near optimum
#
# 4. Higher Dropout (0.15):
#    - Stronger regularization
#    - Forces model to learn robust features
#    - Prevents memorization
#
# 5. More Patience (15):
#    - Allows LR scheduler more time to work
#    - Prevents premature stopping
#    - Model can recover from plateaus
#
# Expected Dice Score: 0.65-0.75 (up from 0.54)
#
# ============================================================================
