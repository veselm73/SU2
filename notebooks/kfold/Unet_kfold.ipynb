{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Colab/Local Setup (core deps)\n",
    "# Torch is preinstalled on Colab; install extras quietly.\n",
    "# Install btrack without resolver to avoid conflicts, then compatible deps.\n",
    "!pip install -q --no-deps btrack==0.6.5\n",
    "!pip install -q numpy==1.23.5 numba==0.57.1 scipy==1.10.1 dill==0.3.7     albumentations==1.4.14 opencv-python-headless==4.10.0.84     tifffile==2024.8.30 scikit-image==0.24.0 scikit-learn==1.5.2     pandas==2.2.2 tqdm==4.66.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2097722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports & Globals\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from skimage.measure import label, regionprops\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# ROI constants\n",
    "ROI_Y_MIN, ROI_Y_MAX = 512, 768\n",
    "ROI_X_MIN, ROI_X_MAX = 256, 512\n",
    "ROI_H, ROI_W = ROI_Y_MAX - ROI_Y_MIN, ROI_X_MAX - ROI_X_MIN\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ba996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data Download Helpers (validation + optional Drive training)\n",
    "import os, shutil, zipfile, requests\n",
    "\n",
    "# Download validation video+labels zip (with SSL chain)\n",
    "def download_validation_data(target_dir: str = \"val_data\",\n",
    "                             url: str = \"https://su2.utia.cas.cz/files/labs/final2025/val_and_sota.zip\",\n",
    "                             cert_url: str = \"https://pki.cesnet.cz/_media/certs/chain-harica-rsa-ov-crosssigned-root.pem\"):\n",
    "    if os.path.exists(target_dir) and len(os.listdir(target_dir)) > 0:\n",
    "        print(f\"'{target_dir}' already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    chain_path = \"chain-harica-cross.pem\"\n",
    "    print(\"1) Downloading SSL certificate chain...\")\n",
    "    r = requests.get(cert_url, timeout=10, stream=True)\n",
    "    r.raise_for_status()\n",
    "    with open(chain_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(\"2) Downloading validation archive...\")\n",
    "    zip_name = os.path.basename(url)\n",
    "    with requests.get(url, stream=True, verify=chain_path, timeout=30) as resp:\n",
    "        resp.raise_for_status()\n",
    "        with open(zip_name, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    print(\"3) Extracting...\")\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_name, \"r\") as zf:\n",
    "        zf.extractall(target_dir)\n",
    "    os.remove(zip_name)\n",
    "    print(f\"Done. Data in '{target_dir}/'\")\n",
    "\n",
    "\n",
    "# Optional: copy training data from Google Drive (manual annotations)\n",
    "def fetch_from_drive(DRIVE_SOURCE_PATH: str = \"/content/drive/MyDrive/unet_train\",\n",
    "                     TARGET_DIR: str = \"real_training_data\"):\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "    except ImportError:\n",
    "        print(\"google.colab not available (not running in Colab).\")\n",
    "        return\n",
    "\n",
    "    print(\"?? Mounting Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    if os.path.exists(TARGET_DIR) and len(os.listdir(TARGET_DIR)) > 0:\n",
    "        print(f\"'{TARGET_DIR}' already exists and is not empty. Skipping copy.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(DRIVE_SOURCE_PATH):\n",
    "        print(f\"? Source not found: {DRIVE_SOURCE_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Copying {DRIVE_SOURCE_PATH} -> {TARGET_DIR}\")\n",
    "    try:\n",
    "        shutil.copytree(DRIVE_SOURCE_PATH, TARGET_DIR)\n",
    "        print(\"? Copy complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"? Copy failed: {e}\")\n",
    "        if os.path.exists(TARGET_DIR):\n",
    "            shutil.rmtree(TARGET_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Model & Loss (Lightweight U-Net)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class LightweightUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_classes=1, features=[16, 32, 64, 128]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        in_ch = in_channels\n",
    "        for feature in features:\n",
    "            self.downs.append(ConvBlock(in_ch, feature))\n",
    "            in_ch = feature\n",
    "\n",
    "        self.bottleneck = ConvBlock(features[-1], features[-1] * 2)\n",
    "\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(ConvBlock(feature * 2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = F.interpolate(x, size=skip_connection.shape[2:], mode=\"bilinear\", align_corners=True)\n",
    "            x = self.ups[idx + 1](torch.cat((skip_connection, x), dim=1))\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.focal = FocalLoss(alpha=0.75, gamma=2)\n",
    "\n",
    "    def dice_loss(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        smooth = 1.0\n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2.0 * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if isinstance(inputs, list):\n",
    "            loss = 0\n",
    "            for item in inputs:\n",
    "                loss += 0.5 * self.focal(item, targets) + 0.5 * self.dice_loss(item, targets)\n",
    "            return loss / len(inputs)\n",
    "        return 0.5 * self.focal(inputs, targets) + 0.5 * self.dice_loss(inputs, targets)\n",
    "\n",
    "# Quick sanity check\n",
    "_dummy = torch.randn(1, 1, 256, 256)\n",
    "_model = LightweightUNet()\n",
    "_out = _model(_dummy)\n",
    "print(f\"Model ok, output shape: {_out.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ba71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data prep: build experiment_dataset\n",
    "\n",
    "def prepare_grand_dataset(\n",
    "    real_data_dir: str = \"real_training_data\",\n",
    "    val_tif_path: str = \"val_data/val.tif\",\n",
    "    val_csv_path: str = \"val_data/val.csv\",\n",
    "    out_dir: str = \"experiment_dataset\",\n",
    "    roi_y: Tuple[int, int] = (ROI_Y_MIN, ROI_Y_MAX),\n",
    "    roi_x: Tuple[int, int] = (ROI_X_MIN, ROI_X_MAX),\n",
    ") -> Path:\n",
    "    out_path = Path(out_dir)\n",
    "    bonus_images = out_path / \"bonus\" / \"images\"\n",
    "    bonus_masks = out_path / \"bonus\" / \"masks\"\n",
    "    video_images = out_path / \"video\" / \"images\"\n",
    "    video_masks = out_path / \"video\" / \"masks\"\n",
    "\n",
    "    if out_path.exists():\n",
    "        shutil.rmtree(out_path)\n",
    "    for p in [bonus_images, bonus_masks, video_images, video_masks]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    src_images = Path(real_data_dir) / \"images\"\n",
    "    src_masks = Path(real_data_dir) / \"masks\"\n",
    "    if not src_images.exists() or not src_masks.exists():\n",
    "        raise FileNotFoundError(\"real_training_data must contain images/ and masks/\")\n",
    "    shutil.copytree(src_images, bonus_images, dirs_exist_ok=True)\n",
    "    shutil.copytree(src_masks, bonus_masks, dirs_exist_ok=True)\n",
    "\n",
    "    if not Path(val_tif_path).exists():\n",
    "        raise FileNotFoundError(f\"Missing video file: {val_tif_path}\")\n",
    "    if not Path(val_csv_path).exists():\n",
    "        raise FileNotFoundError(f\"Missing CSV file: {val_csv_path}\")\n",
    "\n",
    "    video = tifffile.imread(val_tif_path)\n",
    "    coords = pd.read_csv(val_csv_path)\n",
    "    y_min, y_max = roi_y\n",
    "    x_min, x_max = roi_x\n",
    "    records: List[dict] = []\n",
    "\n",
    "    for idx, frame in enumerate(video):\n",
    "        crop = frame[y_min:y_max, x_min:x_max]\n",
    "        mask = np.zeros((ROI_H, ROI_W), dtype=np.uint8)\n",
    "        points = coords[coords[\"frame\"] == idx]\n",
    "        for _, row in points.iterrows():\n",
    "            cx = int(round(row[\"x\"] - x_min))\n",
    "            cy = int(round(row[\"y\"] - y_min))\n",
    "            if 0 <= cx < ROI_W and 0 <= cy < ROI_H:\n",
    "                cv2.circle(mask, (cx, cy), 5, 1, -1)\n",
    "        fname = f\"frame_{idx + 1:03}.png\"\n",
    "        cv2.imwrite(str(video_images / fname), crop)\n",
    "        cv2.imwrite(str(video_masks / fname), mask * 255)\n",
    "        records.append({\"filename\": fname, \"real_frame_idx\": idx})\n",
    "\n",
    "    pd.DataFrame(records).to_csv(out_path / \"video_map.csv\", index=False)\n",
    "    print(f\"Dataset written to {out_path.resolve()}\")\n",
    "    print(f\"Bonus samples: {len(list(bonus_images.glob('*.png')))} | Video frames: {len(records)}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65810886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Transforms & Dataset\n",
    "\n",
    "def get_train_transforms(\n",
    "    rotate_p: float = 0.7,\n",
    "    hflip_p: float = 0.5,\n",
    "    vflip_p: float = 0.5,\n",
    "    clahe_p: float = 0.5,\n",
    "    brightness_p: float = 0.5,\n",
    "    gauss_p: float = 0.3,\n",
    "    elastic_p: float = 0.2,\n",
    "    coarse_p: float = 0.5,\n",
    "    coarse_max_holes: int = 16,\n",
    "    coarse_min_holes: int = 8,\n",
    "    coarse_max_hw: int = 16,\n",
    "    coarse_min_hw: int = 8,\n",
    "    crop_scale_min: float = 0.8,\n",
    "    crop_scale_max: float = 1.0,\n",
    "    crop_ratio_min: float = 0.9,\n",
    "    crop_ratio_max: float = 1.1,\n",
    "):\n",
    "    return A.Compose([\n",
    "        A.Rotate(limit=180, p=rotate_p),\n",
    "        A.HorizontalFlip(p=hflip_p),\n",
    "        A.VerticalFlip(p=vflip_p),\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=clahe_p),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=brightness_p),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=gauss_p),\n",
    "        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=elastic_p),\n",
    "        A.CoarseDropout(\n",
    "            max_holes=coarse_max_holes, max_height=coarse_max_hw, max_width=coarse_max_hw,\n",
    "            min_holes=coarse_min_holes, min_height=coarse_min_hw, min_width=coarse_min_hw,\n",
    "            fill_value=0, mask_fill_value=0, p=coarse_p,\n",
    "        ),\n",
    "        A.RandomResizedCrop(\n",
    "            size=(256, 256),\n",
    "            scale=(crop_scale_min, crop_scale_max),\n",
    "            ratio=(crop_ratio_min, crop_ratio_max),\n",
    "            p=0.5,\n",
    "        ),\n",
    "        A.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_transforms():\n",
    "    return A.Compose([ToTensorV2()])\n",
    "\n",
    "\n",
    "class AugmentedMicroscopyDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, transform=None, return_meta: bool = False):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.return_meta = return_meta\n",
    "        self.image_paths = sorted(self.root_dir.joinpath(\"images\").glob(\"*.png\"))\n",
    "        self.mask_paths = sorted(self.root_dir.joinpath(\"masks\").glob(\"*.png\"))\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "            raise ValueError(f\"Images and masks counts differ in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = cv2.imread(str(self.image_paths[idx]), cv2.IMREAD_UNCHANGED)\n",
    "        mask = cv2.imread(str(self.mask_paths[idx]), cv2.IMREAD_UNCHANGED)\n",
    "        if img is None or mask is None:\n",
    "            raise FileNotFoundError(f\"Missing pair for index {idx}\")\n",
    "        if img.ndim == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        if mask.ndim == 3:\n",
    "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        img = img.astype(np.float32)\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "        else:\n",
    "            img = torch.from_numpy(img).float().unsqueeze(0)\n",
    "            mask = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "\n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = torch.from_numpy(img)\n",
    "        if isinstance(mask, np.ndarray):\n",
    "            mask = torch.from_numpy(mask)\n",
    "\n",
    "        img = img.float()\n",
    "        mask = mask.float()\n",
    "        if img.max() > 1.0:\n",
    "            img = img / 255.0\n",
    "        if img.ndim == 2:\n",
    "            img = img.unsqueeze(0)\n",
    "        if mask.ndim == 2:\n",
    "            mask = mask.unsqueeze(0)\n",
    "\n",
    "        if self.return_meta:\n",
    "            return img, mask, {\"filename\": self.image_paths[idx].name, \"dataset_idx\": int(idx)}\n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b20946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training, inference, metrics, visualization\n",
    "\n",
    "def make_fold_loaders(video_root: Path, bonus_root: Path, train_idx, val_idx, batch_size=8, num_workers=2, use_bonus: bool = True, train_transform=None, val_transform=None):\n",
    "    train_tf = train_transform if train_transform is not None else get_train_transforms()\n",
    "    val_tf = val_transform if val_transform is not None else get_val_transforms()\n",
    "    train_ds = Subset(AugmentedMicroscopyDataset(video_root, transform=train_tf), train_idx)\n",
    "    val_ds = Subset(AugmentedMicroscopyDataset(video_root, transform=val_tf), val_idx)\n",
    "    datasets = [train_ds]\n",
    "    bonus_len = 0\n",
    "    if use_bonus:\n",
    "        bonus_ds = AugmentedMicroscopyDataset(bonus_root, transform=train_tf)\n",
    "        datasets.append(bonus_ds)\n",
    "        bonus_len = len(bonus_ds)\n",
    "    train_loader = DataLoader(ConcatDataset(datasets), batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=max(1, batch_size // 2), shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    print(f\"Train loader: video {len(train_idx)} + bonus {bonus_len} = {len(train_loader.dataset)}\")\n",
    "    print(f\"Val loader: {len(val_idx)}\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def train_one_fold(model, train_loader, val_loader, device, epochs=40, lr=1e-3):\n",
    "    import copy\n",
    "    criterion = ComboLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    best_val = float('inf')\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for imgs, masks in train_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                outputs = outputs[-1]\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * imgs.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                outputs = model(imgs)\n",
    "                if isinstance(outputs, (list, tuple)):\n",
    "                    outputs = outputs[-1]\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"Epoch {epoch:03d}: train {train_loss:.4f} | val {val_loss:.4f} | best {best_val:.4f}\")\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_val\n",
    "\n",
    "\n",
    "def infer_fold_oof(model, val_indices, video_root: Path, video_map: Path, device, threshold: float = 0.5):\n",
    "    infer_ds = Subset(AugmentedMicroscopyDataset(video_root, transform=get_val_transforms(), return_meta=True), val_indices)\n",
    "    infer_loader = DataLoader(infer_ds, batch_size=1, shuffle=False)\n",
    "    frame_lookup = pd.read_csv(video_map).set_index('filename')['real_frame_idx'].to_dict()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _, meta in infer_loader:\n",
    "            filenames = meta['filename'] if isinstance(meta, dict) else [m['filename'] for m in meta]\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                outputs = outputs[-1]\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            for b in range(probs.shape[0]):\n",
    "                bin_mask = (probs[b, 0] >= threshold).astype(np.uint8)\n",
    "                labeled = label(bin_mask)\n",
    "                for prop in regionprops(labeled):\n",
    "                    cy, cx = prop.centroid\n",
    "                    preds.append({'frame': int(frame_lookup[filenames[b]]), 'x': float(cx + ROI_X_MIN), 'y': float(cy + ROI_Y_MIN)})\n",
    "    return preds\n",
    "\n",
    "\n",
    "def _filter_roi_frames(df: pd.DataFrame, frames_filter=None, roi=None):\n",
    "    if frames_filter is not None:\n",
    "        frame_set = set(map(int, frames_filter))\n",
    "        df = df[df.frame.isin(frame_set)]\n",
    "    if roi is not None:\n",
    "        y_min, y_max, x_min, x_max = roi\n",
    "        df = df[(df.y >= y_min) & (df.y < y_max) & (df.x >= x_min) & (df.x < x_max)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def hota_metric(gt: pd.DataFrame, tr: pd.DataFrame, threshold: float = 5.0):\n",
    "    from scipy import spatial, optimize\n",
    "    gt = gt.copy(); tr = tr.copy()\n",
    "    gt.track_id = gt.track_id.map({old: new for old, new in zip(gt.track_id.unique(), range(gt.track_id.nunique()))})\n",
    "    tr.track_id = tr.track_id.map({old: new for old, new in zip(tr.track_id.unique(), range(tr.track_id.nunique()))})\n",
    "    num_gt_ids = gt.track_id.nunique(); num_tr_ids = tr.track_id.nunique()\n",
    "    frames = sorted(set(gt.frame.unique()) | set(tr.frame.unique()))\n",
    "    potential_matches_count = np.zeros((num_gt_ids, num_tr_ids))\n",
    "    gt_id_count = np.zeros((num_gt_ids, 1)); tracker_id_count = np.zeros((1, num_tr_ids))\n",
    "    HOTA_TP = HOTA_FN = HOTA_FP = 0; LocA = 0.0\n",
    "    similarities = [1 - np.clip(spatial.distance.cdist(gt[gt.frame == t][['x', 'y']], tr[tr.frame == t][['x', 'y']]) / threshold, 0, 1) for t in frames]\n",
    "    for t in frames:\n",
    "        gt_ids_t = gt[gt.frame == t].track_id.to_numpy(); tr_ids_t = tr[tr.frame == t].track_id.to_numpy()\n",
    "        similarity = similarities[t]\n",
    "        sim_iou_denom = similarity.sum(0)[np.newaxis, :] + similarity.sum(1)[:, np.newaxis] - similarity\n",
    "        sim_iou = np.zeros_like(similarity)\n",
    "        mask = sim_iou_denom > 0 + np.finfo('float').eps\n",
    "        sim_iou[mask] = similarity[mask] / sim_iou_denom[mask]\n",
    "        potential_matches_count[gt_ids_t[:, None], tr_ids_t[None, :]] += sim_iou\n",
    "        gt_id_count[gt_ids_t] += 1; tracker_id_count[0, tr_ids_t] += 1\n",
    "    global_alignment_score = potential_matches_count / (gt_id_count + tracker_id_count - potential_matches_count)\n",
    "    matches_count = np.zeros_like(potential_matches_count)\n",
    "    for t in frames:\n",
    "        gt_ids_t = gt[gt.frame == t].track_id.to_numpy(); tr_ids_t = tr[tr.frame == t].track_id.to_numpy()\n",
    "        if len(gt_ids_t) == 0:\n",
    "            HOTA_FP += len(tr_ids_t); continue\n",
    "        if len(tr_ids_t) == 0:\n",
    "            HOTA_FN += len(gt_ids_t); continue\n",
    "        similarity = similarities[t]\n",
    "        score_mat = global_alignment_score[gt_ids_t[:, None], tr_ids_t[None, :]] * similarity\n",
    "        match_rows, match_cols = optimize.linear_sum_assignment(-score_mat)\n",
    "        matched_mask = similarity[match_rows, match_cols] > 0\n",
    "        alpha_rows = match_rows[matched_mask]; alpha_cols = match_cols[matched_mask]\n",
    "        num_matches = len(alpha_rows)\n",
    "        HOTA_TP += num_matches\n",
    "        HOTA_FN += len(gt_ids_t) - num_matches\n",
    "        HOTA_FP += len(tr_ids_t) - num_matches\n",
    "        if num_matches > 0:\n",
    "            LocA += sum(similarity[alpha_rows, alpha_cols])\n",
    "            matches_count[gt_ids_t[alpha_rows], tr_ids_t[alpha_cols]] += 1\n",
    "    ass_a = matches_count / np.maximum(1, gt_id_count + tracker_id_count - matches_count)\n",
    "    AssA = np.sum(matches_count * ass_a) / np.maximum(1, HOTA_TP)\n",
    "    DetA = HOTA_TP / np.maximum(1, HOTA_TP + HOTA_FN + HOTA_FP)\n",
    "    HOTA = np.sqrt(DetA * AssA)\n",
    "    return {'HOTA': HOTA, 'AssA': AssA, 'DetA': DetA, 'LocA': LocA, 'HOTA TP': HOTA_TP, 'HOTA FN': HOTA_FN, 'HOTA FP': HOTA_FP}\n",
    "\n",
    "\n",
    "def link_detections(detections_per_frame, max_dist: float = 7.0):\n",
    "    # Lightweight nearest-neighbor tracker to assign track_ids to per-frame detections.\n",
    "    next_id = 0\n",
    "    active = {}\n",
    "    records = []\n",
    "    for frame_idx, dets in enumerate(detections_per_frame):\n",
    "        assigned = [False] * len(dets)\n",
    "        new_active = {}\n",
    "        for tid, (tx, ty, lf) in list(active.items()):\n",
    "            best = None; best_d = max_dist\n",
    "            for i, (x, y) in enumerate(dets):\n",
    "                if assigned[i]:\n",
    "                    continue\n",
    "                d = ((x - tx) ** 2 + (y - ty) ** 2) ** 0.5\n",
    "                if d < best_d:\n",
    "                    best_d = d; best = i\n",
    "            if best is not None:\n",
    "                assigned[best] = True\n",
    "                new_active[tid] = (dets[best][0], dets[best][1], frame_idx)\n",
    "                records.append({'frame': frame_idx, 'x': dets[best][0], 'y': dets[best][1], 'track_id': tid})\n",
    "        for i, (x, y) in enumerate(dets):\n",
    "            if not assigned[i]:\n",
    "                tid = next_id; next_id += 1\n",
    "                new_active[tid] = (x, y, frame_idx)\n",
    "                records.append({'frame': frame_idx, 'x': x, 'y': y, 'track_id': tid})\n",
    "        active = new_active\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def track_detections_simple(preds: pd.DataFrame, max_dist: float):\n",
    "    max_frame = int(preds.frame.max()) if len(preds) else -1\n",
    "    dets = [[] for _ in range(max_frame + 1)]\n",
    "    for _, r in preds.iterrows():\n",
    "        dets[int(r.frame)].append((float(r.x), float(r.y)))\n",
    "    tracks_df = link_detections(dets, max_dist=max_dist)\n",
    "    return tracks_df\n",
    "\n",
    "\n",
    "def show_val_overlay(model, dataset, val_indices, device: str, threshold: float = 0.5):\n",
    "    import numpy as np\n",
    "    if len(val_indices) == 0:\n",
    "        print(\"No validation indices to visualize\")\n",
    "        return\n",
    "    idx = int(np.random.choice(val_indices))\n",
    "    img, mask = dataset[idx]\n",
    "    if isinstance(img, tuple):\n",
    "        img = img[0]\n",
    "    base = img.squeeze().cpu().numpy()\n",
    "    if base.max() > 1:\n",
    "        base = base / 255.0\n",
    "    with torch.no_grad():\n",
    "        out = model(img.unsqueeze(0).to(device))\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            out = out[-1]\n",
    "        prob = torch.sigmoid(out)[0, 0].cpu().numpy()\n",
    "    bin_mask = (prob >= threshold).astype(float)\n",
    "    gt_mask = mask.squeeze().cpu().numpy()\n",
    "    gt_props = regionprops(label(gt_mask > 0.5)); pred_props = regionprops(label(bin_mask))\n",
    "    gx, gy = [], []\n",
    "    for p in gt_props:\n",
    "        cy, cx = p.centroid; gx.append(cx); gy.append(cy)\n",
    "    px, py = [], []\n",
    "    for p in pred_props:\n",
    "        cy, cx = p.centroid; px.append(cx); py.append(cy)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axes[0].imshow(base, cmap='gray'); axes[0].set_title(f\"Val image (idx {idx})\")\n",
    "    axes[1].imshow(prob, cmap='viridis'); axes[1].set_title(\"Prob map\")\n",
    "    axes[2].imshow(base, cmap='gray')\n",
    "    axes[2].scatter(gx, gy, s=35, facecolors='none', edgecolors='lime', linewidths=1.2, label='GT')\n",
    "    axes[2].scatter(px, py, s=30, marker='x', color='red', linewidths=1.2, label='Pred')\n",
    "    axes[2].set_title(\"Overlay: red=pred (x), green=gt (o)\")\n",
    "    axes[2].legend(loc='upper right')\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc8af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title BTrack integration + orchestration\n",
    "\n",
    "def run_btrack_tracking(oof_csv: str, output_csv: str = \"tracks_btrack.csv\", config_path: str | None = None, max_search_radius: float = 12.0):\n",
    "    try:\n",
    "        import btrack\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\"btrack is not installed. Install with `pip install btrack` or rerun the setup cell.\") from e\n",
    "\n",
    "    df = pd.read_csv(oof_csv)\n",
    "    detections = [{\"t\": int(row.frame), \"x\": float(row.x), \"y\": float(row.y), \"z\": 0.0, \"r\": 5.0} for _, row in df.iterrows()]\n",
    "    if config_path is None:\n",
    "        config_path = btrack.datasets.cell_config()\n",
    "\n",
    "    with btrack.BayesianTracker() as tracker:\n",
    "        tracker.configure_from_file(config_path)\n",
    "        tracker.max_search_radius = max_search_radius\n",
    "        tracker.append(detections)\n",
    "        tracker.volume = ((0, 1024), (0, 1024), (0, 1))\n",
    "        tracker.track_interactive(step_size=100)\n",
    "        tracker.optimize()\n",
    "        tracks_out = []\n",
    "        for track in tracker.tracks:\n",
    "            for node in track:\n",
    "                tracks_out.append({\"track_id\": int(track.ID), \"frame\": int(node.t), \"x\": float(node.x), \"y\": float(node.y)})\n",
    "    tracks_df = pd.DataFrame(tracks_out).sort_values([\"frame\", \"track_id\"])\n",
    "    tracks_df.to_csv(output_csv, index=False)\n",
    "    print(f\"BTrack tracks saved to {output_csv}\")\n",
    "    return tracks_df\n",
    "\n",
    "\n",
    "def run_grand_kfold(\n",
    "    out_dir: str = \"experiment_dataset\",\n",
    "    k_splits: int = 5,\n",
    "    use_bonus: bool = True,\n",
    "    epochs: int = 30,\n",
    "    batch_size: int = 8,\n",
    "    lr: float = 1e-3,\n",
    "    num_workers: int = 2,\n",
    "    threshold: float = 0.5,\n",
    "    save_dir: str = \".\",\n",
    "    visualize_samples: bool = True,\n",
    "    sample_idx: int = 0,\n",
    "    match_thresh: float = 5.0,\n",
    "    run_btrack: bool = False,\n",
    "    btrack_config: str | None = None,\n",
    "    btrack_radius: float = 12.0,\n",
    "    train_aug_kwargs: dict | None = None,\n",
    "):\n",
    "    out_path = Path(out_dir)\n",
    "    video_root = out_path / \"video\"\n",
    "    bonus_root = out_path / \"bonus\"\n",
    "    video_map = out_path / \"video_map.csv\"\n",
    "\n",
    "    train_tf = get_train_transforms(**(train_aug_kwargs or {}))\n",
    "    val_tf = get_val_transforms()\n",
    "\n",
    "    video_base = AugmentedMicroscopyDataset(video_root, transform=None)\n",
    "    bonus_base = AugmentedMicroscopyDataset(bonus_root, transform=train_tf)\n",
    "    n_frames = len(video_base)\n",
    "    if n_frames == 0:\n",
    "        raise ValueError(\"No frames found in experiment_dataset/video. Run prepare_grand_dataset first.\")\n",
    "\n",
    "    print(f\"Frames: {n_frames}, Bonus: {len(bonus_base)} (use_bonus={use_bonus})\")\n",
    "\n",
    "    kf = KFold(n_splits=k_splits, shuffle=True, random_state=42)\n",
    "    all_preds = []\n",
    "    model_paths = []\n",
    "    fold_detAs = []\n",
    "    fold_hota_detAs = []\n",
    "\n",
    "    roi_tuple = (ROI_Y_MIN, ROI_Y_MAX, ROI_X_MIN, ROI_X_MAX)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(range(n_frames)), start=1):\n",
    "        print(f\"\n",
    "========== Fold {fold} / {k_splits} ==========\")\n",
    "        train_loader, val_loader = make_fold_loaders(\n",
    "            video_root, bonus_root, train_idx, val_idx,\n",
    "            batch_size=batch_size, num_workers=num_workers, use_bonus=use_bonus,\n",
    "            train_transform=train_tf, val_transform=val_tf,\n",
    "        )\n",
    "        model = LightweightUNet(in_channels=1, n_classes=1).to(device)\n",
    "        model, best_val = train_one_fold(model, train_loader, val_loader, device, epochs=epochs, lr=lr)\n",
    "\n",
    "        ckpt_path = Path(save_dir) / f\"model_fold_{fold}.pth\"\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        model_paths.append(ckpt_path)\n",
    "        print(f\"Saved best checkpoint (val_loss={best_val:.4f}) to {ckpt_path}\")\n",
    "\n",
    "        fold_preds = infer_fold_oof(\n",
    "            model,\n",
    "            val_idx,\n",
    "            video_root=video_root,\n",
    "            video_map=video_map,\n",
    "            device=device,\n",
    "            threshold=threshold,\n",
    "        )\n",
    "        fold_df = pd.DataFrame(fold_preds)\n",
    "        all_preds.extend(fold_preds)\n",
    "\n",
    "        # Track detections per fold and compute HOTA-style DetA\n",
    "        tracks_df_fold = track_detections_simple(fold_df, max_dist=match_thresh)\n",
    "        gt_fold = _filter_roi_frames(pd.read_csv(\"val_data/val.csv\"), frames_filter=val_idx, roi=roi_tuple)\n",
    "        tr_fold = _filter_roi_frames(tracks_df_fold.rename(columns={'track_id': 'track_id'}), frames_filter=val_idx, roi=roi_tuple)\n",
    "        hota_res_fold = hota_metric(gt_fold, tr_fold, threshold=match_thresh)\n",
    "        fold_hota_detAs.append(hota_res_fold['DetA'])\n",
    "        print(f\"Fold {fold} HOTA DetA={hota_res_fold['DetA']:.4f} (TP={hota_res_fold['HOTA TP']} FP={hota_res_fold['HOTA FP']} FN={hota_res_fold['HOTA FN']})\")\n",
    "\n",
    "        if visualize_samples and len(val_idx) > 0:\n",
    "            val_ds = AugmentedMicroscopyDataset(video_root, transform=val_tf)\n",
    "            show_val_overlay(model, val_ds, val_indices=val_idx, device=device, threshold=threshold)\n",
    "\n",
    "    oof_df = pd.DataFrame(all_preds).sort_values(\"frame\").reset_index(drop=True)\n",
    "    oof_path = Path(save_dir) / \"oof_predictions.csv\"\n",
    "    oof_df.to_csv(oof_path, index=False)\n",
    "    print(f\"OOF predictions saved to {oof_path}\")\n",
    "\n",
    "    # Track full OOF detections and compute HOTA-style DetA\n",
    "    tracks_df = track_detections_simple(oof_df, max_dist=match_thresh)\n",
    "    gt_full = _filter_roi_frames(pd.read_csv(\"val_data/val.csv\"), frames_filter=None, roi=roi_tuple)\n",
    "    tr_full = _filter_roi_frames(tracks_df, frames_filter=None, roi=roi_tuple)\n",
    "    hota_res_full = hota_metric(gt_full, tr_full, threshold=match_thresh)\n",
    "    print(f\"HOTA DetA (tracked OOF) @ {match_thresh}px: {hota_res_full['DetA']:.4f} (TP={hota_res_full['HOTA TP']} FP={hota_res_full['HOTA FP']} FN={hota_res_full['HOTA FN']})\")\n",
    "    if fold_hota_detAs:\n",
    "        print(f\"Average fold HOTA DetA: {np.mean(fold_hota_detAs):.4f}\")\n",
    "\n",
    "    if run_btrack:\n",
    "        tracks_df = run_btrack_tracking(\n",
    "            str(oof_path),\n",
    "            output_csv=str(Path(save_dir) / \"tracks_btrack.csv\"),\n",
    "            config_path=btrack_config,\n",
    "            max_search_radius=btrack_radius,\n",
    "        )\n",
    "\n",
    "    if visualize_samples:\n",
    "        sample_ds = AugmentedMicroscopyDataset(video_root, transform=val_tf)\n",
    "        visualize_fold_predictions(model_paths, sample_ds, device=device, threshold=threshold, sample_idx=sample_idx)\n",
    "\n",
    "    return oof_df, model_paths, tracks_df\n",
    "\n",
    "\n",
    "print(\"Helpers ready: prepare_grand_dataset, run_grand_kfold, run_btrack_tracking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a51d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Download / Fetch Data (run once)\n",
    "AUTO_FETCH_DRIVE = False  # set True to copy training data from Drive\n",
    "DRIVE_SOURCE_PATH = \"/content/drive/MyDrive/unet_train\"\n",
    "TARGET_DIR = \"real_training_data\"\n",
    "\n",
    "# Download validation data if missing\n",
    "if not (Path(\"val_data/val.tif\").exists() and Path(\"val_data/val.csv\").exists()):\n",
    "    download_validation_data(target_dir=\"val_data\")\n",
    "else:\n",
    "    print(\"Validation data already present.\")\n",
    "\n",
    "# Optional Drive copy for training data\n",
    "if AUTO_FETCH_DRIVE:\n",
    "    fetch_from_drive(DRIVE_SOURCE_PATH=DRIVE_SOURCE_PATH, TARGET_DIR=TARGET_DIR)\n",
    "else:\n",
    "    print(\"Skipping Drive fetch (AUTO_FETCH_DRIVE=False). Ensure real_training_data/ exists with images/ and masks/.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99396f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run Full Pipeline (edit params as needed)\n",
    "# Paths\n",
    "REAL_DATA_DIR = \"real_training_data\"\n",
    "VAL_TIF = \"val_data/val.tif\"\n",
    "VAL_CSV = \"val_data/val.csv\"\n",
    "OUT_DIR = \"experiment_dataset\"\n",
    "SAVE_DIR = \".\"\n",
    "\n",
    "# Pipeline knobs\n",
    "K_SPLITS = 5\n",
    "USE_BONUS = True          # set False to ignore bonus set\n",
    "EPOCHS = 30               # keep modest for T4\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-3\n",
    "THRESHOLD = 0.5           # sigmoid->mask threshold for detections\n",
    "MATCH_THRESH = 5.0        # px for DetA calculation\n",
    "VISUALIZE = True          # show sample masks from all folds\n",
    "SAMPLE_IDX = 0            # which sample to visualize\n",
    "RUN_BTRACK = True         # toggle btrack tracking\n",
    "BTRACK_CONFIG = None      # None -> use btrack default cell config\n",
    "BTRACK_RADIUS = 12.0      # max_search_radius\n",
    "USE_TRAIN_AUG = True      # set False to disable train-time augmentations\n",
    "\n",
    "# Augmentation knobs\n",
    "TRAIN_AUG_KWARGS = dict(\n",
    "    rotate_p=0.7,\n",
    "    hflip_p=0.5,\n",
    "    vflip_p=0.5,\n",
    "    clahe_p=0.5,\n",
    "    brightness_p=0.5,\n",
    "    gauss_p=0.3,\n",
    "    elastic_p=0.2,\n",
    "    coarse_p=0.5,\n",
    "    coarse_max_holes=16,\n",
    "    coarse_min_holes=8,\n",
    "    coarse_max_hw=16,\n",
    "    coarse_min_hw=8,\n",
    "    crop_scale_min=0.8,\n",
    "    crop_scale_max=1.0,\n",
    "    crop_ratio_min=0.9,\n",
    "    crop_ratio_max=1.1,\n",
    ")\n",
    "\n",
    "# 1) Build grand dataset (cleans OUT_DIR each time)\n",
    "prepare_grand_dataset(\n",
    "    real_data_dir=REAL_DATA_DIR,\n",
    "    val_tif_path=VAL_TIF,\n",
    "    val_csv_path=VAL_CSV,\n",
    "    out_dir=OUT_DIR,\n",
    ")\n",
    "\n",
    "# 2) Train K-fold, infer OOF, compute DetA, (optionally) run btrack, visualize\n",
    "oof_df, model_paths, tracks_df = run_grand_kfold(\n",
    "    out_dir=OUT_DIR,\n",
    "    k_splits=K_SPLITS,\n",
    "    use_bonus=USE_BONUS,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    threshold=THRESHOLD,\n",
    "    save_dir=SAVE_DIR,\n",
    "    visualize_samples=VISUALIZE,\n",
    "    sample_idx=SAMPLE_IDX,\n",
    "    match_thresh=MATCH_THRESH,\n",
    "    run_btrack=RUN_BTRACK,\n",
    "    btrack_config=BTRACK_CONFIG,\n",
    "    btrack_radius=BTRACK_RADIUS,\n",
    "    train_aug_kwargs=TRAIN_AUG_KWARGS if USE_TRAIN_AUG else {},\n",
    ")\n",
    "\n",
    "print(\"Done. OOF detections:\", len(oof_df), \"| Models:\", model_paths)\n",
    "if tracks_df is not None:\n",
    "    print(\"BTrack tracks:\", len(tracks_df))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0270dcaf729e4aa59899251e0c686f22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f578bfeaf57d4a569f0538e5d7aa50f3",
      "max": 30,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_42b0a6d56e114743bb452c4c946c585d",
      "value": 30
     }
    },
    "42b0a6d56e114743bb452c4c946c585d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49466f2bd6d240eab5b72c5ff5fe45bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d0184b0a370455c810dfaf902fbd611",
      "placeholder": "​",
      "style": "IPY_MODEL_4fcd4fcd20524acdb781a28b80faded9",
      "value": "100%"
     }
    },
    "4fcd4fcd20524acdb781a28b80faded9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87be2af840f74ebeb80ba9c32a577cef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c624a7dc8d894d03b8a369653ea0b83e",
      "placeholder": "​",
      "style": "IPY_MODEL_89a22e19eeab4f8f8b00e99d1efed771",
      "value": " 30/30 [00:04&lt;00:00,  7.79it/s]"
     }
    },
    "89a22e19eeab4f8f8b00e99d1efed771": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d0184b0a370455c810dfaf902fbd611": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bca285bd54f24a6c8661b66e285b6750": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd47b70f71d94285ada56cc0c59a0022": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49466f2bd6d240eab5b72c5ff5fe45bb",
       "IPY_MODEL_0270dcaf729e4aa59899251e0c686f22",
       "IPY_MODEL_87be2af840f74ebeb80ba9c32a577cef"
      ],
      "layout": "IPY_MODEL_bca285bd54f24a6c8661b66e285b6750"
     }
    },
    "c624a7dc8d894d03b8a369653ea0b83e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f578bfeaf57d4a569f0538e5d7aa50f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
