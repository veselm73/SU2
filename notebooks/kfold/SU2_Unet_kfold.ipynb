{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae23d297",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4066,
     "status": "ok",
     "timestamp": 1764764918660,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "ae23d297",
    "outputId": "ed361015-b2a9-4279-babe-2d26e9c69ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading uv 0.9.15 x86_64-unknown-linux-gnu\n",
      "no checksums to verify\n",
      "installing to /usr/local/bin\n",
      "  uv\n",
      "  uvx\n",
      "everything's installed!\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m118 packages\u001b[0m \u001b[2min 1.03s\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 719ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 61ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1malbumentations\u001b[0m\u001b[2m==2.0.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1malbumentations\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbtrack\u001b[0m\u001b[2m==0.6.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==1.10.24\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mqudida\u001b[0m\u001b[2m==0.0.4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install uv\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Use uv to install a compatible stack (pydantic v1 + albumentations 1.3.1)\n",
    "!uv pip install --system \\\n",
    "    \"btrack==0.6.5\" \\\n",
    "    \"pydantic<2\" \\\n",
    "    \"albumentations==1.3.1\" \\\n",
    "    numpy \\\n",
    "    pandas \\\n",
    "    scipy \\\n",
    "    scikit-image \\\n",
    "    scikit-learn \\\n",
    "    opencv-python-headless \\\n",
    "    matplotlib \\\n",
    "    seaborn \\\n",
    "    tqdm \\\n",
    "    ipywidgets \\\n",
    "    tifffile \\\n",
    "    numba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2097722a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7498,
     "status": "ok",
     "timestamp": 1764765272934,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "2097722a",
    "outputId": "7cdc44c0-eebd-47aa-e21b-40e914f5284f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# @title Imports & Globals\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from skimage.measure import label, regionprops\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# ROI constants\n",
    "ROI_Y_MIN, ROI_Y_MAX = 512, 768\n",
    "ROI_X_MIN, ROI_X_MAX = 256, 512\n",
    "ROI_H, ROI_W = ROI_Y_MAX - ROI_Y_MIN, ROI_X_MAX - ROI_X_MIN\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "import random\n",
    "from IPython.display import HTML, display\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.collections as mc\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from PIL import Image\n",
    "from scipy import spatial, optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89ba996",
   "metadata": {
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1764765273112,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "e89ba996"
   },
   "outputs": [],
   "source": [
    "# @title Data Download Helpers (validation + optional Drive training)\n",
    "import os, shutil, zipfile, requests\n",
    "\n",
    "# Download validation video+labels zip (with SSL chain)\n",
    "def download_validation_data(target_dir: str = \"val_data\",\n",
    "                             url: str = \"https://su2.utia.cas.cz/files/labs/final2025/val_and_sota.zip\",\n",
    "                             cert_url: str = \"https://pki.cesnet.cz/_media/certs/chain-harica-rsa-ov-crosssigned-root.pem\"):\n",
    "    if os.path.exists(target_dir) and len(os.listdir(target_dir)) > 0:\n",
    "        print(f\"'{target_dir}' already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    chain_path = \"chain-harica-cross.pem\"\n",
    "    print(\"1) Downloading SSL certificate chain...\")\n",
    "    r = requests.get(cert_url, timeout=10, stream=True)\n",
    "    r.raise_for_status()\n",
    "    with open(chain_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(\"2) Downloading validation archive...\")\n",
    "    zip_name = os.path.basename(url)\n",
    "    with requests.get(url, stream=True, verify=chain_path, timeout=30) as resp:\n",
    "        resp.raise_for_status()\n",
    "        with open(zip_name, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    print(\"3) Extracting...\")\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_name, \"r\") as zf:\n",
    "        zf.extractall(target_dir)\n",
    "    os.remove(zip_name)\n",
    "    print(f\"Done. Data in '{target_dir}/'\")\n",
    "\n",
    "\n",
    "# Optional: copy training data from Google Drive (manual annotations)\n",
    "def fetch_from_drive(DRIVE_SOURCE_PATH: str = \"/content/drive/MyDrive/unet_train\",\n",
    "                     TARGET_DIR: str = \"real_training_data\"):\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "    except ImportError:\n",
    "        print(\"google.colab not available (not running in Colab).\")\n",
    "        return\n",
    "\n",
    "    print(\"?? Mounting Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    if os.path.exists(TARGET_DIR) and len(os.listdir(TARGET_DIR)) > 0:\n",
    "        print(f\"'{TARGET_DIR}' already exists and is not empty. Skipping copy.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(DRIVE_SOURCE_PATH):\n",
    "        print(f\"? Source not found: {DRIVE_SOURCE_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Copying {DRIVE_SOURCE_PATH} -> {TARGET_DIR}\")\n",
    "    try:\n",
    "        shutil.copytree(DRIVE_SOURCE_PATH, TARGET_DIR)\n",
    "        print(\"? Copy complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"? Copy failed: {e}\")\n",
    "        if os.path.exists(TARGET_DIR):\n",
    "            shutil.rmtree(TARGET_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "411a0e66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1764767342630,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "411a0e66",
    "outputId": "de9ccc49-8363-46e5-deec-95d9b22039f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ok, output shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class LightweightUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_classes=1, features=[16, 32, 64, 128]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        in_ch = in_channels\n",
    "        for feature in features:\n",
    "            self.downs.append(ConvBlock(in_ch, feature))\n",
    "            in_ch = feature\n",
    "\n",
    "        self.bottleneck = ConvBlock(features[-1], features[-1] * 2)\n",
    "\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(ConvBlock(feature * 2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = F.interpolate(x, size=skip_connection.shape[2:], mode=\"bilinear\", align_corners=True)\n",
    "            x = self.ups[idx + 1](torch.cat((skip_connection, x), dim=1))\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.focal = FocalLoss(alpha=0.75, gamma=2)\n",
    "\n",
    "    def dice_loss(self, pred, target, eps=1e-6):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        smooth = 1.0\n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2.0 * intersection + smooth) / (pred.sum() + target.sum() + smooth + eps)\n",
    "        return 1 - dice\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if isinstance(inputs, list):\n",
    "            loss = 0\n",
    "            for item in inputs:\n",
    "                loss += 0.5 * self.focal(item, targets) + 0.5 * self.dice_loss(item, targets)\n",
    "            return loss / len(inputs)\n",
    "        return 0.5 * self.focal(inputs, targets) + 0.5 * self.dice_loss(inputs, targets)\n",
    "\n",
    "# Quick sanity check\n",
    "_dummy = torch.randn(1, 1, 256, 256)\n",
    "_model = LightweightUNet()\n",
    "_out = _model(_dummy)\n",
    "print(f\"Model ok, output shape: {_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70ba71d",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1764765273460,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "a70ba71d"
   },
   "outputs": [],
   "source": [
    "# @title Data prep: build experiment_dataset\n",
    "\n",
    "def prepare_grand_dataset(\n",
    "    real_data_dir: str = \"real_training_data\",\n",
    "    val_tif_path: str = \"val_data/val.tif\",\n",
    "    val_csv_path: str = \"val_data/val.csv\",\n",
    "    out_dir: str = \"experiment_dataset\",\n",
    "    roi_y: Tuple[int, int] = (ROI_Y_MIN, ROI_Y_MAX),\n",
    "    roi_x: Tuple[int, int] = (ROI_X_MIN, ROI_X_MAX),\n",
    ") -> Path:\n",
    "    out_path = Path(out_dir)\n",
    "    bonus_images = out_path / \"bonus\" / \"images\"\n",
    "    bonus_masks = out_path / \"bonus\" / \"masks\"\n",
    "    video_images = out_path / \"video\" / \"images\"\n",
    "    video_masks = out_path / \"video\" / \"masks\"\n",
    "\n",
    "    if out_path.exists():\n",
    "        shutil.rmtree(out_path)\n",
    "    for p in [bonus_images, bonus_masks, video_images, video_masks]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    src_images = Path(real_data_dir) / \"images\"\n",
    "    src_masks = Path(real_data_dir) / \"masks\"\n",
    "    if not src_images.exists() or not src_masks.exists():\n",
    "        raise FileNotFoundError(\"real_training_data must contain images/ and masks/\")\n",
    "    shutil.copytree(src_images, bonus_images, dirs_exist_ok=True)\n",
    "    shutil.copytree(src_masks, bonus_masks, dirs_exist_ok=True)\n",
    "\n",
    "    if not Path(val_tif_path).exists():\n",
    "        raise FileNotFoundError(f\"Missing video file: {val_tif_path}\")\n",
    "    if not Path(val_csv_path).exists():\n",
    "        raise FileNotFoundError(f\"Missing CSV file: {val_csv_path}\")\n",
    "\n",
    "    video = tifffile.imread(val_tif_path)\n",
    "    coords = pd.read_csv(val_csv_path)\n",
    "    y_min, y_max = roi_y\n",
    "    x_min, x_max = roi_x\n",
    "    records: List[dict] = []\n",
    "\n",
    "    for idx, frame in enumerate(video):\n",
    "        crop = frame[y_min:y_max, x_min:x_max]\n",
    "        mask = np.zeros((ROI_H, ROI_W), dtype=np.uint8)\n",
    "        points = coords[coords[\"frame\"] == idx]\n",
    "        for _, row in points.iterrows():\n",
    "            cx = int(round(row[\"x\"] - x_min))\n",
    "            cy = int(round(row[\"y\"] - y_min))\n",
    "            if 0 <= cx < ROI_W and 0 <= cy < ROI_H:\n",
    "                cv2.circle(mask, (cx, cy), 5, 1, -1)\n",
    "        fname = f\"frame_{idx + 1:03}.png\"\n",
    "        cv2.imwrite(str(video_images / fname), crop)\n",
    "        cv2.imwrite(str(video_masks / fname), mask * 255)\n",
    "        records.append({\"filename\": fname, \"real_frame_idx\": idx})\n",
    "\n",
    "    pd.DataFrame(records).to_csv(out_path / \"video_map.csv\", index=False)\n",
    "    print(f\"Dataset written to {out_path.resolve()}\")\n",
    "    print(f\"Bonus samples: {len(list(bonus_images.glob('*.png')))} | Video frames: {len(records)}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65810886",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1764766171117,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "65810886"
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "def get_train_transforms(\n",
    "    rotate_p: float = 0.7,\n",
    "    hflip_p: float = 0.5,\n",
    "    vflip_p: float = 0.5,\n",
    "    clahe_p: float = 0.5,\n",
    "    brightness_p: float = 0.5,\n",
    "    gauss_p: float = 0.3,\n",
    "    elastic_p: float = 0.2,\n",
    "    coarse_p: float = 0.5,\n",
    "    coarse_max_holes: int = 16,\n",
    "    coarse_min_holes: int = 8,\n",
    "    coarse_max_hw: int = 16,\n",
    "    coarse_min_hw: int = 8,\n",
    "    crop_scale_min: float = 0.8,\n",
    "    crop_scale_max: float = 1.0,\n",
    "    crop_ratio_min: float = 0.9,\n",
    "    crop_ratio_max: float = 1.1,\n",
    "):\n",
    "    return A.Compose([\n",
    "        A.Rotate(limit=180, p=rotate_p),\n",
    "        A.HorizontalFlip(p=hflip_p),\n",
    "        A.VerticalFlip(p=vflip_p),\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=clahe_p),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=brightness_p),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=gauss_p),\n",
    "        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=elastic_p),\n",
    "        A.CoarseDropout(\n",
    "            max_holes=coarse_max_holes, max_height=coarse_max_hw, max_width=coarse_max_hw,\n",
    "            min_holes=coarse_min_holes, min_height=coarse_min_hw, min_width=coarse_min_hw,\n",
    "            fill_value=0, mask_fill_value=0, p=coarse_p,\n",
    "        ),\n",
    "        A.RandomResizedCrop(\n",
    "            height=256,\n",
    "            width=256,\n",
    "            scale=(crop_scale_min, crop_scale_max),\n",
    "            ratio=(crop_ratio_min, crop_ratio_max),\n",
    "            p=0.5,\n",
    "        ),\n",
    "        A.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_transforms():\n",
    "    return A.Compose([ToTensorV2()])\n",
    "\n",
    "\n",
    "class AugmentedMicroscopyDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, transform=None, return_meta: bool = False):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.return_meta = return_meta\n",
    "        self.image_paths = sorted(self.root_dir.joinpath(\"images\").glob(\"*.png\"))\n",
    "        self.mask_paths = sorted(self.root_dir.joinpath(\"masks\").glob(\"*.png\"))\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "            raise ValueError(f\"Images and masks counts differ in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_UNCHANGED)\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_UNCHANGED)\n",
    "        if img is None or mask is None:\n",
    "            raise FileNotFoundError(f\"Missing pair for index {idx}\")\n",
    "        if img.ndim == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        if mask.ndim == 3:\n",
    "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Convert to uint8 for augmentations (CLAHE requires uint8)\n",
    "        img = img.astype(np.uint8)\n",
    "        # Ensure mask is also uint8 (0 or 255) for albumentations\n",
    "        mask = (mask > 127).astype(np.uint8) * 255\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                img = img.float()\n",
    "                if img.max() > 1:\n",
    "                    img = img / 255.0\n",
    "            if isinstance(mask, torch.Tensor):\n",
    "                mask = mask.float()\n",
    "                if mask.max() > 1:\n",
    "                    mask = mask / 255.0\n",
    "                mask = (mask > 0.5).float()\n",
    "        else:\n",
    "            # Manual conversion and normalization if no transform is applied\n",
    "            img = torch.from_numpy(img).float().unsqueeze(0) / 255.0\n",
    "            mask = torch.from_numpy(mask).float().unsqueeze(0) / 255.0\n",
    "            mask = (mask > 0.5).float()\n",
    "\n",
    "        # Ensure mask has a channel dimension (1, H, W)\n",
    "        while mask.ndim < 3:\n",
    "            mask = mask.unsqueeze(0)\n",
    "\n",
    "        if self.return_meta:\n",
    "            meta = {'filename': img_path.name}\n",
    "            return img, mask, meta\n",
    "        else:\n",
    "            return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86b20946",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1764766368941,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "86b20946"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def make_fold_loaders(video_root: Path, bonus_root: Path, train_idx, val_idx, batch_size=8, num_workers=2, use_bonus: bool = True, train_transform=None, val_transform=None):\n",
    "    train_tf = train_transform if train_transform is not None else get_train_transforms()\n",
    "    val_tf = val_transform if val_transform is not None else get_val_transforms()\n",
    "    train_ds = Subset(AugmentedMicroscopyDataset(video_root, transform=train_tf), train_idx)\n",
    "    val_ds = Subset(AugmentedMicroscopyDataset(video_root, transform=val_tf), val_idx)\n",
    "    datasets = [train_ds]\n",
    "    bonus_len = 0\n",
    "    if use_bonus:\n",
    "        bonus_ds = AugmentedMicroscopyDataset(bonus_root, transform=train_tf)\n",
    "        datasets.append(bonus_ds)\n",
    "        bonus_len = len(bonus_ds)\n",
    "    train_loader = DataLoader(ConcatDataset(datasets), batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=max(1, batch_size // 2), shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    print(f\"Train loader: video {len(train_idx)} + bonus {bonus_len} = {len(train_loader.dataset)}\")\n",
    "    print(f\"Val loader: {len(val_idx)}\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def train_one_fold(model, train_loader, val_loader, device, epochs=40, lr=1e-3):\n",
    "    import copy\n",
    "    criterion = ComboLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    best_val = float('inf')\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for imgs, masks in train_loader:\n",
    "            # Explicitly cast to float after moving to device\n",
    "            imgs, masks = imgs.to(device).float(), masks.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                outputs = outputs[-1]\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            # Add gradient clipping to prevent NaN loss from exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * imgs.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                # Explicitly cast to float after moving to device\n",
    "                imgs, masks = imgs.to(device).float(), masks.to(device).float()\n",
    "                outputs = model(imgs)\n",
    "                if isinstance(outputs, (list, tuple)):\n",
    "                    outputs = outputs[-1]\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"Epoch {epoch:03d}: train {train_loss:.4f} | val {val_loss:.4f} | best {best_val:.4f}\")\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_val\n",
    "\n",
    "\n",
    "def infer_fold_oof(model, val_indices, video_root: Path, video_map: Path, device, threshold: float = 0.5):\n",
    "    infer_ds = Subset(AugmentedMicroscopyDataset(video_root, transform=get_val_transforms(), return_meta=True), val_indices)\n",
    "    infer_loader = DataLoader(infer_ds, batch_size=1, shuffle=False)\n",
    "    frame_lookup = pd.read_csv(video_map).set_index('filename')['real_frame_idx'].to_dict()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _, meta in infer_loader:\n",
    "            filenames = meta['filename'] if isinstance(meta, dict) else [m['filename'] for m in meta]\n",
    "            # Explicitly cast to float after moving to device\n",
    "            imgs = imgs.to(device).float()\n",
    "            outputs = model(imgs)\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                outputs = outputs[-1]\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            for b in range(probs.shape[0]):\n",
    "                bin_mask = (probs[b, 0] >= threshold).astype(np.uint8)\n",
    "                labeled = label(bin_mask)\n",
    "                for prop in regionprops(labeled):\n",
    "                    cy, cx = prop.centroid\n",
    "                    preds.append({'frame': int(frame_lookup[filenames[b]]), 'x': float(cx + ROI_X_MIN), 'y': float(cy + ROI_Y_MIN)})\n",
    "    return preds\n",
    "\n",
    "\n",
    "def _filter_roi_frames(df: pd.DataFrame, frames_filter=None, roi=None):\n",
    "    if frames_filter is not None:\n",
    "        frame_set = set(map(int, frames_filter))\n",
    "        df = df[df.frame.isin(frame_set)]\n",
    "    if roi is not None:\n",
    "        y_min, y_max, x_min, x_max = roi\n",
    "        df = df[(df.y >= y_min) & (df.y < y_max) & (df.x >= x_min) & (df.x < x_max)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def hota_metric(gt: pd.DataFrame, tr: pd.DataFrame, threshold: float = 5.0):\n",
    "    \"\"\"Delegate to baseline hota() for consistency.\"\"\"\n",
    "    return hota(gt, tr, threshold=threshold)\n",
    "\n",
    "def link_detections(detections_per_frame, max_dist: float = 7.0):\n",
    "    # Lightweight nearest-neighbor tracker to assign track_ids to per-frame detections.\n",
    "    next_id = 0\n",
    "    active = {}\n",
    "    records = []\n",
    "    for frame_idx, dets in enumerate(detections_per_frame):\n",
    "        assigned = [False] * len(dets)\n",
    "        new_active = {}\n",
    "        for tid, (tx, ty, lf) in list(active.items()):\n",
    "            best = None; best_d = max_dist\n",
    "            for i, (x, y) in enumerate(dets):\n",
    "                if assigned[i]:\n",
    "                    continue\n",
    "                d = ((x - tx) ** 2 + (y - ty) ** 2) ** 0.5\n",
    "                if d < best_d:\n",
    "                    best_d = d; best = i\n",
    "            if best is not None:\n",
    "                assigned[best] = True\n",
    "                new_active[tid] = (dets[best][0], dets[best][1], frame_idx)\n",
    "                records.append({'frame': frame_idx, 'x': dets[best][0], 'y': dets[best][1], 'track_id': tid})\n",
    "        for i, (x, y) in enumerate(dets):\n",
    "            if not assigned[i]:\n",
    "                tid = next_id; next_id += 1\n",
    "                new_active[tid] = (x, y, frame_idx)\n",
    "                records.append({'frame': frame_idx, 'x': x, 'y': y, 'track_id': tid})\n",
    "        active = new_active\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def track_detections_simple(preds: pd.DataFrame, max_dist: float):\n",
    "    max_frame = int(preds.frame.max()) if len(preds) else -1\n",
    "    dets = [[] for _ in range(max_frame + 1)]\n",
    "    for _, r in preds.iterrows():\n",
    "        dets[int(r.frame)].append((float(r.x), float(r.y)))\n",
    "    tracks_df = link_detections(dets, max_dist=max_dist)\n",
    "    return tracks_df\n",
    "\n",
    "\n",
    "def show_val_overlay(model, dataset, val_indices, device: str, threshold: float = 0.5):\n",
    "    import numpy as np\n",
    "    if len(val_indices) == 0:\n",
    "        print(\"No validation indices to visualize\")\n",
    "        return\n",
    "    idx = int(np.random.choice(val_indices))\n",
    "    img, mask = dataset[idx]\n",
    "    if isinstance(img, tuple):\n",
    "        img = img[0]\n",
    "    base = img.squeeze().cpu().numpy()\n",
    "    if base.max() > 1:\n",
    "        base = base / 255.0\n",
    "    with torch.no_grad():\n",
    "        out = model(img.unsqueeze(0).to(device).float()) # Ensure input to model is float\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            out = out[-1]\n",
    "        prob = torch.sigmoid(out)[0, 0].cpu().numpy()\n",
    "    bin_mask = (prob >= threshold).astype(float)\n",
    "    gt_mask = mask.squeeze().cpu().numpy()\n",
    "    gt_props = regionprops(label(gt_mask > 0.5)); pred_props = regionprops(label(bin_mask))\n",
    "    gx, gy = [], []\n",
    "    for p in gt_props:\n",
    "        cy, cx = p.centroid; gx.append(cx); gy.append(cy)\n",
    "    px, py = [], []\n",
    "    for p in pred_props:\n",
    "        cy, cx = p.centroid; px.append(cx); py.append(cy)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axes[0].imshow(base, cmap='gray'); axes[0].set_title(f\"Val image (idx {idx})\")\n",
    "    axes[1].imshow(prob, cmap='viridis'); axes[1].set_title(\"Prob map\")\n",
    "    axes[2].imshow(base, cmap='gray')\n",
    "    axes[2].scatter(gx, gy, s=35, facecolors='none', edgecolors='lime', linewidths=1.2, label='GT')\n",
    "    axes[2].scatter(px, py, s=30, marker='x', color='red', linewidths=1.2, label='Pred')\n",
    "    axes[2].set_title(\"Overlay: red=pred (x), green=gt (o)\")\n",
    "    axes[2].legend(loc='upper right')\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebc8af67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1764766941123,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "ebc8af67",
    "outputId": "e05a2ae3-6e2e-4263-ceaf-da3c56e50acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready: prepare_grand_dataset, run_grand_kfold, run_btrack_tracking\n"
     ]
    }
   ],
   "source": [
    "# @title Metrics & Visualization Helpers\n",
    "\n",
    "def hota(gt: pd.DataFrame, tr: pd.DataFrame, threshold: float = 5) -> dict[str, float]:\n",
    "    \"\"\"Slightly adapted from https://github.com/JonathonLuiten/TrackEval\"\"\"\n",
    "    gt = gt.copy(); tr = tr.copy()\n",
    "    gt.track_id = gt.track_id.map({old: new for old, new in zip(gt.track_id.unique(), range(gt.track_id.nunique()))})\n",
    "    tr.track_id = tr.track_id.map({old: new for old, new in zip(tr.track_id.unique(), range(tr.track_id.nunique()))})\n",
    "    num_gt_ids = gt.track_id.nunique(); num_tr_ids = tr.track_id.nunique()\n",
    "    frames = sorted(set(gt.frame.unique()) | set(tr.frame.unique()))\n",
    "    potential_matches_count = np.zeros((num_gt_ids, num_tr_ids))\n",
    "    gt_id_count = np.zeros((num_gt_ids, 1)); tracker_id_count = np.zeros((1, num_tr_ids))\n",
    "    HOTA_TP = HOTA_FN = HOTA_FP = 0; LocA = 0.0\n",
    "    similarities = [1 - np.clip(spatial.distance.cdist(gt[gt.frame == t][['x', 'y']], tr[tr.frame == t][['x', 'y']]) / threshold, 0, 1) for t in frames]\n",
    "    for t in frames:\n",
    "        gt_ids_t = gt[gt.frame == t].track_id.to_numpy(); tr_ids_t = tr[tr.frame == t].track_id.to_numpy()\n",
    "        similarity = similarities[t]\n",
    "        sim_iou_denom = similarity.sum(0)[np.newaxis, :] + similarity.sum(1)[:, np.newaxis] - similarity\n",
    "        sim_iou = np.zeros_like(similarity)\n",
    "        mask = sim_iou_denom > 0 + np.finfo('float').eps\n",
    "        sim_iou[mask] = similarity[mask] / sim_iou_denom[mask]\n",
    "        potential_matches_count[gt_ids_t[:, None], tr_ids_t[None, :]] += sim_iou\n",
    "        gt_id_count[gt_ids_t] += 1; tracker_id_count[0, tr_ids_t] += 1\n",
    "    global_alignment_score = potential_matches_count / (gt_id_count + tracker_id_count - potential_matches_count)\n",
    "    matches_count = np.zeros_like(potential_matches_count)\n",
    "    for t in frames:\n",
    "        gt_ids_t = gt[gt.frame == t].track_id.to_numpy(); tr_ids_t = tr[tr.frame == t].track_id.to_numpy()\n",
    "        if len(gt_ids_t) == 0:\n",
    "            HOTA_FP += len(tr_ids_t); continue\n",
    "        if len(tr_ids_t) == 0:\n",
    "            HOTA_FN += len(gt_ids_t); continue\n",
    "        similarity = similarities[t]\n",
    "        score_mat = global_alignment_score[gt_ids_t[:, None], tr_ids_t[None, :]] * similarity\n",
    "        match_rows, match_cols = optimize.linear_sum_assignment(-score_mat)\n",
    "        mask = similarity[match_rows, match_cols] > 0\n",
    "        alpha_match_rows = match_rows[mask]; alpha_match_cols = match_cols[mask]\n",
    "        num_matches = len(alpha_match_rows)\n",
    "        HOTA_TP += num_matches; HOTA_FN += len(gt_ids_t) - num_matches; HOTA_FP += len(tr_ids_t) - num_matches\n",
    "        if num_matches > 0:\n",
    "            LocA += float(np.sum(similarity[alpha_match_rows, alpha_match_cols]))\n",
    "            matches_count[gt_ids_t[alpha_match_rows], tr_ids_t[alpha_match_cols]] += 1\n",
    "    ass_a = matches_count / np.maximum(1, gt_id_count + tracker_id_count - matches_count)\n",
    "    AssA = np.sum(matches_count * ass_a) / np.maximum(1, HOTA_TP)\n",
    "    DetA = HOTA_TP / np.maximum(1, HOTA_TP + HOTA_FN + HOTA_FP)\n",
    "    HOTA = np.sqrt(DetA * AssA)\n",
    "    return {'HOTA': HOTA, 'AssA': AssA, 'DetA': DetA, 'LocA': LocA, 'HOTA TP': HOTA_TP, 'HOTA FN': HOTA_FN, 'HOTA FP': HOTA_FP}\n",
    "\n",
    "\n",
    "def link_detections(detections_per_frame: list[list[tuple[int, int]]], max_dist: float = 7.0) -> pd.DataFrame:\n",
    "    next_track_id = 0\n",
    "    active_tracks: dict[int, tuple[int, int, int]] = {}\n",
    "    records: list[dict[str, int]] = []\n",
    "    for frame_idx, detections in enumerate(detections_per_frame):\n",
    "        assigned = [False] * len(detections)\n",
    "        detection_track_id: list[int | None] = [None] * len(detections)\n",
    "        updated_tracks: dict[int, tuple[int, int, int]] = {}\n",
    "        for track_id, (tx, ty, last_frame) in list(active_tracks.items()):\n",
    "            best_dist = max_dist; best_idx: int | None = None\n",
    "            for i, (x, y) in enumerate(detections):\n",
    "                if assigned[i]:\n",
    "                    continue\n",
    "                dist = math.hypot(x - tx, y - ty)\n",
    "                if dist < best_dist:\n",
    "                    best_dist = dist; best_idx = i\n",
    "            if best_idx is not None:\n",
    "                assigned[best_idx] = True; detection_track_id[best_idx] = track_id\n",
    "                updated_tracks[track_id] = (detections[best_idx][0], detections[best_idx][1], frame_idx)\n",
    "        for i, (x, y) in enumerate(detections):\n",
    "            if not assigned[i]:\n",
    "                tid = next_track_id; next_track_id += 1\n",
    "                detection_track_id[i] = tid\n",
    "                updated_tracks[tid] = (x, y, frame_idx)\n",
    "        active_tracks = updated_tracks\n",
    "        for i, (x, y) in enumerate(detections):\n",
    "            tid = detection_track_id[i]\n",
    "            records.append({'frame': frame_idx, 'x': x, 'y': y, 'track_id': tid})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def show_tracking(data, image_stack, y_min=512, y_max=768, x_min=256, x_max=512, tail_length=10, color='yellow', show_roi=True):\n",
    "    if isinstance(data, str):\n",
    "        trajectories_df = pd.read_csv(data)\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        trajectories_df = data.copy()\n",
    "    else:\n",
    "        raise TypeError(\"`data` must be a CSV file path or a pandas DataFrame.\")\n",
    "    tracks_in_roi = trajectories_df.groupby('track_id').filter(lambda t: (y_min < t.y.mean() < y_max) and (x_min < t.x.mean() < x_max))\n",
    "    html_code_linking = loading_html(\"Loading cropped region and tracks, please wait...\")\n",
    "    display(HTML(html_code_linking))\n",
    "    if show_roi:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.imshow(image_stack[0], cmap='magma')\n",
    "        rect = Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='cyan', facecolor='none')\n",
    "        ax.add_patch(rect); ax.set_title(\"Full image (cyan box shows cropped region)\")\n",
    "        plt.show()\n",
    "    def animate_trajectories_cropped(trajectories_df, image_stack, tail_length=10, color='yellow'):\n",
    "        cropped_stack = image_stack[:, y_min:y_max, x_min:x_max]\n",
    "        fig, ax = plt.subplots(); im = ax.imshow(cropped_stack[0], cmap='magma')\n",
    "        particles = trajectories_df['track_id'].unique()\n",
    "        line_collections = {pid: mc.LineCollection([], linewidths=1, colors=color) for pid in particles}\n",
    "        for lc in line_collections.values():\n",
    "            ax.add_collection(lc)\n",
    "        dot = ax.scatter([], [], s=5, c=color)\n",
    "        def animate(i):\n",
    "            im.set_array(cropped_stack[i])\n",
    "            window = trajectories_df[(trajectories_df['frame'] >= i - tail_length) & (trajectories_df['frame'] <= i)]\n",
    "            now = window[window['frame'] == i]\n",
    "            if len(now) > 0:\n",
    "                coords = np.column_stack((now.x.values - x_min, now.y.values - y_min))\n",
    "                dot.set_offsets(coords)\n",
    "            else:\n",
    "                dot.set_offsets(np.empty((0, 2)))\n",
    "            for pid in particles:\n",
    "                traj = window[window['track_id'] == pid].sort_values('frame')\n",
    "                if len(traj) >= 2:\n",
    "                    segs = [[(x0 - x_min, y0 - y_min), (x1 - x_min, y1 - y_min)] for (x0, y0, x1, y1) in zip(traj.x.values[:-1], traj.y.values[:-1], traj.x.values[1:], traj.y.values[1:])]\n",
    "                    line_collections[pid].set_segments(segs)\n",
    "                else:\n",
    "                    line_collections[pid].set_segments([])\n",
    "            return [im, dot] + list(line_collections.values())\n",
    "        ani = FuncAnimation(fig, animate, frames=cropped_stack.shape[0], interval=100, blit=True)\n",
    "        plt.close(fig)\n",
    "        return HTML(ani.to_jshtml())\n",
    "    html = animate_trajectories_cropped(tracks_in_roi, image_stack, tail_length, color)\n",
    "    display(html); display(HTML(replace_loading_js_empty))\n",
    "    print(\"Total number of trajectories in ROI:\", len(tracks_in_roi['track_id'].unique()))\n",
    "\n",
    "\n",
    "def visualize_model_on_dataset(model, dataset, device, num_samples=4, threshold=0.5, sigma=1.0):\n",
    "    model.eval(); loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    with torch.no_grad():\n",
    "        for i, (img_tensor, mask_tensor) in enumerate(loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            img_tensor = img_tensor.to(device); mask_tensor = mask_tensor.to(device)\n",
    "            logits = model(img_tensor)\n",
    "            prob_map = torch.sigmoid(logits[0, 0]).cpu().numpy()\n",
    "            if sigma > 0:\n",
    "                prob_map = gaussian_filter(prob_map, sigma=sigma)\n",
    "            pred_mask = (prob_map >= threshold).astype(float)\n",
    "            img = img_tensor[0, 0].cpu().numpy(); gt_mask = mask_tensor[0, 0].cpu().numpy()\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(14, 4))\n",
    "            axes[0].imshow(img, cmap='magma'); axes[0].set_title(\"Input Image\")\n",
    "            axes[1].imshow(gt_mask, cmap='gray', vmin=0, vmax=1); axes[1].set_title(\"Ground Truth Mask\")\n",
    "            axes[2].imshow(prob_map, cmap='viridis'); axes[2].set_title(\"Predicted Heatmap\")\n",
    "            axes[3].imshow(pred_mask, cmap='gray', vmin=0, vmax=1); axes[3].set_title(f\"Thresholded Output (> {threshold})\")\n",
    "            for ax in axes:\n",
    "                ax.axis('off')\n",
    "            plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "def show_detections(detections_per_frame, image_stack, y_min=512, y_max=768, x_min=256, x_max=512, color='yellow', max_frames=5):\n",
    "    rows = [(frame_idx, x, y) for frame_idx, dets in enumerate(detections_per_frame) for (x, y) in dets]\n",
    "    detections_df = pd.DataFrame(rows, columns=[\"frame\", \"x\", \"y\"])\n",
    "    detections_df = detections_df[(detections_df.y.between(y_min, y_max)) & (detections_df.x.between(x_min, x_max))]\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(image_stack[0], cmap='magma')\n",
    "    rect = Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='cyan', facecolor='none')\n",
    "    ax.add_patch(rect); ax.set_title(\"Full image (cyan box shows cropped region)\")\n",
    "    plt.show()\n",
    "    def animate_detections_cropped(detections_df, image_stack, color='yellow'):\n",
    "        cropped_stack = image_stack[:, y_min:y_max, x_min:x_max]\n",
    "        fig, ax = plt.subplots(); im = ax.imshow(cropped_stack[0], cmap='magma')\n",
    "        dot = ax.scatter([], [], s=10, c=color)\n",
    "        def animate(i):\n",
    "            im.set_array(cropped_stack[i])\n",
    "            now = detections_df[detections_df['frame'] == i]\n",
    "            if len(now) > 0:\n",
    "                coords = np.column_stack((now.x.values - x_min, now.y.values - y_min))\n",
    "                dot.set_offsets(coords)\n",
    "            else:\n",
    "                dot.set_offsets(np.empty((0, 2)))\n",
    "            return [im, dot]\n",
    "        ani = FuncAnimation(fig, animate, frames=min(max_frames, cropped_stack.shape[0]), interval=1000, blit=True)\n",
    "        plt.close(fig); return HTML(ani.to_jshtml())\n",
    "    html = animate_detections_cropped(detections_df, np.array(image_stack), color)\n",
    "    display(html); print(f\"Total detections in ROI : {len(detections_df)} \n",
    "Showing first {max_frames} frames\")\n",
    "\n",
    "\n",
    "def calculate_performance(gt_path, tracks, y_min=512, y_max=768, x_min=256, x_max=512, name=\"Method\"):\n",
    "    val_gt = pd.read_csv(gt_path)\n",
    "    val_gt = val_gt.groupby('track_id').filter(lambda t: (y_min < t.y.mean() < y_max) and (x_min < t.x.mean() < x_max))\n",
    "    if isinstance(tracks, str):\n",
    "        val_tracks = pd.read_csv(tracks)\n",
    "    elif isinstance(tracks, pd.DataFrame):\n",
    "        val_tracks = tracks.copy()\n",
    "    else:\n",
    "        raise TypeError(\"`tracks` must be a CSV path or a pandas DataFrame.\")\n",
    "    val_tracks = val_tracks.groupby('track_id').filter(lambda t: (y_min < t.y.mean() < y_max) and (x_min < t.x.mean() < x_max))\n",
    "    results = hota(val_gt, val_tracks)\n",
    "    print(f\"{name}:\"); print(f\"  HOTA: {results['HOTA']:.2f} (AssA: {results['AssA']:.2f}, DetA: {results['DetA']:.2f})\n",
    "\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def open_tiff_file(name: str) -> np.ndarray:\n",
    "    img = Image.open(name); frames = []\n",
    "    for i in range(img.n_frames):\n",
    "        img.seek(i); frames.append(np.array(img))\n",
    "    return np.array(frames).squeeze()\n",
    "\n",
    "\n",
    "def loading_html(message: str) -> str:\n",
    "    return f\"\"\"\n",
    "<div id=\"loading-msg\">\n",
    "  <br /><br />\n",
    "  <b><span style='display:inline-block;animation:flipPause 2s ease infinite;'>?</span>\n",
    "  {message}</b>\n",
    "</div>\n",
    "<style>\n",
    "@keyframes flipPause {{\n",
    "  0% {{transform:rotate(0deg);}}\n",
    "  40%{{transform:rotate(180deg);}}\n",
    "  50%{{transform:rotate(180deg);}}\n",
    "  90%{{transform:rotate(360deg);}}\n",
    "  100%{{transform:rotate(360deg);}}\n",
    "}}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "html_code_reconstruction = loading_html(\"Showing input validation data, please wait...\")\n",
    "\n",
    "\n",
    "def replace_loading_js(message: str, delay_ms: int = 0) -> str:\n",
    "    return f\"\"\"\n",
    "<script>\n",
    "  setTimeout(function(){{\n",
    "    var loadingDiv = document.getElementById(\"loading-msg\");\n",
    "    if (loadingDiv) {{\n",
    "      loadingDiv.innerHTML = '<br /><b>{message}</b>';\n",
    "    }}\n",
    "  }}, {delay_ms});\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "replace_loading_js_default = replace_loading_js(\"Only the first 50 frames are displayed.\")\n",
    "replace_loading_js_empty = replace_loading_js(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43a51d7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2230,
     "status": "ok",
     "timestamp": 1764766177704,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "43a51d7b",
    "outputId": "81664adf-0ccd-4d8f-9e17-ffa15f840627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data already present.\n",
      "?? Mounting Drive...\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "'real_training_data' already exists and is not empty. Skipping copy.\n"
     ]
    }
   ],
   "source": [
    "# @title Download / Fetch Data (run once)\n",
    "AUTO_FETCH_DRIVE = True  # set True to copy training data from Drive\n",
    "DRIVE_SOURCE_PATH = \"/content/drive/MyDrive/unet_train\"\n",
    "TARGET_DIR = \"real_training_data\"\n",
    "\n",
    "# Download validation data if missing\n",
    "if not (Path(\"val_data/val.tif\").exists() and Path(\"val_data/val.csv\").exists()):\n",
    "    download_validation_data(target_dir=\"val_data\")\n",
    "else:\n",
    "    print(\"Validation data already present.\")\n",
    "\n",
    "# Optional Drive copy for training data\n",
    "if AUTO_FETCH_DRIVE:\n",
    "    fetch_from_drive(DRIVE_SOURCE_PATH=DRIVE_SOURCE_PATH, TARGET_DIR=TARGET_DIR)\n",
    "else:\n",
    "    print(\"Skipping Drive fetch (AUTO_FETCH_DRIVE=False). Ensure real_training_data/ exists with images/ and masks/.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4eb1fce",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1764766941556,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "f4eb1fce"
   },
   "outputs": [],
   "source": [
    "# This cell was a duplicate of ebc8af67 and has been removed to prevent definition conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99396f36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 42705,
     "status": "error",
     "timestamp": 1764767385770,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "99396f36",
    "outputId": "86af8a7e-7ca8-4ca6-b381-bca89d4b5696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset written to /content/experiment_dataset\n",
      "Bonus samples: 42 | Video frames: 120\n",
      "Frames: 120, Bonus: 42 (use_bonus=True)\n",
      "\n",
      "========== Fold 1 / 5 ==========\n",
      "Train loader: video 96 + bonus 42 = 138\n",
      "Val loader: 24\n",
      "Epoch 001: train nan | val nan | best inf\n",
      "Epoch 002: train nan | val nan | best inf\n",
      "Epoch 003: train nan | val nan | best inf\n",
      "Epoch 004: train nan | val nan | best inf\n",
      "Epoch 005: train nan | val nan | best inf\n",
      "Saved best checkpoint (val_loss=inf) to model_fold_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2025/12/03 01:09:33 PM] Loaded btrack: /usr/local/lib/python3.12/dist-packages/btrack/libs/libtracker.so\n",
      "INFO:btrack.libwrapper:Loaded btrack: /usr/local/lib/python3.12/dist-packages/btrack/libs/libtracker.so\n",
      "[INFO][2025/12/03 01:09:33 PM] Starting BayesianTracker session\n",
      "INFO:btrack.core:Starting BayesianTracker session\n",
      "[INFO][2025/12/03 01:09:33 PM] Loading configuration file: /root/.cache/btrack-examples/examples/cell_config.json\n",
      "INFO:btrack.config:Loading configuration file: /root/.cache/btrack-examples/examples/cell_config.json\n",
      "[INFO][2025/12/03 01:09:33 PM] Objects are of type: <class 'pandas.core.frame.DataFrame'>\n",
      "INFO:btrack.io.utils:Objects are of type: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using btrack.datasets.cell_config() (btrack>=0.6.x)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING][2025/12/03 01:09:33 PM] `track_interactive` will be deprecated. Use `track` instead.\n",
      "WARNING:btrack.core:`track_interactive` will be deprecated. Use `track` instead.\n",
      "[INFO][2025/12/03 01:09:33 PM] Starting tracking... \n",
      "INFO:btrack.core:Starting tracking... \n",
      "[INFO][2025/12/03 01:09:33 PM] Update using: ['MOTION']\n",
      "INFO:btrack.core:Update using: ['MOTION']\n",
      "[INFO][2025/12/03 01:09:33 PM] Tracking objects in frames 0 to 99 (of 110)...\n",
      "INFO:btrack.core:Tracking objects in frames 0 to 99 (of 110)...\n",
      "[INFO][2025/12/03 01:09:39 PM]  - Timing (Bayesian updates: 222.52ms, Linking: 2.86ms)\n",
      "INFO:btrack.utils: - Timing (Bayesian updates: 222.52ms, Linking: 2.86ms)\n",
      "[INFO][2025/12/03 01:09:39 PM]  - Probabilities (Link: 0.09724, Lost: 0.10917)\n",
      "INFO:btrack.utils: - Probabilities (Link: 0.09724, Lost: 0.10917)\n",
      "[INFO][2025/12/03 01:09:39 PM]  - Stats (Active: 616, Lost: 8530, Conflicts resolved: 686)\n",
      "INFO:btrack.utils: - Stats (Active: 616, Lost: 8530, Conflicts resolved: 686)\n",
      "[INFO][2025/12/03 01:09:39 PM] Tracking objects in frames 100 to 110 (of 110)...\n",
      "INFO:btrack.core:Tracking objects in frames 100 to 110 (of 110)...\n",
      "[INFO][2025/12/03 01:09:39 PM]  - Timing (Bayesian updates: 315.83ms, Linking: 4.64ms)\n",
      "INFO:btrack.utils: - Timing (Bayesian updates: 315.83ms, Linking: 4.64ms)\n",
      "[INFO][2025/12/03 01:09:39 PM]  - Probabilities (Link: 0.09724, Lost: 0.16581)\n",
      "INFO:btrack.utils: - Probabilities (Link: 0.09724, Lost: 0.16581)\n",
      "[INFO][2025/12/03 01:09:39 PM] SUCCESS.\n",
      "INFO:btrack.core:SUCCESS.\n",
      "[INFO][2025/12/03 01:09:39 PM]  - Found 13597 tracks in 110 frames (in 0.0s)\n",
      "INFO:btrack.core: - Found 13597 tracks in 110 frames (in 0.0s)\n",
      "[INFO][2025/12/03 01:09:39 PM]  - Inserted 174 dummy objects to fill tracking gaps\n",
      "INFO:btrack.core: - Inserted 174 dummy objects to fill tracking gaps\n",
      "[INFO][2025/12/03 01:09:39 PM] Loading hypothesis model: cell_hypothesis\n",
      "INFO:btrack.core:Loading hypothesis model: cell_hypothesis\n",
      "[INFO][2025/12/03 01:09:39 PM] Calculating hypotheses (relax: True)...\n",
      "INFO:btrack.core:Calculating hypotheses (relax: True)...\n",
      "[INFO][2025/12/03 01:09:46 PM] Setting up constraints matrix for global optimisation...\n",
      "INFO:btrack.optimise.optimiser:Setting up constraints matrix for global optimisation...\n",
      "[INFO][2025/12/03 01:09:48 PM] Ending BayesianTracker session\n",
      "INFO:btrack.core:Ending BayesianTracker session\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1532669653.py\u001b[0m in \u001b[0;36mrun_btrack_tracking\u001b[0;34m(oof_csv, output_csv, config_path, max_search_radius)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mtracks_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/btrack/core.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;34m\"\"\"Proxy for `optimise` for our American friends ;)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/btrack/core.py\u001b[0m in \u001b[0;36moptimise\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mtrack_linker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhypotheses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0mselected_hypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack_linker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0moptimised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhypotheses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_hypotheses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/btrack/optimise/optimiser.py\u001b[0m in \u001b[0;36moptimise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2894534607.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# 2) Train K-fold, infer OOF, compute DetA, (optionally) run btrack, visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m oof_df, model_paths, tracks_df = run_grand_kfold(\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mout_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mk_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK_SPLITS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1532669653.py\u001b[0m in \u001b[0;36mrun_grand_kfold\u001b[0;34m(out_dir, k_splits, use_bonus, epochs, batch_size, lr, num_workers, threshold, save_dir, visualize_samples, sample_idx, match_thresh, run_btrack, btrack_config, btrack_radius, train_aug_kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Only run btrack and calculate performance if there are actual detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfold_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mtracks_df_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_btrack_tracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_pred_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"tracks_fold_{fold}.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbtrack_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_search_radius\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatch_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;31m# Check if tracks_df_fold is not empty before calculating performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracks_df_fold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1532669653.py\u001b[0m in \u001b[0;36mrun_btrack_tracking\u001b[0;34m(oof_csv, output_csv, config_path, max_search_radius)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No btrack config available. Pass `config_path` explicitly when using btrack>=0.6.5.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mbtrack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBayesianTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_search_radius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_search_radius\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/btrack/core.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ending BayesianTracker session\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdel_interface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# @title Run Full Pipeline (edit params as needed)\n",
    "# Paths\n",
    "REAL_DATA_DIR = \"real_training_data\"\n",
    "VAL_TIF = \"val_data/val.tif\"\n",
    "VAL_CSV = \"val_data/val.csv\"\n",
    "OUT_DIR = \"experiment_dataset\"\n",
    "SAVE_DIR = \".\"\n",
    "\n",
    "# Pipeline knobs\n",
    "K_SPLITS = 5\n",
    "USE_BONUS = True          # set False to ignore bonus set\n",
    "EPOCHS = 5               # Increased epochs to give more time for training convergence\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-5                # Reduced learning rate further to combat persistent NaN loss\n",
    "THRESHOLD = 0.5           # sigmoid->mask threshold for detections\n",
    "MATCH_THRESH = 5.0        # px for DetA calculation\n",
    "VISUALIZE = True          # show sample masks from all folds\n",
    "SAMPLE_IDX = 0            # which sample to visualize\n",
    "RUN_BTRACK = True         # Toggle btrack tracking ON for testing\n",
    "BTRACK_CONFIG = None      # None -> use btrack.datasets.cell_config() default (btrack>=0.6.x)\n",
    "BTRACK_RADIUS = 12.0      # max_search_radius\n",
    "USE_TRAIN_AUG = True      # set False to disable train-time augmentations\n",
    "\n",
    "# Augmentation knobs\n",
    "TRAIN_AUG_KWARTS = dict(\n",
    "    rotate_p=0.7,\n",
    "    hflip_p=0.5,\n",
    "    vflip_p=0.5,\n",
    "    clahe_p=0.5,\n",
    "    brightness_p=0.5,\n",
    "    gauss_p=0.3,\n",
    "    elastic_p=0.2,\n",
    "    coarse_p=0.5,\n",
    "    coarse_max_holes=16,\n",
    "    coarse_min_holes=8,\n",
    "    coarse_max_hw=16,\n",
    "    coarse_min_hw=8,\n",
    "    crop_scale_min=0.8,\n",
    "    crop_scale_max=1.0,\n",
    "    crop_ratio_min=0.9,\n",
    "    crop_ratio_max=1.1,\n",
    ")\n",
    "\n",
    "# 1) Build grand dataset (cleans OUT_DIR each time)\n",
    "prepare_grand_dataset(\n",
    "    real_data_dir=REAL_DATA_DIR,\n",
    "    val_tif_path=VAL_TIF,\n",
    "    val_csv_path=VAL_CSV,\n",
    "    out_dir=OUT_DIR,\n",
    ")\n",
    "\n",
    "# 2) Train K-fold, infer OOF, compute DetA, (optionally) run btrack, visualize\n",
    "oof_df, model_paths, tracks_df = run_grand_kfold(\n",
    "    out_dir=OUT_DIR,\n",
    "    k_splits=K_SPLITS,\n",
    "    use_bonus=USE_BONUS,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    threshold=THRESHOLD,\n",
    "    save_dir=SAVE_DIR,\n",
    "    visualize_samples=VISUALIZE,\n",
    "    sample_idx=SAMPLE_IDX,\n",
    "    match_thresh=MATCH_THRESH,\n",
    "    run_btrack=RUN_BTRACK,\n",
    "    btrack_config=BTRACK_CONFIG,\n",
    "    btrack_radius=BTRACK_RADIUS,\n",
    "    train_aug_kwargs=TRAIN_AUG_KWARTS if USE_TRAIN_AUG else {},\n",
    ")\n",
    "\n",
    "print(\"Done. OOF detections:\", len(oof_df), \"| Models:\", model_paths)\n",
    "if tracks_df is not None:\n",
    "    print(\"BTrack tracks:\", len(tracks_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a08adf",
   "metadata": {
    "executionInfo": {
     "elapsed": 9970,
     "status": "aborted",
     "timestamp": 1764766382476,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "02a08adf"
   },
   "outputs": [],
   "source": [
    "# @title Compare Baselines vs UNet (OOF tracked)\n",
    "# Assumes you already ran the UNet pipeline (oof_predictions.csv + tracks_btrack.csv) and baseline outputs exist.\n",
    "# Adjust baseline paths as needed.\n",
    "\n",
    "BASE_GT = \"val_data/val.csv\"\n",
    "BASE_SOTA = \"val_data/sota.csv\"          # example baseline detections/tracks\n",
    "BASE_SIMPLE = None                        # set to CSV if available\n",
    "UNET_OOF = \"tracks_btrack.csv\"           # produced by run_grand_kfold when run_btrack=True\n",
    "\n",
    "# Load GT and filter to ROI\n",
    "roi_tuple = (ROI_Y_MIN, ROI_Y_MAX, ROI_X_MIN, ROI_X_MAX)\n",
    "def _load_and_filter(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df[(df.y.between(ROI_Y_MIN, ROI_Y_MAX)) & (df.x.between(ROI_X_MIN, ROI_X_MAX))]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Baseline: SOTA\n",
    "if BASE_SOTA and Path(BASE_SOTA).exists():\n",
    "    res_sota = calculate_performance(BASE_GT, BASE_SOTA, y_min=ROI_Y_MIN, y_max=ROI_Y_MAX, x_min=ROI_X_MIN, x_max=ROI_X_MAX, name=\"SOTA\")\n",
    "    results.append((\"SOTA\", res_sota))\n",
    "\n",
    "# Baseline: Simple\n",
    "if BASE_SIMPLE and Path(BASE_SIMPLE).exists():\n",
    "    res_simple = calculate_performance(BASE_GT, BASE_SIMPLE, y_min=ROI_Y_MIN, y_max=ROI_Y_MAX, x_min=ROI_X_MIN, x_max=ROI_X_MAX, name=\"Simple\" )\n",
    "    results.append((\"Simple\", res_simple))\n",
    "\n",
    "# UNet OOF (tracked with btrack)\n",
    "if Path(UNET_OOF).exists():\n",
    "    res_unet = calculate_performance(BASE_GT, UNET_OOF, y_min=ROI_Y_MIN, y_max=ROI_Y_MAX, x_min=ROI_X_MIN, x_max=ROI_X_MAX, name=\"UNet OOF\")\n",
    "    results.append((\"UNet OOF\", res_unet))\n",
    "else:\n",
    "    print(\"UNet tracked output not found; run k-fold pipeline with run_btrack=True\")\n",
    "\n",
    "print(\"\\nSummary (HOTA/DetA):\")\n",
    "for name, res in results:\n",
    "    print(f\"{name}: HOTA={res['HOTA']:.3f}, DetA={res['DetA']:.3f}, AssA={res['AssA']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f1479",
   "metadata": {
    "id": "4f5f1479"
   },
   "source": [
    "?? Environment requirements: see [`requirements.txt`](requirements.txt) for pinned versions (btrack, pydantic<2, numpy, albumentations, etc.). Run the setup cell to install from this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MZhx1u_sNPUR",
   "metadata": {
    "executionInfo": {
     "elapsed": 9412,
     "status": "aborted",
     "timestamp": 1764766382488,
     "user": {
      "displayName": "Matyáš Veselý",
      "userId": "14309958602951870321"
     },
     "user_tz": -60
    },
    "id": "MZhx1u_sNPUR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
