{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/veselm73/SU2/blob/main/notebooks/SU2_StarDist_export.ipynb)\n",
    "\n",
    "# StarDist Competition Export & Inference\n",
    "\n",
    "This notebook exports trained StarDist models for competition submission and provides an easy-to-use inference API.\n",
    "\n",
    "**Features:**\n",
    "- Export all K-fold models for ensemble inference\n",
    "- Save model and inference configurations as JSON\n",
    "- Single-function inference API: `infer_video(path) -> DataFrame`\n",
    "- Visualization of predictions\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run `SU2_StarDist_final.ipynb` first to train models\n",
    "- Trained model checkpoints in `results/stardist/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies using uv (much faster than pip)\n",
    "!pip install uv -q\n",
    "\n",
    "# Uninstall conflicting packages  \n",
    "!uv pip uninstall torch torchvision torchaudio tensorflow tensorflow-metal --system -q 2>/dev/null || true\n",
    "\n",
    "# Install PyTorch with CUDA 12.1\n",
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --system -q\n",
    "\n",
    "# Install other dependencies\n",
    "!uv pip install \"numpy<2\" cellseg-models-pytorch pytorch-lightning laptrack tifffile --system -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path('/content/SU2').exists():\n",
    "        !git clone https://github.com/veselm73/SU2.git /content/SU2\n",
    "    else:\n",
    "        !cd /content/SU2 && git pull\n",
    "    os.chdir('/content/SU2')\n",
    "    repo_root = Path('/content/SU2')\n",
    "else:\n",
    "    notebook_dir = Path(os.getcwd())\n",
    "    if notebook_dir.name == 'notebooks':\n",
    "        repo_root = notebook_dir.parent\n",
    "    else:\n",
    "        repo_root = notebook_dir\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"Repository root: {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import from stardist_helpers\n",
    "from modules.stardist_helpers import (\n",
    "    StarDistLightning,\n",
    "    run_laptrack,\n",
    "    ROI_X_MIN, ROI_X_MAX, ROI_Y_MIN, ROI_Y_MAX\n",
    ")\n",
    "\n",
    "try:\n",
    "    from cellseg_models_pytorch.postproc.functional.stardist.stardist import post_proc_stardist\n",
    "    HAS_STARDIST = True\n",
    "except ImportError:\n",
    "    HAS_STARDIST = False\n",
    "    print(\"Warning: cellseg_models_pytorch not available\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Paths to trained models (from SU2_StarDist_final.ipynb)\n",
    "MODELS_DIR = repo_root / \"results\" / \"stardist\"\n",
    "EXPORT_DIR = repo_root / \"results\" / \"stardist\" / \"competition\"\n",
    "\n",
    "# Model architecture (must match training)\n",
    "MODEL_CONFIG = {\n",
    "    \"architecture\": \"StarDist\",\n",
    "    \"encoder_name\": \"resnet18\",\n",
    "    \"n_rays\": 32,\n",
    "    \"input_channels\": 1,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "# Inference configuration (optimal values from training)\n",
    "INFERENCE_CONFIG = {\n",
    "    \"prob_thresh\": 0.3,\n",
    "    \"nms_thresh\": 0.3,\n",
    "    \"track_cost_cutoff\": 49,           # 7px squared\n",
    "    \"gap_closing_cost_cutoff\": 25,     # 5px squared\n",
    "    \"gap_closing_max_frame_count\": 2,\n",
    "    \"roi\": {\n",
    "        \"x_min\": ROI_X_MIN,\n",
    "        \"x_max\": ROI_X_MAX,\n",
    "        \"y_min\": ROI_Y_MIN,\n",
    "        \"y_max\": ROI_Y_MAX\n",
    "    }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "K_FOLDS = 5\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Models dir: {MODELS_DIR}\")\n",
    "print(f\"  Export dir: {EXPORT_DIR}\")\n",
    "print(f\"  Model: {MODEL_CONFIG['encoder_name']}, n_rays={MODEL_CONFIG['n_rays']}\")\n",
    "print(f\"  Thresholds: prob={INFERENCE_CONFIG['prob_thresh']}, nms={INFERENCE_CONFIG['nms_thresh']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path, config=MODEL_CONFIG):\n",
    "    \"\"\"\n",
    "    Load a StarDist model from a Lightning checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to .ckpt file\n",
    "        config: Model configuration dict\n",
    "    \n",
    "    Returns:\n",
    "        Loaded model in eval mode\n",
    "    \"\"\"\n",
    "    model = StarDistLightning(\n",
    "        n_rays=config['n_rays'],\n",
    "        encoder_name=config['encoder_name'],\n",
    "        dropout=config.get('dropout', 0.0)\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model_from_state_dict(state_dict_path, config=MODEL_CONFIG):\n",
    "    \"\"\"\n",
    "    Load a StarDist model from a state_dict .pth file.\n",
    "    \n",
    "    Args:\n",
    "        state_dict_path: Path to .pth file\n",
    "        config: Model configuration dict\n",
    "    \n",
    "    Returns:\n",
    "        Loaded model in eval mode\n",
    "    \"\"\"\n",
    "    model = StarDistLightning(\n",
    "        n_rays=config['n_rays'],\n",
    "        encoder_name=config['encoder_name'],\n",
    "        dropout=config.get('dropout', 0.0)\n",
    "    )\n",
    "    \n",
    "    state_dict = torch.load(state_dict_path, map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all K-fold models\n",
    "print(\"Loading K-fold models...\")\n",
    "fold_models = []\n",
    "\n",
    "for fold in range(1, K_FOLDS + 1):\n",
    "    # Try checkpoint first, then state_dict\n",
    "    ckpt_path = MODELS_DIR / f\"stardist_fold_{fold}\" / \"best_model.ckpt\"\n",
    "    pth_path = EXPORT_DIR / \"models\" / f\"fold_{fold}.pth\"\n",
    "    \n",
    "    if ckpt_path.exists():\n",
    "        model = load_model_from_checkpoint(ckpt_path)\n",
    "        fold_models.append(model)\n",
    "        print(f\"  Fold {fold}: Loaded from {ckpt_path}\")\n",
    "    elif pth_path.exists():\n",
    "        model = load_model_from_state_dict(pth_path)\n",
    "        fold_models.append(model)\n",
    "        print(f\"  Fold {fold}: Loaded from {pth_path}\")\n",
    "    else:\n",
    "        print(f\"  Fold {fold}: NOT FOUND (expected at {ckpt_path})\")\n",
    "\n",
    "print(f\"\\nLoaded {len(fold_models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export Competition Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create export directory structure\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(EXPORT_DIR / \"models\").mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Export directory: {EXPORT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all fold models as state_dict (.pth)\n",
    "print(\"Exporting fold models...\")\n",
    "\n",
    "for i, model in enumerate(fold_models, start=1):\n",
    "    save_path = EXPORT_DIR / \"models\" / f\"fold_{i}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"  Saved: {save_path}\")\n",
    "\n",
    "print(f\"\\nExported {len(fold_models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export configuration files\n",
    "print(\"Exporting configuration files...\")\n",
    "\n",
    "# Model config\n",
    "model_config_path = EXPORT_DIR / \"model_config.json\"\n",
    "with open(model_config_path, 'w') as f:\n",
    "    json.dump(MODEL_CONFIG, f, indent=2)\n",
    "print(f\"  Saved: {model_config_path}\")\n",
    "\n",
    "# Inference config\n",
    "inference_config_path = EXPORT_DIR / \"inference_config.json\"\n",
    "with open(inference_config_path, 'w') as f:\n",
    "    json.dump(INFERENCE_CONFIG, f, indent=2)\n",
    "print(f\"  Saved: {inference_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Easy-to-Use Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_frame(model, frame, prob_thresh=0.3, nms_thresh=0.3, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Run StarDist inference on a single frame.\n",
    "    \n",
    "    Args:\n",
    "        model: StarDistLightning model\n",
    "        frame: 2D numpy array (H, W)\n",
    "        prob_thresh: Probability threshold for detection\n",
    "        nms_thresh: NMS threshold\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        List of (x, y) detection coordinates\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Normalize frame\n",
    "    frame_norm = (frame - frame.mean()) / (frame.std() + 1e-8)\n",
    "    \n",
    "    # Prepare tensor\n",
    "    x = torch.from_numpy(frame_norm).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_dist, pred_bin = model.model(x)\n",
    "        pred_dist = pred_dist.cpu().numpy()[0]\n",
    "        pred_bin = torch.sigmoid(pred_bin).cpu().numpy()[0, 0]\n",
    "    \n",
    "    # Post-process\n",
    "    if HAS_STARDIST:\n",
    "        labels = post_proc_stardist(\n",
    "            pred_dist, pred_bin,\n",
    "            prob_thresh=prob_thresh,\n",
    "            nms_thresh=nms_thresh\n",
    "        )\n",
    "        \n",
    "        # Extract centroids\n",
    "        from skimage.measure import regionprops\n",
    "        detections = []\n",
    "        for prop in regionprops(labels):\n",
    "            y, x = prop.centroid\n",
    "            detections.append((x, y))\n",
    "        return detections\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def infer_frame_ensemble(models, frame, prob_thresh=0.3, nms_thresh=0.3, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Run ensemble inference on a single frame.\n",
    "    Averages probability maps from all models before post-processing.\n",
    "    \n",
    "    Args:\n",
    "        models: List of StarDistLightning models\n",
    "        frame: 2D numpy array (H, W)\n",
    "        prob_thresh: Probability threshold for detection\n",
    "        nms_thresh: NMS threshold\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        List of (x, y) detection coordinates\n",
    "    \"\"\"\n",
    "    # Normalize frame\n",
    "    frame_norm = (frame - frame.mean()) / (frame.std() + 1e-8)\n",
    "    x = torch.from_numpy(frame_norm).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Collect predictions from all models\n",
    "    all_dist = []\n",
    "    all_bin = []\n",
    "    \n",
    "    for model in models:\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_dist, pred_bin = model.model(x)\n",
    "            all_dist.append(pred_dist.cpu().numpy()[0])\n",
    "            all_bin.append(torch.sigmoid(pred_bin).cpu().numpy()[0, 0])\n",
    "    \n",
    "    # Average predictions\n",
    "    avg_dist = np.mean(all_dist, axis=0)\n",
    "    avg_bin = np.mean(all_bin, axis=0)\n",
    "    \n",
    "    # Post-process\n",
    "    if HAS_STARDIST:\n",
    "        labels = post_proc_stardist(\n",
    "            avg_dist, avg_bin,\n",
    "            prob_thresh=prob_thresh,\n",
    "            nms_thresh=nms_thresh\n",
    "        )\n",
    "        \n",
    "        from skimage.measure import regionprops\n",
    "        detections = []\n",
    "        for prop in regionprops(labels):\n",
    "            y, x = prop.centroid\n",
    "            detections.append((x, y))\n",
    "        return detections\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_video(video_path, output_csv=None, use_ensemble=True, \n",
    "                models=None, config=None, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Run detection + tracking on a video file.\n",
    "    \n",
    "    This is the main inference API for competition submission.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to TIFF video file\n",
    "        output_csv: Optional path to save predictions CSV\n",
    "        use_ensemble: Use all fold models (True) or first model only (False)\n",
    "        models: List of models (uses global fold_models if None)\n",
    "        config: Inference config dict (uses INFERENCE_CONFIG if None)\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: frame, x, y, track_id\n",
    "    \n",
    "    Example:\n",
    "        >>> predictions = infer_video('test.tif')\n",
    "        >>> predictions.to_csv('submission.csv', index=False)\n",
    "    \"\"\"\n",
    "    # Use defaults if not provided\n",
    "    if models is None:\n",
    "        models = fold_models\n",
    "    if config is None:\n",
    "        config = INFERENCE_CONFIG\n",
    "    \n",
    "    if len(models) == 0:\n",
    "        raise ValueError(\"No models loaded. Run model loading cell first.\")\n",
    "    \n",
    "    # Load video\n",
    "    print(f\"Loading video: {video_path}\")\n",
    "    video = tifffile.imread(video_path)\n",
    "    print(f\"  Shape: {video.shape}\")\n",
    "    \n",
    "    # Extract ROI\n",
    "    roi = config['roi']\n",
    "    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    print(f\"  ROI: y=[{roi['y_min']}, {roi['y_max']}], x=[{roi['x_min']}, {roi['x_max']}]\")\n",
    "    \n",
    "    # Run inference\n",
    "    prob_thresh = config['prob_thresh']\n",
    "    nms_thresh = config['nms_thresh']\n",
    "    \n",
    "    print(f\"Running inference (ensemble={use_ensemble})...\")\n",
    "    all_detections = []\n",
    "    \n",
    "    for frame_idx in tqdm(range(len(video_roi)), desc=\"Detecting\"):\n",
    "        frame = video_roi[frame_idx].astype(np.float32)\n",
    "        \n",
    "        if use_ensemble and len(models) > 1:\n",
    "            dets = infer_frame_ensemble(models, frame, prob_thresh, nms_thresh, device)\n",
    "        else:\n",
    "            dets = infer_frame(models[0], frame, prob_thresh, nms_thresh, device)\n",
    "        \n",
    "        for x, y in dets:\n",
    "            all_detections.append({\n",
    "                'frame': frame_idx,\n",
    "                'x': x + roi['x_min'],  # Convert back to full image coords\n",
    "                'y': y + roi['y_min']\n",
    "            })\n",
    "    \n",
    "    detections_df = pd.DataFrame(all_detections)\n",
    "    print(f\"  Detections: {len(detections_df)}\")\n",
    "    \n",
    "    if detections_df.empty:\n",
    "        return pd.DataFrame(columns=['frame', 'x', 'y', 'track_id'])\n",
    "    \n",
    "    # Run tracking\n",
    "    print(\"Running tracking...\")\n",
    "    tracked_df = run_laptrack(\n",
    "        detections_df,\n",
    "        track_cost_cutoff=config['track_cost_cutoff'],\n",
    "        gap_closing_cost_cutoff=config['gap_closing_cost_cutoff'],\n",
    "        gap_closing_max_frame_count=config['gap_closing_max_frame_count']\n",
    "    )\n",
    "    print(f\"  Tracks: {tracked_df['track_id'].nunique()}\")\n",
    "    \n",
    "    # Save if requested\n",
    "    if output_csv:\n",
    "        tracked_df.to_csv(output_csv, index=False)\n",
    "        print(f\"  Saved: {output_csv}\")\n",
    "    \n",
    "    return tracked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Usage Example & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Run inference on validation video\n",
    "VAL_TIF = repo_root / \"data\" / \"val\" / \"val.tif\"\n",
    "\n",
    "if VAL_TIF.exists() and len(fold_models) > 0:\n",
    "    print(\"Running demo inference on validation video...\")\n",
    "    predictions = infer_video(\n",
    "        VAL_TIF,\n",
    "        output_csv=EXPORT_DIR / \"demo_predictions.csv\",\n",
    "        use_ensemble=True\n",
    "    )\n",
    "    print(f\"\\nDemo complete! Predictions shape: {predictions.shape}\")\n",
    "else:\n",
    "    print(\"Skipping demo: validation video not found or no models loaded\")\n",
    "    print(f\"  VAL_TIF exists: {VAL_TIF.exists()}\")\n",
    "    print(f\"  Models loaded: {len(fold_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions\n",
    "if VAL_TIF.exists() and 'predictions' in dir() and len(predictions) > 0:\n",
    "    video = tifffile.imread(VAL_TIF)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    \n",
    "    sample_frames = [0, 30, 60, 90]\n",
    "    fig, axes = plt.subplots(1, len(sample_frames), figsize=(16, 4))\n",
    "    \n",
    "    for ax, fidx in zip(axes, sample_frames):\n",
    "        if fidx >= len(video_roi):\n",
    "            continue\n",
    "        \n",
    "        ax.imshow(video_roi[fidx], cmap='gray')\n",
    "        \n",
    "        # Plot predictions\n",
    "        frame_preds = predictions[predictions.frame == fidx]\n",
    "        ax.scatter(\n",
    "            frame_preds.x - roi['x_min'],\n",
    "            frame_preds.y - roi['y_min'],\n",
    "            c='red', s=20, marker='x', alpha=0.8\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f'Frame {fidx} ({len(frame_preds)} detections)')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Ensemble Predictions', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPORT_DIR / 'demo_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of exported files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nExport directory: {EXPORT_DIR}\")\n",
    "print(\"\\nExported files:\")\n",
    "\n",
    "for item in sorted(EXPORT_DIR.rglob('*')):\n",
    "    if item.is_file():\n",
    "        rel_path = item.relative_to(EXPORT_DIR)\n",
    "        size_kb = item.stat().st_size / 1024\n",
    "        print(f\"  {rel_path} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"USAGE INSTRUCTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "To run inference on new data:\n",
    "\n",
    "1. Load this notebook or import the inference function\n",
    "2. Call: predictions = infer_video('path/to/test.tif')\n",
    "3. Save: predictions.to_csv('submission.csv', index=False)\n",
    "\n",
    "The output CSV will have columns: frame, x, y, track_id\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
