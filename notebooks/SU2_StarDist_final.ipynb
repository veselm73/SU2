{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StarDist Cell Detection Training Pipeline\n",
    "\n",
    "This notebook trains a StarDist model for cell detection using K-Fold cross-validation.\n",
    "\n",
    "**Key Features:**\n",
    "- StarDist architecture with configurable backbone (ResNet18/34/50, EfficientNet)\n",
    "- Combined loss function (Focal + Dice + Smooth L1)\n",
    "- Data augmentation with albumentations\n",
    "- Regularization: Weight decay + LR scheduling + Early stopping\n",
    "- Post-training threshold sweep for optimal prob_thresh/nms_thresh\n",
    "- Tracking evaluation with HOTA metric\n",
    "\n",
    "**Data Sources:**\n",
    "- Validation video: Downloaded from UTIA server\n",
    "- Bonus training data: Fetched from GitHub repository (annotated frames)\n",
    "\n",
    "**Outputs:**\n",
    "- Trained StarDist model (best fold)\n",
    "- Detection predictions CSV\n",
    "- Training curves and metrics\n",
    "- Tracking visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REPOSITORY CONFIGURATION\n",
    "# ============================================================\n",
    "# Set these if running from Colab or a different location\n",
    "\n",
    "REPO_URL = \"https://github.com/veselm73/SU2\"\n",
    "REPO_BRANCH = \"main\"\n",
    "\n",
    "# Path to bonus training data within the repo\n",
    "BONUS_DATA_SUBPATH = \"annotation/sam_data/unet_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies using uv (much faster than pip)\n",
    "!pip install uv -q\n",
    "\n",
    "# Uninstall conflicting packages  \n",
    "!uv pip uninstall torch torchvision torchaudio tensorflow tensorflow-metal --system -q 2>/dev/null || true\n",
    "\n",
    "# Install PyTorch with CUDA 12.1\n",
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --system -q\n",
    "\n",
    "# Install other dependencies (numpy<2 for compatibility)\n",
    "!uv pip install \"numpy<2\" cellseg-models-pytorch pytorch-lightning laptrack btrack \"albumentations>=1.3.1\" tifffile opencv-python-headless --system -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone repo if not already present, otherwise pull latest\n",
    "    if not Path('/content/SU2').exists():\n",
    "        !git clone https://github.com/veselm73/SU2.git /content/SU2\n",
    "    else:\n",
    "        !cd /content/SU2 && git pull\n",
    "    os.chdir('/content/SU2')\n",
    "    repo_root = Path('/content/SU2')\n",
    "else:\n",
    "    # Local setup - find repo root\n",
    "    notebook_dir = Path(os.getcwd())\n",
    "    if notebook_dir.name == 'notebooks':\n",
    "        repo_root = notebook_dir.parent\n",
    "    else:\n",
    "        repo_root = notebook_dir\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"Repository root: {repo_root}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions\n",
    "from modules.stardist_helpers import (\n",
    "    # Data fetching\n",
    "    download_validation_data,\n",
    "    fetch_training_data_from_github,\n",
    "    prepare_grand_dataset,\n",
    "    create_stardist_label_mask,\n",
    "    # Training\n",
    "    train_stardist_kfold,\n",
    "    infer_stardist_full_video,\n",
    "    sweep_stardist_thresholds,\n",
    "    # Metrics\n",
    "    calculate_deta_robust,\n",
    "    hota,\n",
    "    # Tracking\n",
    "    run_btrack_tracking,\n",
    "    run_laptrack,\n",
    "    run_tracking_sweep,\n",
    "    # Visualization\n",
    "    plot_training_history,\n",
    "    show_detection_overlay,\n",
    "    show_tracking_animation,\n",
    "    print_results_summary,\n",
    "    # Utilities\n",
    "    set_seed,\n",
    "    get_device,\n",
    "    ROI_X_MIN, ROI_X_MAX, ROI_Y_MIN, ROI_Y_MAX\n",
    ")\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "set_seed(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "This section downloads/fetches the required data:\n",
    "1. **Validation video**: From UTIA server (val.tif + val.csv)\n",
    "2. **Bonus training data**: From GitHub repository (annotated frames with masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "VAL_DIR = repo_root / \"data\" / \"val\"\n",
    "VAL_TIF = VAL_DIR / \"val.tif\"\n",
    "VAL_CSV = VAL_DIR / \"val.csv\"\n",
    "BONUS_DATA_DIR = repo_root / \"bonus_training_data\"\n",
    "\n",
    "print(\"Data paths:\")\n",
    "print(f\"  Validation: {VAL_DIR}\")\n",
    "print(f\"  Bonus data: {BONUS_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download validation data from UTIA server\n",
    "if not VAL_TIF.exists():\n",
    "    print(\"Downloading validation data from UTIA server...\")\n",
    "    download_validation_data(target_dir=str(VAL_DIR))\n",
    "else:\n",
    "    print(f\"Validation data exists: {VAL_TIF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch bonus training data from GitHub\n",
    "# This will:\n",
    "# 1. Check if data exists locally (in cloned repo)\n",
    "# 2. If not, download from GitHub API\n",
    "\n",
    "bonus_path = fetch_training_data_from_github(\n",
    "    repo_url=REPO_URL,\n",
    "    branch=REPO_BRANCH,\n",
    "    data_subpath=BONUS_DATA_SUBPATH,\n",
    "    target_dir=str(BONUS_DATA_DIR),\n",
    "    use_local_if_available=True\n",
    ")\n",
    "\n",
    "if bonus_path:\n",
    "    print(f\"\\nBonus data ready at: {bonus_path}\")\n",
    "else:\n",
    "    print(\"\\nWarning: Could not fetch bonus data. Training will use only video frames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset structure (creates experiment_dataset/)\n",
    "# This combines:\n",
    "# - Video frames cropped to ROI with generated disk masks\n",
    "# - Bonus annotated frames with instance masks\n",
    "\n",
    "DATASET_DIR = repo_root / \"experiment_dataset\"\n",
    "DISK_RADIUS = 6  # Radius for disk masks (StarDist needs instance labels)\n",
    "\n",
    "print(\"Preparing combined dataset...\")\n",
    "prepare_grand_dataset(\n",
    "    bonus_data_dir=str(BONUS_DATA_DIR) if bonus_path else None,\n",
    "    val_tif_path=str(VAL_TIF),\n",
    "    val_csv_path=str(VAL_CSV),\n",
    "    out_dir=str(DATASET_DIR),\n",
    "    disk_radius=DISK_RADIUS\n",
    ")\n",
    "\n",
    "VIDEO_MAP_PATH = DATASET_DIR / \"video_map.csv\"\n",
    "print(f\"\\nDataset ready at: {DATASET_DIR}\")\n",
    "print(f\"Video frame map: {VIDEO_MAP_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "Configure the training parameters below.\n",
    "\n",
    "**Mode selection:**\n",
    "- **Baseline mode**: Set `USE_AUGMENTATION = False`, `WEIGHT_DECAY = 0`\n",
    "- **Improved mode**: Use defaults (all regularization features enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING CONFIGURATION - EDIT THESE VALUES\n",
    "# ============================================================\n",
    "\n",
    "# Basic training parameters\n",
    "K_SPLITS = 5          # Number of cross-validation folds\n",
    "EPOCHS = 50           # Maximum epochs per fold (may stop earlier with early stopping)\n",
    "BATCH_SIZE = 4        # Batch size (reduce if OOM)\n",
    "LR = 1e-4             # Initial learning rate\n",
    "\n",
    "# StarDist model parameters\n",
    "N_RAYS = 32           # Number of radial directions (32, 64, or 96)\n",
    "ENCODER_NAME = \"resnet18\"  # Backbone: resnet18, resnet34, resnet50, efficientnet-b0\n",
    "\n",
    "# Detection thresholds (will be optimized by sweep)\n",
    "PROB_THRESH = 0.5     # Initial probability threshold\n",
    "NMS_THRESH = 0.3      # Initial NMS IoU threshold\n",
    "MATCH_THRESH = 5.0    # Distance threshold for DetA calculation (pixels)\n",
    "\n",
    "# Data options\n",
    "USE_BONUS = True      # Include bonus training data from GitHub\n",
    "\n",
    "# ============================================================\n",
    "# IMPROVEMENT SETTINGS (set to False/0 for baseline)\n",
    "# ============================================================\n",
    "\n",
    "# Data augmentation\n",
    "USE_AUGMENTATION = True  # Enable data augmentation (False for baseline)\n",
    "AUG_PARAMS = {           # Augmentation probabilities (only used if USE_AUGMENTATION=True)\n",
    "    'rotate_p': 0.7,\n",
    "    'flip_p': 0.5,\n",
    "    'brightness_p': 0.3,\n",
    "    'noise_p': 0.2,\n",
    "    'blur_p': 0.1,\n",
    "    'elastic_p': 0.2\n",
    "}\n",
    "\n",
    "# Regularization\n",
    "WEIGHT_DECAY = 1e-4      # L2 regularization (0 to disable)\n",
    "\n",
    "# Combined loss weights\n",
    "FOCAL_WEIGHT = 1.0       # Focal loss weight (handles class imbalance)\n",
    "DICE_WEIGHT = 1.0        # Dice loss weight (overlap-based)\n",
    "DIST_WEIGHT = 1.0        # Distance regression weight\n",
    "\n",
    "# Learning rate scheduling\n",
    "SCHEDULER_PATIENCE = 5   # Epochs to wait before reducing LR\n",
    "SCHEDULER_FACTOR = 0.5   # Factor to reduce LR by\n",
    "\n",
    "# Early stopping\n",
    "EARLY_STOPPING_PATIENCE = 10  # Epochs to wait before stopping\n",
    "\n",
    "# Post-training threshold sweep\n",
    "RUN_THRESHOLD_SWEEP = True\n",
    "SWEEP_PROB_THRESHOLDS = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "SWEEP_NMS_THRESHOLDS = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Output directory\n",
    "SAVE_DIR = repo_root / \"models\" / \"stardist\" / \"run\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Mode: {'IMPROVED' if USE_AUGMENTATION else 'BASELINE'}\")\n",
    "print(f\"  Encoder: {ENCODER_NAME}, N_Rays: {N_RAYS}\")\n",
    "print(f\"  Epochs: {EPOCHS}, Batch: {BATCH_SIZE}, LR: {LR}\")\n",
    "print(f\"  Augmentation: {USE_AUGMENTATION}, Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Save directory: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "\n",
    "Run K-Fold cross-validation training. Each fold:\n",
    "1. Trains with combined loss (Focal + Dice + Smooth L1)\n",
    "2. Uses LR scheduling and early stopping\n",
    "3. Runs threshold sweep to find optimal prob/nms thresholds\n",
    "4. Evaluates DetA metric on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "fold_results, best_fold, all_preds_df = train_stardist_kfold(\n",
    "    dataset_root=DATASET_DIR,\n",
    "    video_map_path=VIDEO_MAP_PATH,\n",
    "    val_csv_path=str(VAL_CSV),\n",
    "    k_splits=K_SPLITS,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    n_rays=N_RAYS,\n",
    "    prob_thresh=PROB_THRESH,\n",
    "    nms_thresh=NMS_THRESH,\n",
    "    use_bonus=USE_BONUS,\n",
    "    save_dir=SAVE_DIR,\n",
    "    match_thresh=MATCH_THRESH,\n",
    "    device=device,\n",
    "    # Improvement parameters\n",
    "    use_augmentation=USE_AUGMENTATION,\n",
    "    aug_params=AUG_PARAMS if USE_AUGMENTATION else None,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    focal_weight=FOCAL_WEIGHT,\n",
    "    dice_weight=DICE_WEIGHT,\n",
    "    dist_weight=DIST_WEIGHT,\n",
    "    scheduler_patience=SCHEDULER_PATIENCE,\n",
    "    scheduler_factor=SCHEDULER_FACTOR,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    encoder_name=ENCODER_NAME,\n",
    "    run_threshold_sweep=RUN_THRESHOLD_SWEEP,\n",
    "    sweep_prob_thresholds=SWEEP_PROB_THRESHOLDS,\n",
    "    sweep_nms_thresholds=SWEEP_NMS_THRESHOLDS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display fold results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"K-FOLD RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for result in fold_results:\n",
    "    print(f\"\\nFold {result['fold']}:\")\n",
    "    print(f\"  DetA = {result['deta']:.4f}\")\n",
    "    print(f\"  Optimal thresholds: prob={result['best_prob_thresh']:.2f}, nms={result['best_nms_thresh']:.2f}\")\n",
    "    print(f\"  Stopped at epoch: {result['stopped_epoch']}\")\n",
    "\n",
    "deta_values = [r['deta'] for r in fold_results]\n",
    "print(f\"\\n\" + \"-\"*40)\n",
    "print(f\"Mean DetA: {np.mean(deta_values):.4f} +/- {np.std(deta_values):.4f}\")\n",
    "print(f\"Best Fold: {best_fold['fold']} (DetA = {best_fold['deta']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for best fold\n",
    "history = best_fold['history']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0]\n",
    "ax1.plot(history['train_loss'], label='Train Loss', color='blue', linewidth=2)\n",
    "ax1.plot(history['val_loss'], label='Val Loss', color='orange', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title(f'Training Curves - Best Fold {best_fold[\"fold\"]}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "ax2 = axes[1]\n",
    "if 'lr' in history and history['lr']:\n",
    "    ax2.plot(history['lr'], color='green', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.set_title('Learning Rate Schedule')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'LR history not available', ha='center', va='center', transform=ax2.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(SAVE_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Video Inference\n",
    "\n",
    "Run the best model on all video frames using optimized thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on full video\n",
    "best_model = best_fold['model']\n",
    "best_prob = best_fold['best_prob_thresh']\n",
    "best_nms = best_fold['best_nms_thresh']\n",
    "\n",
    "print(f\"Running inference with prob_thresh={best_prob:.2f}, nms_thresh={best_nms:.2f}...\")\n",
    "\n",
    "full_preds_df = infer_stardist_full_video(\n",
    "    model=best_model,\n",
    "    video_root=DATASET_DIR / \"video\",\n",
    "    video_map_path=VIDEO_MAP_PATH,\n",
    "    device=device,\n",
    "    prob_thresh=best_prob,\n",
    "    nms_thresh=best_nms\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal detections: {len(full_preds_df)}\")\n",
    "print(f\"Frames covered: {full_preds_df['frame'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final DetA on full predictions\n",
    "gt_df = pd.read_csv(VAL_CSV)\n",
    "gt_roi = gt_df[\n",
    "    (gt_df.x >= ROI_X_MIN) & (gt_df.x < ROI_X_MAX) &\n",
    "    (gt_df.y >= ROI_Y_MIN) & (gt_df.y < ROI_Y_MAX)\n",
    "].copy()\n",
    "\n",
    "common_frames = set(gt_roi.frame.unique()) & set(full_preds_df.frame.unique())\n",
    "gt_filtered = gt_roi[gt_roi.frame.isin(common_frames)]\n",
    "pred_filtered = full_preds_df[full_preds_df.frame.isin(common_frames)]\n",
    "\n",
    "final_deta = calculate_deta_robust(gt_filtered, pred_filtered, match_thresh=MATCH_THRESH)\n",
    "print(f\"\\nFinal DetA (all frames): {final_deta:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tracking with default parameters\n",
    "print(\"Running tracking...\")\n",
    "\n",
    "try:\n",
    "    tracked_df = run_btrack_tracking(full_preds_df)\n",
    "    tracker_name = \"BTrack\"\n",
    "except Exception as e:\n",
    "    print(f\"BTrack failed ({e}), falling back to LapTrack...\")\n",
    "    tracked_df = run_laptrack(full_preds_df)\n",
    "    tracker_name = \"LapTrack\"\n",
    "\n",
    "print(f\"\\n{tracker_name} Results:\")\n",
    "print(f\"  Total tracks: {tracked_df['track_id'].nunique()}\")\n",
    "print(f\"  Total detections: {len(tracked_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate HOTA metric\n",
    "gt_for_hota = gt_roi[['frame', 'x', 'y', 'track']].rename(columns={'track': 'track_id'})\n",
    "pred_for_hota = tracked_df[['frame', 'x', 'y', 'track_id']]\n",
    "\n",
    "hota_score = hota(gt_for_hota, pred_for_hota, match_thresh=MATCH_THRESH)\n",
    "print(f\"\\nHOTA Score: {hota_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample frames with detections\n",
    "import tifffile\n",
    "\n",
    "video_frames = tifffile.imread(VAL_TIF)\n",
    "roi = video_frames[:, ROI_Y_MIN:ROI_Y_MAX, ROI_X_MIN:ROI_X_MAX]\n",
    "\n",
    "sample_frames = [0, 30, 60, 90]\n",
    "fig, axes = plt.subplots(1, len(sample_frames), figsize=(16, 4))\n",
    "\n",
    "for ax, fidx in zip(axes, sample_frames):\n",
    "    if fidx >= len(roi):\n",
    "        continue\n",
    "    \n",
    "    ax.imshow(roi[fidx], cmap='gray')\n",
    "    \n",
    "    # Plot predictions\n",
    "    frame_preds = full_preds_df[full_preds_df.frame == fidx]\n",
    "    ax.scatter(\n",
    "        frame_preds.x - ROI_X_MIN,\n",
    "        frame_preds.y - ROI_Y_MIN,\n",
    "        c='lime', s=20, marker='o', alpha=0.8, label='Predicted'\n",
    "    )\n",
    "    \n",
    "    # Plot GT\n",
    "    frame_gt = gt_roi[gt_roi.frame == fidx]\n",
    "    ax.scatter(\n",
    "        frame_gt.x - ROI_X_MIN,\n",
    "        frame_gt.y - ROI_Y_MIN,\n",
    "        c='red', s=30, marker='x', alpha=0.8, label='GT'\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'Frame {fidx}')\n",
    "    ax.axis('off')\n",
    "\n",
    "axes[0].legend(loc='upper left')\n",
    "plt.suptitle('StarDist Detection Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_DIR / 'detection_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions and tracking results\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "full_preds_df.to_csv(SAVE_DIR / 'stardist_predictions.csv', index=False)\n",
    "tracked_df.to_csv(SAVE_DIR / 'stardist_tracked.csv', index=False)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'best_fold': best_fold['fold'],\n",
    "    'best_deta': best_fold['deta'],\n",
    "    'final_deta': final_deta,\n",
    "    'hota': hota_score,\n",
    "    'best_prob_thresh': best_prob,\n",
    "    'best_nms_thresh': best_nms,\n",
    "    'total_detections': len(full_preds_df),\n",
    "    'total_tracks': tracked_df['track_id'].nunique(),\n",
    "    'use_augmentation': USE_AUGMENTATION,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'encoder': ENCODER_NAME,\n",
    "    'n_rays': N_RAYS,\n",
    "    'repo_url': REPO_URL\n",
    "}\n",
    "\n",
    "pd.DataFrame([summary]).to_csv(SAVE_DIR / 'training_summary.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for k, v in summary.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "print(f\"\\nResults saved to: {SAVE_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}