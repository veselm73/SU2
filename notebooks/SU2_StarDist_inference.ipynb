{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/veselm73/SU2/blob/main/notebooks/SU2_StarDist_inference.ipynb)\n\n# Cell Detection & Tracking - Model Benchmark\n\n**Author:** Mateusz Vesel  \n**Task:** Compare 3 trained StarDist ensembles on held-out validation data\n\n---\n\n## Pipeline Overview\n\n### Detection: StarDist with 5-Fold Ensemble\n- **Architecture:** StarDist with ResNet18 encoder\n- **Training:** 5-Fold cross-validation on training data\n- **Inference:** Ensemble averaging of probability maps from all 5 folds\n- **Post-processing:** Non-maximum suppression to extract cell centroids\n\n### Evaluation\n- **Held-out data:** 10 manually annotated frames (58-67) never seen during training\n- **Metrics:** DetA (Detection Accuracy), Precision, Recall\n\n---\n\n## Available Pre-trained Models\n\n| Model | Epochs | Augmentation | N_Rays |\n|-------|--------|--------------|--------|\n| `100e_noaug_32rays` | 100 | No | 32 |\n| `100e_noaug_64rays` | 100 | No | 64 |\n| `120e_aug_32rays` | 120 | Yes | 32 |\n\n---\n\n## How to Use This Notebook\n\n1. **Upload `val.tif`** when prompted (validation video containing frames 58-67)\n2. **Run all cells** - model weights auto-download from GitHub\n3. All 3 models are benchmarked on 10 held-out annotated frames\n4. Results show true generalization performance (unbiased evaluation)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (fast with uv)\n",
    "!pip install uv -q\n",
    "!uv pip uninstall torch torchvision torchaudio --system -q 2>/dev/null || true\n",
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --system -q\n",
    "!uv pip install \"numpy<2\" cellseg-models-pytorch pytorch-lightning laptrack tifffile --system -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone/update repository\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path('/content/SU2').exists():\n",
    "        !git clone https://github.com/veselm73/SU2.git /content/SU2\n",
    "    else:\n",
    "        !cd /content/SU2 && git pull\n",
    "    os.chdir('/content/SU2')\n",
    "    repo_root = Path('/content/SU2')\n",
    "else:\n",
    "    notebook_dir = Path(os.getcwd())\n",
    "    repo_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"Repository: {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "from cellseg_models_pytorch.postproc.functional.stardist.stardist import post_proc_stardist\n",
    "\n",
    "# Import tracking and metrics\n",
    "from modules.stardist_helpers import (\n",
    "    run_laptrack,\n",
    "    hota,\n",
    "    ROI_X_MIN, ROI_X_MAX, ROI_Y_MIN, ROI_Y_MAX\n",
    ")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Available pre-trained models\nAVAILABLE_MODELS = {\n    \"100e_noaug_32rays\": \"100 epochs, no aug, 32 rays\",\n    \"100e_noaug_64rays\": \"100 epochs, no aug, 64 rays\",\n    \"120e_aug_32rays\": \"120 epochs, with aug, 32 rays\"\n}\n\nprint(\"Models to benchmark:\")\nfor name, desc in AVAILABLE_MODELS.items():\n    print(f\"  - {name}: {desc}\")"
  },
  {
   "cell_type": "markdown",
   "id": "s1m341p2vza",
   "source": "## 2. Available Models",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wh9oc1ybphg",
   "source": "import urllib.request\nimport json\n\ndef download_weights_if_needed(model_name, repo_root):\n    \"\"\"Download model weights from GitHub if not present locally.\"\"\"\n    weights_dir = repo_root / \"weights\" / model_name\n    models_dir = weights_dir / \"models\"\n    \n    # Check if weights already exist\n    if models_dir.exists() and len(list(models_dir.glob(\"*.pth\"))) == 5:\n        print(f\"Weights found locally: {weights_dir}\")\n        return weights_dir\n    \n    print(f\"Downloading weights for '{model_name}' from GitHub...\")\n    weights_dir.mkdir(parents=True, exist_ok=True)\n    models_dir.mkdir(parents=True, exist_ok=True)\n    \n    base_url = f\"https://raw.githubusercontent.com/veselm73/SU2/main/weights/{model_name}\"\n    \n    # Download config files\n    for cfg in [\"model_config.json\", \"inference_config.json\"]:\n        url = f\"{base_url}/{cfg}\"\n        dest = weights_dir / cfg\n        print(f\"  Downloading {cfg}...\")\n        urllib.request.urlretrieve(url, dest)\n    \n    # Download fold weights (5 folds, ~53MB each)\n    for fold in range(1, 6):\n        url = f\"{base_url}/models/fold_{fold}.pth\"\n        dest = models_dir / f\"fold_{fold}.pth\"\n        print(f\"  Downloading fold_{fold}.pth (~53MB)...\")\n        urllib.request.urlretrieve(url, dest)\n    \n    print(\"Download complete!\")\n    return weights_dir\n\nK_FOLDS = 5",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from cellseg_models_pytorch.models.stardist.stardist import StarDist\n",
    "import torch.nn as nn\n",
    "\n",
    "class StarDistLightning(pl.LightningModule):\n",
    "    \"\"\"StarDist model wrapper for inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_rays=32, encoder_name=\"resnet18\", dropout=0.0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_rays = n_rays\n",
    "        wrapper = StarDist(\n",
    "            n_nuc_classes=1,\n",
    "            n_rays=n_rays,\n",
    "            enc_name=encoder_name,\n",
    "            model_kwargs={\"encoder_kws\": {\"in_chans\": 1}}\n",
    "        )\n",
    "        self.model = wrapper.model\n",
    "        self.dropout = nn.Dropout2d(p=dropout) if dropout > 0 else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def load_fold_model(fold_path, config):\n",
    "    \"\"\"Load a single fold model from .pth file.\"\"\"\n",
    "    model = StarDistLightning(\n",
    "        n_rays=config['n_rays'],\n",
    "        encoder_name=config['encoder_name'],\n",
    "        dropout=config.get('dropout', 0.0)\n",
    "    )\n",
    "    state_dict = torch.load(fold_path, map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "def preprocess_frame(frame):\n    \"\"\"Percentile normalization.\"\"\"\n    frame = frame.astype(np.float32)\n    p1, p99 = np.percentile(frame, (1, 99.8))\n    frame = np.clip(frame, p1, p99)\n    frame = (frame - p1) / (p99 - p1 + 1e-8)\n    return frame\n\n\ndef detect_cells_ensemble(models, frame, prob_thresh, nms_thresh, device):\n    \"\"\"Detect cells using ensemble (average probability maps before thresholding).\"\"\"\n    x = torch.from_numpy(frame).float().unsqueeze(0).unsqueeze(0).to(device)\n    \n    all_stardist = []\n    all_prob = []\n    \n    with torch.no_grad():\n        for model in models:\n            out = model(x)\n            nuc_out = out['nuc']\n            stardist_map = nuc_out.aux_map.cpu().numpy()[0]\n            prob_map = torch.sigmoid(nuc_out.binary_map).cpu().numpy()[0, 0]\n            all_stardist.append(stardist_map)\n            all_prob.append(prob_map)\n    \n    avg_stardist = np.mean(all_stardist, axis=0)\n    avg_prob = np.mean(all_prob, axis=0)\n    \n    try:\n        labels = post_proc_stardist(\n            avg_prob, avg_stardist,\n            score_thresh=prob_thresh,\n            iou_thresh=nms_thresh\n        )\n        detections = [(prop.centroid[1], prop.centroid[0]) for prop in regionprops(labels)]\n        return detections\n    except:\n        return []"
  },
  {
   "cell_type": "markdown",
   "id": "781s9xkd5lq",
   "source": "## 5. Upload Validation Video & Load Annotations\n\nUpload `val.tif` to benchmark models on your 10 manually annotated frames (58-67).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "k8sryrcunm",
   "source": "# Upload val.tif \nimport os\nimport shutil\n\nVAL_TIF = repo_root / \"data\" / \"val\" / \"val.tif\"\n\nif not VAL_TIF.exists():\n    print(\"val.tif not found. Please upload it.\")\n    if IN_COLAB:\n        from google.colab import files\n        os.makedirs(repo_root / \"data\" / \"val\", exist_ok=True)\n        print(\"\\nUpload val.tif:\")\n        uploaded = files.upload()\n        for filename in uploaded.keys():\n            shutil.move(filename, VAL_TIF)\n            print(f\"Moved to {VAL_TIF}\")\n    else:\n        raise FileNotFoundError(f\"Please place val.tif at {VAL_TIF}\")\nelse:\n    print(f\"Found: {VAL_TIF}\")\n\n# Load video\nvideo = tifffile.imread(VAL_TIF)\nprint(f\"Video shape: {video.shape}\")\n\n# Download held-out validation annotations (frames 58-67)\nSANITY_CHECK_CSV_URL = \"https://raw.githubusercontent.com/veselm73/SU2/main/data/val_annotations/ensemble_sanity_check.csv\"\nSANITY_CHECK_CSV = repo_root / \"data\" / \"val_annotations\" / \"ensemble_sanity_check.csv\"\n\nif not SANITY_CHECK_CSV.exists():\n    print(\"Downloading held-out validation annotations...\")\n    SANITY_CHECK_CSV.parent.mkdir(parents=True, exist_ok=True)\n    urllib.request.urlretrieve(SANITY_CHECK_CSV_URL, SANITY_CHECK_CSV)\n    \nsanity_gt = pd.read_csv(SANITY_CHECK_CSV)\nprint(f\"\\nLoaded {len(sanity_gt)} annotations across frames {sorted(sanity_gt['frame'].unique())}\")\nprint(f\"Annotations per frame: ~{len(sanity_gt) // sanity_gt['frame'].nunique()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hdfeghy1vs",
   "source": "## 6. Benchmark All Models\n\nCompare all 3 trained models on the held-out validation frames.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "218qcimy7vm",
   "source": "def benchmark_model_on_sanity_check(model_name, sanity_gt, video, device=DEVICE):\n    \"\"\"\n    Run a single model on sanity check frames and compute detection metrics.\n    Returns DetA score and per-frame detection results.\n    \"\"\"\n    from scipy.spatial.distance import cdist\n    \n    # Load model weights and config\n    weights_dir = download_weights_if_needed(model_name, repo_root)\n    with open(weights_dir / \"model_config.json\") as f:\n        model_config = json.load(f)\n    with open(weights_dir / \"inference_config.json\") as f:\n        inf_config = json.load(f)\n    \n    # Load models\n    models = []\n    for fold in range(1, 6):\n        model = load_fold_model(weights_dir / \"models\" / f\"fold_{fold}.pth\", model_config)\n        model = model.to(device)\n        models.append(model)\n    \n    # Get unique frames in sanity check\n    frames = sorted(sanity_gt['frame'].unique())\n    \n    # ROI config - sanity check uses ROI coordinates already in the CSV\n    # The images are 256x256 crops, so x,y in CSV are relative to ROI\n    roi = inf_config.get('roi', {'x_min': 256, 'x_max': 512, 'y_min': 512, 'y_max': 768})\n    \n    all_results = []\n    total_tp, total_fp, total_fn = 0, 0, 0\n    \n    for frame_idx in tqdm(frames, desc=f\"Benchmarking {model_name}\"):\n        # Get frame from video (already in ROI coordinates in CSV)\n        frame = video[frame_idx, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n        frame_norm = preprocess_frame(frame)\n        \n        # Detect cells\n        detections = detect_cells_ensemble(\n            models, frame_norm,\n            inf_config['prob_thresh'], inf_config['nms_thresh'], device\n        )\n        \n        # Get GT for this frame (coordinates are in ROI space, 0-256)\n        frame_gt = sanity_gt[sanity_gt['frame'] == frame_idx][['x', 'y']].values\n        \n        # Match predictions to GT using Hungarian algorithm\n        if len(detections) > 0 and len(frame_gt) > 0:\n            pred_coords = np.array(detections)  # (x, y) format\n            gt_coords = frame_gt  # (x, y) format\n            \n            # Compute distance matrix\n            dist_matrix = cdist(pred_coords, gt_coords)\n            \n            # Match within threshold (5 pixels)\n            match_thresh = 5.0\n            tp = 0\n            matched_gt = set()\n            matched_pred = set()\n            \n            # Greedy matching (for simplicity)\n            for _ in range(min(len(pred_coords), len(gt_coords))):\n                if dist_matrix.size == 0:\n                    break\n                min_idx = np.unravel_index(np.argmin(dist_matrix), dist_matrix.shape)\n                if dist_matrix[min_idx] <= match_thresh:\n                    tp += 1\n                    matched_pred.add(min_idx[0])\n                    matched_gt.add(min_idx[1])\n                    dist_matrix[min_idx[0], :] = np.inf\n                    dist_matrix[:, min_idx[1]] = np.inf\n                else:\n                    break\n            \n            fp = len(pred_coords) - tp\n            fn = len(gt_coords) - tp\n        elif len(detections) > 0:\n            tp, fp, fn = 0, len(detections), 0\n        elif len(frame_gt) > 0:\n            tp, fp, fn = 0, 0, len(frame_gt)\n        else:\n            tp, fp, fn = 0, 0, 0\n        \n        total_tp += tp\n        total_fp += fp\n        total_fn += fn\n        \n        all_results.append({\n            'frame': frame_idx,\n            'n_gt': len(frame_gt),\n            'n_pred': len(detections),\n            'tp': tp, 'fp': fp, 'fn': fn\n        })\n    \n    # Compute DetA\n    if total_tp + total_fp + total_fn > 0:\n        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n        deta = total_tp / (total_tp + total_fp + total_fn)\n    else:\n        precision, recall, deta = 0, 0, 0\n    \n    # Clean up models from GPU\n    del models\n    torch.cuda.empty_cache()\n    \n    return {\n        'model': model_name,\n        'DetA': deta,\n        'Precision': precision,\n        'Recall': recall,\n        'TP': total_tp,\n        'FP': total_fp,\n        'FN': total_fn,\n        'per_frame': pd.DataFrame(all_results)\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5ggtin8tzfj",
   "source": "# Run benchmark on all 3 models\nprint(\"=\"*60)\nprint(\"BENCHMARK: Comparing 3 Models on Held-Out Validation Data\")\nprint(\"=\"*60)\nprint(f\"Frames: {sorted(sanity_gt['frame'].unique())}\")\nprint(f\"Total annotations: {len(sanity_gt)}\")\nprint(\"=\"*60)\n\n# Benchmark each model\nbenchmark_results = []\nfor model_name in AVAILABLE_MODELS.keys():\n    print(f\"\\n>>> Testing: {model_name}\")\n    result = benchmark_model_on_sanity_check(model_name, sanity_gt, video)\n    benchmark_results.append(result)\n    print(f\"    DetA: {result['DetA']:.4f} | Precision: {result['Precision']:.4f} | Recall: {result['Recall']:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "imd6sliibe8",
   "source": "## 7. Results Summary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f1q6yc2ti6a",
   "source": "# Display benchmark summary table\nsummary_df = pd.DataFrame([{\n    'Model': r['model'],\n    'DetA': f\"{r['DetA']:.4f}\",\n    'Precision': f\"{r['Precision']:.4f}\",\n    'Recall': f\"{r['Recall']:.4f}\",\n    'TP': r['TP'],\n    'FP': r['FP'],\n    'FN': r['FN']\n} for r in benchmark_results])\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"BENCHMARK RESULTS SUMMARY\")\nprint(\"=\"*60)\nprint(summary_df.to_string(index=False))\nprint(\"=\"*60)\n\n# Find best model\nbest_idx = np.argmax([r['DetA'] for r in benchmark_results])\nprint(f\"\\nüèÜ Best Model: {benchmark_results[best_idx]['model']} (DetA={benchmark_results[best_idx]['DetA']:.4f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jlwvqm89kla",
   "source": "# Visualize benchmark: bar chart comparison\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\nmodels = [r['model'] for r in benchmark_results]\ndeta_scores = [r['DetA'] for r in benchmark_results]\nprecision_scores = [r['Precision'] for r in benchmark_results]\nrecall_scores = [r['Recall'] for r in benchmark_results]\n\ncolors = ['#2ecc71', '#3498db', '#e74c3c']\n\n# DetA comparison\nax = axes[0]\nbars = ax.bar(range(len(models)), deta_scores, color=colors)\nax.set_xticks(range(len(models)))\nax.set_xticklabels([m.replace('_', '\\n') for m in models], fontsize=9)\nax.set_ylabel('DetA')\nax.set_title('Detection Accuracy (DetA)', fontweight='bold')\nax.set_ylim(0, 1)\nfor bar, score in zip(bars, deta_scores):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n            f'{score:.3f}', ha='center', va='bottom', fontsize=10)\n\n# Precision comparison\nax = axes[1]\nbars = ax.bar(range(len(models)), precision_scores, color=colors)\nax.set_xticks(range(len(models)))\nax.set_xticklabels([m.replace('_', '\\n') for m in models], fontsize=9)\nax.set_ylabel('Precision')\nax.set_title('Precision (TP / (TP + FP))', fontweight='bold')\nax.set_ylim(0, 1)\nfor bar, score in zip(bars, precision_scores):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n            f'{score:.3f}', ha='center', va='bottom', fontsize=10)\n\n# Recall comparison\nax = axes[2]\nbars = ax.bar(range(len(models)), recall_scores, color=colors)\nax.set_xticks(range(len(models)))\nax.set_xticklabels([m.replace('_', '\\n') for m in models], fontsize=9)\nax.set_ylabel('Recall')\nax.set_title('Recall (TP / (TP + FN))', fontweight='bold')\nax.set_ylim(0, 1)\nfor bar, score in zip(bars, recall_scores):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n            f'{score:.3f}', ha='center', va='bottom', fontsize=10)\n\nplt.suptitle('Model Benchmark on Held-Out Validation Data (10 frames, 1480 annotations)', \n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "81p3njiu8gu",
   "source": "## 8. Visualization: Best Model Detections",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "h4mw7nxhyo",
   "source": "# Visualize detections from best model on sample frames\nbest_result = benchmark_results[best_idx]\nbest_model_name = best_result['model']\n\n# Reload best model for visualization\nweights_dir = download_weights_if_needed(best_model_name, repo_root)\nwith open(weights_dir / \"model_config.json\") as f:\n    model_config = json.load(f)\nwith open(weights_dir / \"inference_config.json\") as f:\n    inf_config = json.load(f)\n\nbest_models = []\nfor fold in range(1, 6):\n    model = load_fold_model(weights_dir / \"models\" / f\"fold_{fold}.pth\", model_config)\n    model = model.to(DEVICE)\n    best_models.append(model)\n\nroi = inf_config.get('roi', {'x_min': 256, 'x_max': 512, 'y_min': 512, 'y_max': 768})\nframes_to_show = sorted(sanity_gt['frame'].unique())[:5]  # First 5 frames\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 8))\n\nfor i, frame_idx in enumerate(frames_to_show):\n    # Get frame\n    frame = video[frame_idx, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n    frame_norm = preprocess_frame(frame)\n    \n    # Detect\n    detections = detect_cells_ensemble(\n        best_models, frame_norm,\n        inf_config['prob_thresh'], inf_config['nms_thresh'], DEVICE\n    )\n    \n    # Get GT\n    frame_gt = sanity_gt[sanity_gt['frame'] == frame_idx][['x', 'y']].values\n    \n    # Plot GT (top row)\n    axes[0, i].imshow(frame, cmap='gray')\n    axes[0, i].scatter(frame_gt[:, 0], frame_gt[:, 1], c='lime', s=40, \n                       marker='o', facecolors='none', linewidths=1.5)\n    axes[0, i].set_title(f'Frame {frame_idx} - GT ({len(frame_gt)})')\n    axes[0, i].axis('off')\n    \n    # Plot Predictions (bottom row)\n    axes[1, i].imshow(frame, cmap='gray')\n    if detections:\n        pred_coords = np.array(detections)\n        axes[1, i].scatter(pred_coords[:, 0], pred_coords[:, 1], c='red', s=40, \n                          marker='x', linewidths=1.5)\n    axes[1, i].set_title(f'Frame {frame_idx} - Pred ({len(detections)})')\n    axes[1, i].axis('off')\n\nplt.suptitle(f'Best Model: {best_model_name}\\nTop: Ground Truth (green) | Bottom: Predictions (red)', \n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Cleanup\ndel best_models\ntorch.cuda.empty_cache()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}