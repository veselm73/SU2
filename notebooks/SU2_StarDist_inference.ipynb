{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/veselm73/SU2/blob/main/notebooks/SU2_StarDist_inference.ipynb)\n",
    "\n",
    "# Cell Detection - Model Benchmark\n",
    "\n",
    "**Author:** Mateusz Vesel  \n",
    "**Task:** Compare 3 trained StarDist ensembles on held-out validation data\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Automatic benchmark** on 10 held-out validation frames (58-67)\n",
    "2. **Compare all 3 models** and determine the best performer\n",
    "3. **Optional:** Run inference on custom data with the best model\n",
    "\n",
    "---\n",
    "\n",
    "## Available Pre-trained Models\n",
    "\n",
    "| Model | Epochs | Augmentation | N_Rays |\n",
    "|-------|--------|--------------|--------|\n",
    "| `100e_noaug_32rays` | 100 | No | 32 |\n",
    "| `100e_noaug_64rays` | 100 | No | 64 |\n",
    "| `120e_aug_32rays` | 120 | Yes | 32 |\n",
    "\n",
    "---\n",
    "\n",
    "**Just run all cells** - everything downloads automatically from GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install uv -q\n",
    "!uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121 --system -q\n",
    "!uv pip install \"numpy<2\" cellseg-models-pytorch pytorch-lightning tifffile scipy --system -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path('/content/SU2').exists():\n",
    "        !git clone https://github.com/veselm73/SU2.git /content/SU2\n",
    "    else:\n",
    "        !cd /content/SU2 && git pull\n",
    "    os.chdir('/content/SU2')\n",
    "    repo_root = Path('/content/SU2')\n",
    "else:\n",
    "    repo_root = Path('.').resolve()\n",
    "    if repo_root.name == 'notebooks':\n",
    "        repo_root = repo_root.parent\n",
    "\n",
    "print(f\"Repository: {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.measure import regionprops\n",
    "from scipy.spatial.distance import cdist\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "from cellseg_models_pytorch.postproc.functional.stardist.stardist import post_proc_stardist\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "AVAILABLE_MODELS = {\n",
    "    \"100e_noaug_32rays\": \"100 epochs, no aug, 32 rays\",\n",
    "    \"100e_noaug_64rays\": \"100 epochs, no aug, 64 rays\",\n",
    "    \"120e_aug_32rays\": \"120 epochs, with aug, 32 rays\"\n",
    "}\n",
    "\n",
    "print(f\"\\nModels to benchmark: {list(AVAILABLE_MODELS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Download Validation Data & Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download validation frames and annotations from GitHub\n",
    "BASE_URL = \"https://raw.githubusercontent.com/veselm73/SU2/main\"\n",
    "\n",
    "# Paths\n",
    "VAL_DIR = repo_root / \"data\" / \"val_annotations\"\n",
    "FRAMES_DIR = VAL_DIR / \"frames\"\n",
    "ANNOTATIONS_CSV = VAL_DIR / \"ensemble_sanity_check.csv\"\n",
    "\n",
    "VAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FRAMES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download annotations\n",
    "if not ANNOTATIONS_CSV.exists():\n",
    "    print(\"Downloading annotations...\")\n",
    "    urllib.request.urlretrieve(f\"{BASE_URL}/data/val_annotations/ensemble_sanity_check.csv\", ANNOTATIONS_CSV)\n",
    "\n",
    "# Download validation frames (58-67)\n",
    "VALIDATION_FRAMES = list(range(58, 68))\n",
    "print(f\"Downloading {len(VALIDATION_FRAMES)} validation frames...\")\n",
    "for frame_idx in VALIDATION_FRAMES:\n",
    "    frame_path = FRAMES_DIR / f\"frame_{frame_idx:04d}.png\"\n",
    "    if not frame_path.exists():\n",
    "        url = f\"{BASE_URL}/data/val_annotations/frames/frame_{frame_idx:04d}.png\"\n",
    "        urllib.request.urlretrieve(url, frame_path)\n",
    "        print(f\"  Downloaded frame_{frame_idx:04d}.png\")\n",
    "\n",
    "# Load annotations\n",
    "gt_df = pd.read_csv(ANNOTATIONS_CSV)\n",
    "print(f\"\\nLoaded {len(gt_df)} annotations across frames {VALIDATION_FRAMES}\")\n",
    "print(f\"Annotations per frame: ~{len(gt_df) // len(VALIDATION_FRAMES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model_weights(model_name):\n",
    "    \"\"\"Download model weights from GitHub if not present.\"\"\"\n",
    "    weights_dir = repo_root / \"weights\" / model_name\n",
    "    models_dir = weights_dir / \"models\"\n",
    "    \n",
    "    if models_dir.exists() and len(list(models_dir.glob(\"*.pth\"))) == 5:\n",
    "        return weights_dir\n",
    "    \n",
    "    print(f\"Downloading {model_name} weights...\")\n",
    "    weights_dir.mkdir(parents=True, exist_ok=True)\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    base_url = f\"{BASE_URL}/weights/{model_name}\"\n",
    "    \n",
    "    for cfg in [\"model_config.json\", \"inference_config.json\"]:\n",
    "        urllib.request.urlretrieve(f\"{base_url}/{cfg}\", weights_dir / cfg)\n",
    "    \n",
    "    for fold in range(1, 6):\n",
    "        print(f\"  Downloading fold_{fold}.pth (~53MB)...\")\n",
    "        urllib.request.urlretrieve(f\"{base_url}/models/fold_{fold}.pth\", models_dir / f\"fold_{fold}.pth\")\n",
    "    \n",
    "    return weights_dir\n",
    "\n",
    "# Pre-download all model weights\n",
    "print(\"Downloading model weights (this may take a few minutes)...\\n\")\n",
    "for model_name in AVAILABLE_MODELS:\n",
    "    download_model_weights(model_name)\n",
    "print(\"\\nAll weights downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Model & Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from cellseg_models_pytorch.models.stardist.stardist import StarDist\n",
    "import torch.nn as nn\n",
    "\n",
    "class StarDistModel(pl.LightningModule):\n",
    "    def __init__(self, n_rays=32, encoder_name=\"resnet18\", dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.n_rays = n_rays\n",
    "        wrapper = StarDist(\n",
    "            n_nuc_classes=1, n_rays=n_rays, enc_name=encoder_name,\n",
    "            model_kwargs={\"encoder_kws\": {\"in_chans\": 1}}\n",
    "        )\n",
    "        self.model = wrapper.model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def load_ensemble(model_name):\n",
    "    \"\"\"Load 5-fold ensemble for a model.\"\"\"\n",
    "    weights_dir = repo_root / \"weights\" / model_name\n",
    "    with open(weights_dir / \"model_config.json\") as f:\n",
    "        config = json.load(f)\n",
    "    with open(weights_dir / \"inference_config.json\") as f:\n",
    "        inf_config = json.load(f)\n",
    "    \n",
    "    models = []\n",
    "    for fold in range(1, 6):\n",
    "        model = StarDistModel(n_rays=config['n_rays'], encoder_name=config['encoder_name'])\n",
    "        model.load_state_dict(torch.load(weights_dir / \"models\" / f\"fold_{fold}.pth\", map_location='cpu'))\n",
    "        model.eval().to(DEVICE)\n",
    "        models.append(model)\n",
    "    \n",
    "    return models, inf_config\n",
    "\n",
    "\n",
    "def preprocess(frame):\n",
    "    \"\"\"Percentile normalization.\"\"\"\n",
    "    frame = frame.astype(np.float32)\n",
    "    p1, p99 = np.percentile(frame, (1, 99.8))\n",
    "    return np.clip((frame - p1) / (p99 - p1 + 1e-8), 0, 1)\n",
    "\n",
    "\n",
    "def detect(models, frame, prob_thresh, nms_thresh):\n",
    "    \"\"\"Ensemble detection.\"\"\"\n",
    "    x = torch.from_numpy(frame).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    all_stardist, all_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            out = model(x)['nuc']\n",
    "            all_stardist.append(out.aux_map.cpu().numpy()[0])\n",
    "            all_prob.append(torch.sigmoid(out.binary_map).cpu().numpy()[0, 0])\n",
    "    \n",
    "    try:\n",
    "        labels = post_proc_stardist(\n",
    "            np.mean(all_prob, axis=0), np.mean(all_stardist, axis=0),\n",
    "            score_thresh=prob_thresh, iou_thresh=nms_thresh\n",
    "        )\n",
    "        return [(p.centroid[1], p.centroid[0]) for p in regionprops(labels)]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Benchmark All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model_name, gt_df, frames_dir):\n",
    "    \"\"\"Benchmark a model on validation frames.\"\"\"\n",
    "    models, inf_config = load_ensemble(model_name)\n",
    "    prob_thresh = inf_config['prob_thresh']\n",
    "    nms_thresh = inf_config['nms_thresh']\n",
    "    \n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    \n",
    "    for frame_idx in tqdm(VALIDATION_FRAMES, desc=model_name):\n",
    "        # Load frame\n",
    "        img = np.array(Image.open(frames_dir / f\"frame_{frame_idx:04d}.png\"))\n",
    "        frame = preprocess(img)\n",
    "        \n",
    "        # Detect\n",
    "        detections = detect(models, frame, prob_thresh, nms_thresh)\n",
    "        \n",
    "        # Get GT\n",
    "        frame_gt = gt_df[gt_df['frame'] == frame_idx][['x', 'y']].values\n",
    "        \n",
    "        # Match (greedy, 5px threshold)\n",
    "        if len(detections) > 0 and len(frame_gt) > 0:\n",
    "            dist = cdist(np.array(detections), frame_gt)\n",
    "            tp = 0\n",
    "            for _ in range(min(len(detections), len(frame_gt))):\n",
    "                min_idx = np.unravel_index(np.argmin(dist), dist.shape)\n",
    "                if dist[min_idx] <= 5.0:\n",
    "                    tp += 1\n",
    "                    dist[min_idx[0], :] = np.inf\n",
    "                    dist[:, min_idx[1]] = np.inf\n",
    "                else:\n",
    "                    break\n",
    "            fp = len(detections) - tp\n",
    "            fn = len(frame_gt) - tp\n",
    "        elif len(detections) > 0:\n",
    "            tp, fp, fn = 0, len(detections), 0\n",
    "        else:\n",
    "            tp, fp, fn = 0, 0, len(frame_gt)\n",
    "        \n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "    \n",
    "    # Cleanup\n",
    "    del models\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Metrics\n",
    "    deta = total_tp / (total_tp + total_fp + total_fn) if (total_tp + total_fp + total_fn) > 0 else 0\n",
    "    prec = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    rec = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    \n",
    "    return {'model': model_name, 'DetA': deta, 'Precision': prec, 'Recall': rec, 'TP': total_tp, 'FP': total_fp, 'FN': total_fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARKING ALL MODELS ON HELD-OUT VALIDATION DATA\")\n",
    "print(f\"Frames: {VALIDATION_FRAMES}\")\n",
    "print(f\"Total GT annotations: {len(gt_df)}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "results = []\n",
    "for model_name in AVAILABLE_MODELS:\n",
    "    result = benchmark_model(model_name, gt_df, FRAMES_DIR)\n",
    "    results.append(result)\n",
    "    print(f\"  {model_name}: DetA={result['DetA']:.4f}, Precision={result['Precision']:.4f}, Recall={result['Recall']:.4f}\\n\")\n",
    "\n",
    "# Find best\n",
    "best_idx = np.argmax([r['DetA'] for r in results])\n",
    "BEST_MODEL = results[best_idx]['model']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"BEST MODEL: {BEST_MODEL} (DetA = {results[best_idx]['DetA']:.4f})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['DetA'] = results_df['DetA'].apply(lambda x: f\"{x:.4f}\")\n",
    "results_df['Precision'] = results_df['Precision'].apply(lambda x: f\"{x:.4f}\")\n",
    "results_df['Recall'] = results_df['Recall'].apply(lambda x: f\"{x:.4f}\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "models = [r['model'] for r in results]\n",
    "metrics = [('DetA', [r['DetA'] if isinstance(r['DetA'], float) else float(r['DetA']) for r in results]),\n",
    "           ('Precision', [r['Precision'] if isinstance(r['Precision'], float) else float(r['Precision']) for r in results]),\n",
    "           ('Recall', [r['Recall'] if isinstance(r['Recall'], float) else float(r['Recall']) for r in results])]\n",
    "\n",
    "# Recalculate from original results\n",
    "results_raw = [benchmark_model(m, gt_df, FRAMES_DIR) for m in AVAILABLE_MODELS] if not isinstance(results[0]['DetA'], float) else results\n",
    "\n",
    "for ax, (name, _) in zip(axes, metrics):\n",
    "    values = [r[name] for r in results_raw] if results_raw else [float(r[name]) for r in results]\n",
    "    bars = ax.bar(range(len(models)), values, color=colors)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels([m.replace('_', '\\n') for m in models], fontsize=8)\n",
    "    ax.set_ylabel(name)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(name, fontweight='bold')\n",
    "    for bar, v in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{v:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Model Comparison on Held-Out Validation (10 frames)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Visualize Best Model Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_models, best_config = load_ensemble(BEST_MODEL)\n",
    "\n",
    "# Visualize first 5 frames\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "for i, frame_idx in enumerate(VALIDATION_FRAMES[:5]):\n",
    "    img = np.array(Image.open(FRAMES_DIR / f\"frame_{frame_idx:04d}.png\"))\n",
    "    frame = preprocess(img)\n",
    "    detections = detect(best_models, frame, best_config['prob_thresh'], best_config['nms_thresh'])\n",
    "    frame_gt = gt_df[gt_df['frame'] == frame_idx][['x', 'y']].values\n",
    "    \n",
    "    # GT (top)\n",
    "    axes[0, i].imshow(img, cmap='gray')\n",
    "    axes[0, i].scatter(frame_gt[:, 0], frame_gt[:, 1], c='lime', s=40, marker='o', facecolors='none', linewidths=1.5)\n",
    "    axes[0, i].set_title(f'Frame {frame_idx} - GT ({len(frame_gt)})')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Predictions (bottom)\n",
    "    axes[1, i].imshow(img, cmap='gray')\n",
    "    if detections:\n",
    "        pred = np.array(detections)\n",
    "        axes[1, i].scatter(pred[:, 0], pred[:, 1], c='red', s=40, marker='x', linewidths=1.5)\n",
    "    axes[1, i].set_title(f'Frame {frame_idx} - Pred ({len(detections)})')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle(f'Best Model: {BEST_MODEL}\\nTop: Ground Truth (green) | Bottom: Predictions (red)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del best_models\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. (Optional) Run Inference on Custom Data\n",
    "\n",
    "Upload your own `.tif` video to run inference with the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell if you don't want to run custom inference\n",
    "RUN_CUSTOM_INFERENCE = False  # Set to True to enable\n",
    "\n",
    "if RUN_CUSTOM_INFERENCE and IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import tifffile\n",
    "    \n",
    "    print(f\"Using best model: {BEST_MODEL}\")\n",
    "    print(\"Upload your .tif video:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        video = tifffile.imread(filename)\n",
    "        print(f\"\\nLoaded {filename}: {video.shape}\")\n",
    "        \n",
    "        # Load model\n",
    "        models, config = load_ensemble(BEST_MODEL)\n",
    "        roi = config.get('roi', {'x_min': 256, 'x_max': 512, 'y_min': 512, 'y_max': 768})\n",
    "        \n",
    "        # Run inference\n",
    "        all_detections = []\n",
    "        for frame_idx in tqdm(range(len(video)), desc=\"Detecting\"):\n",
    "            frame = video[frame_idx, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "            frame = preprocess(frame)\n",
    "            dets = detect(models, frame, config['prob_thresh'], config['nms_thresh'])\n",
    "            for x, y in dets:\n",
    "                all_detections.append({'frame': frame_idx, 'x': x + roi['x_min'], 'y': y + roi['y_min']})\n",
    "        \n",
    "        results_df = pd.DataFrame(all_detections)\n",
    "        print(f\"\\nDetected {len(results_df)} cells across {len(video)} frames\")\n",
    "        \n",
    "        # Save\n",
    "        output_csv = filename.replace('.tif', '_detections.csv')\n",
    "        results_df.to_csv(output_csv, index=False)\n",
    "        files.download(output_csv)\n",
    "        print(f\"Saved to {output_csv}\")\n",
    "        \n",
    "        del models\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
