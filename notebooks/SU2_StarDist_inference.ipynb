{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/veselm73/SU2/blob/main/notebooks/SU2_StarDist_inference.ipynb)\n\n# Cell Detection & Tracking - Competition Inference\n\n**Author:** Mateusz Vesel  \n**Task:** Detect and track cells in microscopy video\n\n---\n\n## Pipeline Overview\n\n### 1. Detection: StarDist with 5-Fold Ensemble\n- **Architecture:** StarDist with ResNet18 encoder\n- **Training:** 5-Fold stratified cross-validation on 120 annotated frames + bonus data\n- **Inference:** Ensemble averaging of probability maps from all 5 folds\n- **Post-processing:** Non-maximum suppression to extract cell centroids\n\n### 2. Tracking: LapTrack\n- **Method:** Linear Assignment Problem (LAP) based tracking\n- **Features:** Frame-to-frame linking with gap closing for missed detections\n\n---\n\n## Available Pre-trained Models\n\n| Model | Epochs | Augmentation | N_Rays |\n|-------|--------|--------------|--------|\n| `100e_noaug_32rays` | 100 | No | 32 |\n| `100e_noaug_64rays` | 100 | No | 64 |\n| `120e_aug_32rays` | 120 | Yes | 32 |\n\n> **Note:** Run Section 8 (Benchmark) to compare models on held-out validation data.\n\n---\n\n## How to Use This Notebook\n\n1. **Select a model** in the configuration cell below\n2. **Run all cells** - weights auto-download from GitHub if needed\n3. Results are evaluated against ground truth and visualized\n4. **Section 8:** Benchmark all models on held-out sanity check data (unbiased evaluation)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (fast with uv)\n",
    "!pip install uv -q\n",
    "!uv pip uninstall torch torchvision torchaudio --system -q 2>/dev/null || true\n",
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --system -q\n",
    "!uv pip install \"numpy<2\" cellseg-models-pytorch pytorch-lightning laptrack tifffile --system -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone/update repository\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path('/content/SU2').exists():\n",
    "        !git clone https://github.com/veselm73/SU2.git /content/SU2\n",
    "    else:\n",
    "        !cd /content/SU2 && git pull\n",
    "    os.chdir('/content/SU2')\n",
    "    repo_root = Path('/content/SU2')\n",
    "else:\n",
    "    notebook_dir = Path(os.getcwd())\n",
    "    repo_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"Repository: {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "from cellseg_models_pytorch.postproc.functional.stardist.stardist import post_proc_stardist\n",
    "\n",
    "# Import tracking and metrics\n",
    "from modules.stardist_helpers import (\n",
    "    run_laptrack,\n",
    "    hota,\n",
    "    ROI_X_MIN, ROI_X_MAX, ROI_Y_MIN, ROI_Y_MAX\n",
    ")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## 2. Model Selection & Configuration\n\n**Choose your model below.** Weights will be auto-downloaded from GitHub if not found locally."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# MODEL SELECTION\n# ============================================================\n# Available pre-trained models:\n#   - \"100e_noaug_32rays\": 100 epochs, no augmentation, 32 rays\n#   - \"100e_noaug_64rays\": 100 epochs, no augmentation, 64 rays\n#   - \"120e_aug_32rays\":   120 epochs, with augmentation, 32 rays\n#\n# Run Section 8 (Benchmark) for unbiased comparison on held-out data.\n\nAVAILABLE_MODELS = {\n    \"100e_noaug_32rays\": \"100 epochs, no aug, 32 rays\",\n    \"100e_noaug_64rays\": \"100 epochs, no aug, 64 rays\",\n    \"120e_aug_32rays\": \"120 epochs, with aug, 32 rays\"\n}\n\n# >>> CHANGE THIS TO SELECT MODEL <<<\nSELECTED_MODEL = \"100e_noaug_32rays\"\n\nprint(\"Available models:\")\nfor name, desc in AVAILABLE_MODELS.items():\n    marker = \">>>\" if name == SELECTED_MODEL else \"   \"\n    print(f\"  {marker} {name}: {desc}\")\nprint(f\"\\nSelected: {SELECTED_MODEL}\")"
  },
  {
   "cell_type": "code",
   "id": "wh9oc1ybphg",
   "source": "import urllib.request\nimport json\n\ndef download_weights_if_needed(model_name, repo_root):\n    \"\"\"Download model weights from GitHub if not present locally.\"\"\"\n    weights_dir = repo_root / \"weights\" / model_name\n    models_dir = weights_dir / \"models\"\n    \n    # Check if weights already exist\n    if models_dir.exists() and len(list(models_dir.glob(\"*.pth\"))) == 5:\n        print(f\"Weights found locally: {weights_dir}\")\n        return weights_dir\n    \n    print(f\"Downloading weights for '{model_name}' from GitHub...\")\n    weights_dir.mkdir(parents=True, exist_ok=True)\n    models_dir.mkdir(parents=True, exist_ok=True)\n    \n    base_url = f\"https://raw.githubusercontent.com/veselm73/SU2/main/weights/{model_name}\"\n    \n    # Download config files\n    for cfg in [\"model_config.json\", \"inference_config.json\"]:\n        url = f\"{base_url}/{cfg}\"\n        dest = weights_dir / cfg\n        print(f\"  Downloading {cfg}...\")\n        urllib.request.urlretrieve(url, dest)\n    \n    # Download fold weights (5 folds, ~53MB each)\n    for fold in range(1, 6):\n        url = f\"{base_url}/models/fold_{fold}.pth\"\n        dest = models_dir / f\"fold_{fold}.pth\"\n        print(f\"  Downloading fold_{fold}.pth (~53MB)...\")\n        urllib.request.urlretrieve(url, dest)\n    \n    print(\"Download complete!\")\n    return weights_dir\n\n\n# Download weights if needed\nWEIGHTS_DIR = download_weights_if_needed(SELECTED_MODEL, repo_root)\n\n# Load configuration from JSON files\nwith open(WEIGHTS_DIR / \"model_config.json\") as f:\n    MODEL_CONFIG = json.load(f)\n\nwith open(WEIGHTS_DIR / \"inference_config.json\") as f:\n    INFERENCE_CONFIG = json.load(f)\n\n# Add ROI from helpers if not in config\nif 'roi' not in INFERENCE_CONFIG:\n    INFERENCE_CONFIG['roi'] = {\n        'x_min': ROI_X_MIN, 'x_max': ROI_X_MAX,\n        'y_min': ROI_Y_MIN, 'y_max': ROI_Y_MAX\n    }\n\n# Set paths\nMODELS_DIR = WEIGHTS_DIR / \"models\"\nVAL_TIF = repo_root / \"data\" / \"val\" / \"val.tif\"\nVAL_CSV = repo_root / \"data\" / \"val\" / \"val.csv\"\nK_FOLDS = 5\n\nprint(\"\\nConfiguration loaded:\")\nprint(f\"  Model: {MODEL_CONFIG['encoder_name']}, n_rays={MODEL_CONFIG['n_rays']}\")\nprint(f\"  Detection: prob_thresh={INFERENCE_CONFIG['prob_thresh']}, nms_thresh={INFERENCE_CONFIG['nms_thresh']}\")\nif 'track_max_dist' in INFERENCE_CONFIG:\n    print(f\"  Tracking: max_dist={INFERENCE_CONFIG['track_max_dist']}px\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from cellseg_models_pytorch.models.stardist.stardist import StarDist\n",
    "import torch.nn as nn\n",
    "\n",
    "class StarDistLightning(pl.LightningModule):\n",
    "    \"\"\"StarDist model wrapper for inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_rays=32, encoder_name=\"resnet18\", dropout=0.0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_rays = n_rays\n",
    "        wrapper = StarDist(\n",
    "            n_nuc_classes=1,\n",
    "            n_rays=n_rays,\n",
    "            enc_name=encoder_name,\n",
    "            model_kwargs={\"encoder_kws\": {\"in_chans\": 1}}\n",
    "        )\n",
    "        self.model = wrapper.model\n",
    "        self.dropout = nn.Dropout2d(p=dropout) if dropout > 0 else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def load_fold_model(fold_path, config):\n",
    "    \"\"\"Load a single fold model from .pth file.\"\"\"\n",
    "    model = StarDistLightning(\n",
    "        n_rays=config['n_rays'],\n",
    "        encoder_name=config['encoder_name'],\n",
    "        dropout=config.get('dropout', 0.0)\n",
    "    )\n",
    "    state_dict = torch.load(fold_path, map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 5-fold ensemble\n",
    "print(f\"Loading models from: {MODELS_DIR}\")\n",
    "\n",
    "fold_models = []\n",
    "for fold in range(1, K_FOLDS + 1):\n",
    "    fold_path = MODELS_DIR / f\"fold_{fold}.pth\"\n",
    "    if fold_path.exists():\n",
    "        model = load_fold_model(fold_path, MODEL_CONFIG)\n",
    "        model = model.to(DEVICE)\n",
    "        fold_models.append(model)\n",
    "        print(f\"  ‚úì Fold {fold}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Fold {fold}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nLoaded {len(fold_models)}/{K_FOLDS} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "def preprocess_frame(frame):\n    \"\"\"Percentile normalization.\"\"\"\n    frame = frame.astype(np.float32)\n    p1, p99 = np.percentile(frame, (1, 99.8))\n    frame = np.clip(frame, p1, p99)\n    frame = (frame - p1) / (p99 - p1 + 1e-8)\n    return frame\n\n\ndef detect_cells_ensemble(models, frame, prob_thresh, nms_thresh, device):\n    \"\"\"Detect cells using ensemble (average probability maps before thresholding).\"\"\"\n    x = torch.from_numpy(frame).float().unsqueeze(0).unsqueeze(0).to(device)\n    \n    all_stardist = []\n    all_prob = []\n    \n    with torch.no_grad():\n        for model in models:\n            out = model(x)\n            nuc_out = out['nuc']\n            stardist_map = nuc_out.aux_map.cpu().numpy()[0]\n            prob_map = torch.sigmoid(nuc_out.binary_map).cpu().numpy()[0, 0]\n            all_stardist.append(stardist_map)\n            all_prob.append(prob_map)\n    \n    avg_stardist = np.mean(all_stardist, axis=0)\n    avg_prob = np.mean(all_prob, axis=0)\n    \n    try:\n        labels = post_proc_stardist(\n            avg_prob, avg_stardist,\n            score_thresh=prob_thresh,\n            iou_thresh=nms_thresh\n        )\n        detections = [(prop.centroid[1], prop.centroid[0]) for prop in regionprops(labels)]\n        return detections\n    except:\n        return []\n\n\ndef infer_video(video_path, config, models=None):\n    \"\"\"Run detection + tracking on video. Returns DataFrame with frame, x, y, track_id.\"\"\"\n    if models is None:\n        models = fold_models\n    \n    video = tifffile.imread(video_path)\n    roi = config['roi']\n    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n    \n    print(f\"Video: {video.shape} -> ROI: {video_roi.shape}\")\n    print(f\"Running ensemble detection ({len(models)} models)...\")\n    \n    all_detections = []\n    for frame_idx in tqdm(range(len(video_roi))):\n        frame = preprocess_frame(video_roi[frame_idx])\n        detections = detect_cells_ensemble(\n            models, frame,\n            config['prob_thresh'], config['nms_thresh'], DEVICE\n        )\n        for x, y in detections:\n            all_detections.append({\n                'frame': frame_idx,\n                'x': x + roi['x_min'],\n                'y': y + roi['y_min']\n            })\n    \n    detections_df = pd.DataFrame(all_detections)\n    print(f\"Detections: {len(detections_df)}\")\n    \n    # Handle different tracking config formats\n    if 'track_max_dist' in config:\n        max_dist = config['track_max_dist']\n        closing_gap = config.get('gap_closing_frames', 2)\n    elif 'track_cost_cutoff' in config:\n        # Convert squared distance to distance\n        max_dist = int(np.sqrt(config['track_cost_cutoff']))\n        closing_gap = config.get('gap_closing_max_frame_count', 1)\n    else:\n        # Default values\n        max_dist = 5\n        closing_gap = 2\n    \n    print(f\"Running tracking (max_dist={max_dist}px, gap={closing_gap})...\")\n    tracked_df = run_laptrack(\n        detections_df,\n        max_dist=max_dist,\n        closing_gap=closing_gap,\n        min_length=2\n    )\n    print(f\"Tracks: {tracked_df['track_id'].nunique()}\")\n    \n    return tracked_df"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Run Inference on Validation Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "if len(fold_models) > 0 and VAL_TIF.exists():\n    predictions = infer_video(VAL_TIF, INFERENCE_CONFIG)\nelse:\n    print(\"Models not loaded or validation video not found.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Evaluation (HOTA Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAL_CSV.exists() and 'predictions' in dir() and len(predictions) > 0:\n",
    "    # Load ground truth\n",
    "    gt_df = pd.read_csv(VAL_CSV)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    gt_roi = gt_df[\n",
    "        (gt_df.x >= roi['x_min']) & (gt_df.x < roi['x_max']) &\n",
    "        (gt_df.y >= roi['y_min']) & (gt_df.y < roi['y_max'])\n",
    "    ].copy()\n",
    "    \n",
    "    # Calculate HOTA\n",
    "    hota_scores = hota(gt_roi, predictions, threshold=5.0)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\n  HOTA: {hota_scores['HOTA']:.4f}\")\n",
    "    print(f\"  DetA: {hota_scores['DetA']:.4f}\")\n",
    "    print(f\"  AssA: {hota_scores['AssA']:.4f}\")\n",
    "    print(f\"\\n  Detections: {len(predictions)}\")\n",
    "    print(f\"  Tracks: {predictions['track_id'].nunique()}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection visualization: GT vs Predictions\n",
    "if VAL_TIF.exists() and 'predictions' in dir() and len(predictions) > 0:\n",
    "    video = tifffile.imread(VAL_TIF)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    \n",
    "    gt_available = VAL_CSV.exists()\n",
    "    if gt_available:\n",
    "        gt_df = pd.read_csv(VAL_CSV)\n",
    "        gt_roi = gt_df[\n",
    "            (gt_df.x >= roi['x_min']) & (gt_df.x < roi['x_max']) &\n",
    "            (gt_df.y >= roi['y_min']) & (gt_df.y < roi['y_max'])\n",
    "        ]\n",
    "    \n",
    "    sample_frames = [0, 30, 60, 90]\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    for ax, fidx in zip(axes, sample_frames):\n",
    "        ax.imshow(video_roi[fidx], cmap='gray')\n",
    "        \n",
    "        if gt_available:\n",
    "            frame_gt = gt_roi[gt_roi.frame == fidx]\n",
    "            ax.scatter(frame_gt.x - roi['x_min'], frame_gt.y - roi['y_min'],\n",
    "                      c='lime', s=40, marker='o', facecolors='none', linewidths=1.5, label='GT')\n",
    "        \n",
    "        frame_preds = predictions[predictions.frame == fidx]\n",
    "        ax.scatter(frame_preds.x - roi['x_min'], frame_preds.y - roi['y_min'],\n",
    "                  c='red', s=25, marker='x', linewidths=1.5, label='Pred')\n",
    "        \n",
    "        ax.set_title(f'Frame {fidx}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    axes[0].legend(loc='upper left')\n",
    "    plt.suptitle('Detection: Green=GT, Red=Predicted', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Tracking visualization: color by track_id\nif 'predictions' in dir() and len(predictions) > 0:\n    video = tifffile.imread(VAL_TIF)\n    roi = INFERENCE_CONFIG['roi']\n    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n    \n    sample_frames = [0, 30, 60, 90]\n    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n    \n    unique_tracks = predictions['track_id'].unique()\n    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_tracks)))\n    track_colors = {t: colors[i % len(colors)] for i, t in enumerate(unique_tracks)}\n    \n    for ax, fidx in zip(axes, sample_frames):\n        ax.imshow(video_roi[fidx], cmap='gray')\n        frame_preds = predictions[predictions.frame == fidx]\n        for _, row in frame_preds.iterrows():\n            ax.scatter(row['x'] - roi['x_min'], row['y'] - roi['y_min'],\n                      c=[track_colors[row['track_id']]], s=30, marker='o')\n        ax.set_title(f'Frame {fidx} ({len(frame_preds)} cells)')\n        ax.axis('off')\n    \n    plt.suptitle('Tracking: color = track ID', fontsize=12)\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "781s9xkd5lq",
   "source": "## 8. Model Benchmark on Held-Out Validation Data\n\n**Unbiased evaluation** comparing all 3 trained models on manually annotated frames (58-67).\n\nThese frames were annotated separately and never seen during training - this gives a true measure of generalization performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "k8sryrcunm",
   "source": "# Download sanity check validation annotations\nSANITY_CHECK_CSV_URL = \"https://raw.githubusercontent.com/veselm73/SU2/main/data/val_annotations/ensemble_sanity_check.csv\"\nSANITY_CHECK_CSV = repo_root / \"data\" / \"val_annotations\" / \"ensemble_sanity_check.csv\"\n\nif not SANITY_CHECK_CSV.exists():\n    print(\"Downloading sanity check annotations...\")\n    SANITY_CHECK_CSV.parent.mkdir(parents=True, exist_ok=True)\n    urllib.request.urlretrieve(SANITY_CHECK_CSV_URL, SANITY_CHECK_CSV)\n    \nsanity_gt = pd.read_csv(SANITY_CHECK_CSV)\nprint(f\"Loaded {len(sanity_gt)} annotations across frames {sanity_gt['frame'].min()}-{sanity_gt['frame'].max()}\")\nprint(f\"Annotations per frame: ~{len(sanity_gt) // sanity_gt['frame'].nunique()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "218qcimy7vm",
   "source": "def benchmark_model_on_sanity_check(model_name, sanity_gt, video, device=DEVICE):\n    \"\"\"\n    Run a single model on sanity check frames and compute detection metrics.\n    Returns DetA score and per-frame detection results.\n    \"\"\"\n    from scipy.spatial.distance import cdist\n    \n    # Load model weights and config\n    weights_dir = download_weights_if_needed(model_name, repo_root)\n    with open(weights_dir / \"model_config.json\") as f:\n        model_config = json.load(f)\n    with open(weights_dir / \"inference_config.json\") as f:\n        inf_config = json.load(f)\n    \n    # Load models\n    models = []\n    for fold in range(1, 6):\n        model = load_fold_model(weights_dir / \"models\" / f\"fold_{fold}.pth\", model_config)\n        model = model.to(device)\n        models.append(model)\n    \n    # Get unique frames in sanity check\n    frames = sorted(sanity_gt['frame'].unique())\n    \n    # ROI config - sanity check uses ROI coordinates already in the CSV\n    # The images are 256x256 crops, so x,y in CSV are relative to ROI\n    roi = inf_config.get('roi', {'x_min': 256, 'x_max': 512, 'y_min': 512, 'y_max': 768})\n    \n    all_results = []\n    total_tp, total_fp, total_fn = 0, 0, 0\n    \n    for frame_idx in tqdm(frames, desc=f\"Benchmarking {model_name}\"):\n        # Get frame from video (already in ROI coordinates in CSV)\n        frame = video[frame_idx, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n        frame_norm = preprocess_frame(frame)\n        \n        # Detect cells\n        detections = detect_cells_ensemble(\n            models, frame_norm,\n            inf_config['prob_thresh'], inf_config['nms_thresh'], device\n        )\n        \n        # Get GT for this frame (coordinates are in ROI space, 0-256)\n        frame_gt = sanity_gt[sanity_gt['frame'] == frame_idx][['x', 'y']].values\n        \n        # Match predictions to GT using Hungarian algorithm\n        if len(detections) > 0 and len(frame_gt) > 0:\n            pred_coords = np.array(detections)  # (x, y) format\n            gt_coords = frame_gt  # (x, y) format\n            \n            # Compute distance matrix\n            dist_matrix = cdist(pred_coords, gt_coords)\n            \n            # Match within threshold (5 pixels)\n            match_thresh = 5.0\n            tp = 0\n            matched_gt = set()\n            matched_pred = set()\n            \n            # Greedy matching (for simplicity)\n            for _ in range(min(len(pred_coords), len(gt_coords))):\n                if dist_matrix.size == 0:\n                    break\n                min_idx = np.unravel_index(np.argmin(dist_matrix), dist_matrix.shape)\n                if dist_matrix[min_idx] <= match_thresh:\n                    tp += 1\n                    matched_pred.add(min_idx[0])\n                    matched_gt.add(min_idx[1])\n                    dist_matrix[min_idx[0], :] = np.inf\n                    dist_matrix[:, min_idx[1]] = np.inf\n                else:\n                    break\n            \n            fp = len(pred_coords) - tp\n            fn = len(gt_coords) - tp\n        elif len(detections) > 0:\n            tp, fp, fn = 0, len(detections), 0\n        elif len(frame_gt) > 0:\n            tp, fp, fn = 0, 0, len(frame_gt)\n        else:\n            tp, fp, fn = 0, 0, 0\n        \n        total_tp += tp\n        total_fp += fp\n        total_fn += fn\n        \n        all_results.append({\n            'frame': frame_idx,\n            'n_gt': len(frame_gt),\n            'n_pred': len(detections),\n            'tp': tp, 'fp': fp, 'fn': fn\n        })\n    \n    # Compute DetA\n    if total_tp + total_fp + total_fn > 0:\n        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n        deta = total_tp / (total_tp + total_fp + total_fn)\n    else:\n        precision, recall, deta = 0, 0, 0\n    \n    # Clean up models from GPU\n    del models\n    torch.cuda.empty_cache()\n    \n    return {\n        'model': model_name,\n        'DetA': deta,\n        'Precision': precision,\n        'Recall': recall,\n        'TP': total_tp,\n        'FP': total_fp,\n        'FN': total_fn,\n        'per_frame': pd.DataFrame(all_results)\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5ggtin8tzfj",
   "source": "# Run benchmark on all 3 models\nprint(\"=\"*60)\nprint(\"BENCHMARK: Comparing 3 Models on Sanity Check Data\")\nprint(\"=\"*60)\nprint(f\"Frames: {sorted(sanity_gt['frame'].unique())}\")\nprint(f\"Total annotations: {len(sanity_gt)}\")\nprint(\"=\"*60)\n\n# Load video once\nvideo = tifffile.imread(VAL_TIF)\n\n# Benchmark each model\nbenchmark_results = []\nfor model_name in AVAILABLE_MODELS.keys():\n    print(f\"\\n>>> Testing: {model_name}\")\n    result = benchmark_model_on_sanity_check(model_name, sanity_gt, video)\n    benchmark_results.append(result)\n    print(f\"    DetA: {result['DetA']:.4f} | Precision: {result['Precision']:.4f} | Recall: {result['Recall']:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f1q6yc2ti6a",
   "source": "# Display benchmark summary table\nsummary_df = pd.DataFrame([{\n    'Model': r['model'],\n    'DetA': f\"{r['DetA']:.4f}\",\n    'Precision': f\"{r['Precision']:.4f}\",\n    'Recall': f\"{r['Recall']:.4f}\",\n    'TP': r['TP'],\n    'FP': r['FP'],\n    'FN': r['FN']\n} for r in benchmark_results])\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"BENCHMARK RESULTS SUMMARY\")\nprint(\"=\"*60)\nprint(summary_df.to_string(index=False))\nprint(\"=\"*60)\n\n# Find best model\nbest_idx = np.argmax([r['DetA'] for r in benchmark_results])\nprint(f\"\\nüèÜ Best Model: {benchmark_results[best_idx]['model']} (DetA={benchmark_results[best_idx]['DetA']:.4f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jlwvqm89kla",
   "source": "# Visualize benchmark: bar chart comparison\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\nmodels = [r['model'] for r in benchmark_results]\ndeta_scores = [r['DetA'] for r in benchmark_results]\nprecision_scores = [r['Precision'] for r in benchmark_results]\nrecall_scores = [r['Recall'] for r in benchmark_results]\n\ncolors = ['#2ecc71', '#3498db', '#e74c3c']\n\n# DetA comparison\nax = axes[0]\nbars = ax.bar(range(len(models)), deta_scores, color=colors)\nax.set_xticks(range(len(models)))\nax.set_xticklabels([m.replace('_', '\\n') for m in models], fontsize=9)\nax.set_ylabel('DetA')\nax.set_title('Detection Accuracy (DetA)', fontweight='bold')\nax.set_ylim(0, 1)\nfor bar, score in zip(bars, deta_scores):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n            f'{score:.3f}', ha='center', va='bottom', fontsize=10)\n\n# Precision comparison\nax = axes[1]\nbars = ax.bar(range(len(models)), precision_scores, color=colors)\nax.set_xticks(range(len(models)))\nax.set_xticklabels([m.replace('_', '\\n') for m in models], fontsize=9)\nax.set_ylabel('Precision')\nax.set_title('Precision (TP / (TP + FP))', fontweight='bold')\nax.set_ylim(0, 1)\nfor bar, score in zip(bars, precision_scores):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n            f'{score:.3f}', ha='center', va='bottom', fontsize=10)\n\n# Recall comparison\nax = axes[2]\nbars = ax.bar(range(len(models)), recall_scores, color=colors)\nax.set_xticks(range(len(models)))\nax.set_xticklabels([m.replace('_', '\\n') for m in models], fontsize=9)\nax.set_ylabel('Recall')\nax.set_title('Recall (TP / (TP + FN))', fontweight='bold')\nax.set_ylim(0, 1)\nfor bar, score in zip(bars, recall_scores):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n            f'{score:.3f}', ha='center', va='bottom', fontsize=10)\n\nplt.suptitle('Model Benchmark on Sanity Check Validation Data (10 frames, 1480 annotations)', \n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "h4mw7nxhyo",
   "source": "# Visualize detections from best model on sample frames\nbest_result = benchmark_results[best_idx]\nbest_model_name = best_result['model']\n\n# Reload best model for visualization\nweights_dir = download_weights_if_needed(best_model_name, repo_root)\nwith open(weights_dir / \"model_config.json\") as f:\n    model_config = json.load(f)\nwith open(weights_dir / \"inference_config.json\") as f:\n    inf_config = json.load(f)\n\nbest_models = []\nfor fold in range(1, 6):\n    model = load_fold_model(weights_dir / \"models\" / f\"fold_{fold}.pth\", model_config)\n    model = model.to(DEVICE)\n    best_models.append(model)\n\nroi = inf_config.get('roi', {'x_min': 256, 'x_max': 512, 'y_min': 512, 'y_max': 768})\nframes_to_show = sorted(sanity_gt['frame'].unique())[:5]  # First 5 frames\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 8))\n\nfor i, frame_idx in enumerate(frames_to_show):\n    # Get frame\n    frame = video[frame_idx, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n    frame_norm = preprocess_frame(frame)\n    \n    # Detect\n    detections = detect_cells_ensemble(\n        best_models, frame_norm,\n        inf_config['prob_thresh'], inf_config['nms_thresh'], DEVICE\n    )\n    \n    # Get GT\n    frame_gt = sanity_gt[sanity_gt['frame'] == frame_idx][['x', 'y']].values\n    \n    # Plot GT (top row)\n    axes[0, i].imshow(frame, cmap='gray')\n    axes[0, i].scatter(frame_gt[:, 0], frame_gt[:, 1], c='lime', s=40, \n                       marker='o', facecolors='none', linewidths=1.5)\n    axes[0, i].set_title(f'Frame {frame_idx} - GT ({len(frame_gt)})')\n    axes[0, i].axis('off')\n    \n    # Plot Predictions (bottom row)\n    axes[1, i].imshow(frame, cmap='gray')\n    if detections:\n        pred_coords = np.array(detections)\n        axes[1, i].scatter(pred_coords[:, 0], pred_coords[:, 1], c='red', s=40, \n                          marker='x', linewidths=1.5)\n    axes[1, i].set_title(f'Frame {frame_idx} - Pred ({len(detections)})')\n    axes[1, i].axis('off')\n\nplt.suptitle(f'Best Model: {best_model_name}\\nTop: Ground Truth (green) | Bottom: Predictions (red)', \n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Cleanup\ndel best_models\ntorch.cuda.empty_cache()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}