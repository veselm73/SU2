{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/veselm73/SU2/blob/main/notebooks/SU2_StarDist_inference.ipynb)\n",
    "\n",
    "# Cell Detection & Tracking - Competition Submission\n",
    "\n",
    "**Author:** Mateusz Vesel  \n",
    "**Task:** Detect and track cells in microscopy video\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "This notebook presents my solution for cell detection and tracking:\n",
    "\n",
    "### 1. Detection: StarDist with 5-Fold Ensemble\n",
    "- **Architecture:** StarDist with ResNet18 encoder\n",
    "- **Training:** 5-Fold cross-validation on 120 annotated frames + bonus data\n",
    "- **Inference:** Ensemble averaging of probability maps from all 5 folds\n",
    "- **Post-processing:** Non-maximum suppression to extract cell centroids\n",
    "\n",
    "### 2. Tracking: LapTrack\n",
    "- **Method:** Linear Assignment Problem (LAP) based tracking\n",
    "- **Features:** Frame-to-frame linking with gap closing for missed detections\n",
    "\n",
    "### Key Results (Validation Set)\n",
    "- **DetA:** ~0.82 (detection accuracy)\n",
    "- **AssA:** ~0.69 (association accuracy)  \n",
    "- **HOTA:** ~0.76 (overall tracking accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run all cells** to load pre-trained models\n",
    "2. **Upload your test video** (TIFF format)\n",
    "3. **Call `infer_video()`** to get predictions\n",
    "4. **Download results** as CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (fast with uv)\n",
    "!pip install uv -q\n",
    "!uv pip uninstall torch torchvision torchaudio --system -q 2>/dev/null || true\n",
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --system -q\n",
    "!uv pip install \"numpy<2\" cellseg-models-pytorch pytorch-lightning laptrack tifffile --system -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone/update repository\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path('/content/SU2').exists():\n",
    "        !git clone https://github.com/veselm73/SU2.git /content/SU2\n",
    "    else:\n",
    "        !cd /content/SU2 && git pull\n",
    "    os.chdir('/content/SU2')\n",
    "    repo_root = Path('/content/SU2')\n",
    "else:\n",
    "    notebook_dir = Path(os.getcwd())\n",
    "    repo_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"Repository: {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "from cellseg_models_pytorch.postproc.functional.stardist.stardist import post_proc_stardist\n",
    "\n",
    "# Import model class and tracking\n",
    "from modules.stardist_helpers import (\n",
    "    run_laptrack,\n",
    "    hota,\n",
    "    ROI_X_MIN, ROI_X_MAX, ROI_Y_MIN, ROI_Y_MAX\n",
    ")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "These settings match the trained models. **Do not modify unless retraining.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL CONFIGURATION (must match training)\n",
    "# ============================================================\n",
    "MODEL_CONFIG = {\n",
    "    \"encoder_name\": \"resnet18\",\n",
    "    \"n_rays\": 32,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# INFERENCE CONFIGURATION (optimized on validation set)\n",
    "# ============================================================\n",
    "INFERENCE_CONFIG = {\n",
    "    # Detection thresholds (tuned via ensemble sweep)\n",
    "    \"prob_thresh\": 0.5,     # score_thresh for post_proc_stardist\n",
    "    \"nms_thresh\": 0.3,      # iou_thresh for post_proc_stardist\n",
    "    \n",
    "    # Tracking parameters\n",
    "    \"track_max_dist\": 5,    # pixels\n",
    "    \"gap_closing_frames\": 2,\n",
    "    \n",
    "    # Region of Interest\n",
    "    \"roi\": {\n",
    "        \"x_min\": ROI_X_MIN,  # 256\n",
    "        \"x_max\": ROI_X_MAX,  # 512\n",
    "        \"y_min\": ROI_Y_MIN,  # 512\n",
    "        \"y_max\": ROI_Y_MAX   # 768\n",
    "    }\n",
    "}\n",
    "\n",
    "# Paths\n",
    "MODELS_DIR = repo_root / \"results\" / \"stardist\" / \"competition\" / \"models\"\n",
    "K_FOLDS = 5\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Encoder: {MODEL_CONFIG['encoder_name']}\")\n",
    "print(f\"  Detection: prob_thresh={INFERENCE_CONFIG['prob_thresh']}, nms_thresh={INFERENCE_CONFIG['nms_thresh']}\")\n",
    "print(f\"  Tracking: max_dist={INFERENCE_CONFIG['track_max_dist']}px, gap={INFERENCE_CONFIG['gap_closing_frames']} frames\")\n",
    "print(f\"  ROI: {INFERENCE_CONFIG['roi']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Models\n",
    "\n",
    "Load the 5-fold ensemble from saved checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StarDistLightning class\n",
    "# This is defined in stardist_helpers but we need to import it properly\n",
    "import pytorch_lightning as pl\n",
    "from cellseg_models_pytorch.models.stardist.stardist import StarDist\n",
    "import torch.nn as nn\n",
    "\n",
    "class StarDistLightning(pl.LightningModule):\n",
    "    \"\"\"StarDist model wrapper for inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_rays=32, encoder_name=\"resnet18\", dropout=0.0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_rays = n_rays\n",
    "        \n",
    "        # Create StarDist model\n",
    "        wrapper = StarDist(\n",
    "            n_nuc_classes=1,\n",
    "            n_rays=n_rays,\n",
    "            enc_name=encoder_name,\n",
    "            model_kwargs={\"encoder_kws\": {\"in_chans\": 1}}\n",
    "        )\n",
    "        self.model = wrapper.model\n",
    "        self.dropout = nn.Dropout2d(p=dropout) if dropout > 0 else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def load_fold_model(fold_path, config):\n",
    "    \"\"\"Load a single fold model from .pth file.\"\"\"\n",
    "    model = StarDistLightning(\n",
    "        n_rays=config['n_rays'],\n",
    "        encoder_name=config['encoder_name'],\n",
    "        dropout=config.get('dropout', 0.0)\n",
    "    )\n",
    "    state_dict = torch.load(fold_path, map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 5 fold models\n",
    "print(\"Loading 5-fold ensemble models...\")\n",
    "print(f\"  Looking in: {MODELS_DIR}\")\n",
    "\n",
    "fold_models = []\n",
    "\n",
    "for fold in range(1, K_FOLDS + 1):\n",
    "    fold_path = MODELS_DIR / f\"fold_{fold}.pth\"\n",
    "    \n",
    "    if fold_path.exists():\n",
    "        model = load_fold_model(fold_path, MODEL_CONFIG)\n",
    "        model = model.to(DEVICE)\n",
    "        fold_models.append(model)\n",
    "        print(f\"  ✓ Fold {fold}: Loaded\")\n",
    "    else:\n",
    "        print(f\"  ✗ Fold {fold}: NOT FOUND at {fold_path}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(fold_models)}/{K_FOLDS} models\")\n",
    "\n",
    "if len(fold_models) == 0:\n",
    "    print(\"\\n⚠️  No models found! Please ensure trained weights are in:\")\n",
    "    print(f\"   {MODELS_DIR}\")\n",
    "    print(\"\\n   Run SU2_StarDist_final.ipynb first to train models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Inference Functions\n",
    "\n",
    "Core functions for detection and tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"Normalize frame using percentile normalization.\"\"\"\n",
    "    frame = frame.astype(np.float32)\n",
    "    p1, p99 = np.percentile(frame, (1, 99.8))\n",
    "    frame = np.clip(frame, p1, p99)\n",
    "    frame = (frame - p1) / (p99 - p1 + 1e-8)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def detect_cells_ensemble(models, frame, prob_thresh, nms_thresh, device):\n",
    "    \"\"\"\n",
    "    Detect cells using ensemble of models.\n",
    "    \n",
    "    Averages probability maps from all models before thresholding.\n",
    "    This is more robust than averaging final detections.\n",
    "    \n",
    "    Args:\n",
    "        models: List of StarDistLightning models\n",
    "        frame: 2D numpy array (H, W), already preprocessed\n",
    "        prob_thresh: Detection threshold (score_thresh)\n",
    "        nms_thresh: NMS threshold (iou_thresh)\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        List of (x, y) coordinates in ROI space\n",
    "    \"\"\"\n",
    "    # Prepare input tensor\n",
    "    x = torch.from_numpy(frame).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Collect predictions from all models\n",
    "    all_stardist = []\n",
    "    all_prob = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            out = model(x)\n",
    "            nuc_out = out['nuc']\n",
    "            \n",
    "            # aux_map = stardist rays (n_rays, H, W)\n",
    "            # binary_map = probability map (1, H, W)\n",
    "            stardist_map = nuc_out.aux_map.cpu().numpy()[0]\n",
    "            prob_map = torch.sigmoid(nuc_out.binary_map).cpu().numpy()[0, 0]\n",
    "            \n",
    "            all_stardist.append(stardist_map)\n",
    "            all_prob.append(prob_map)\n",
    "    \n",
    "    # Average across all folds\n",
    "    avg_stardist = np.mean(all_stardist, axis=0)\n",
    "    avg_prob = np.mean(all_prob, axis=0)\n",
    "    \n",
    "    # Post-process to get detections\n",
    "    try:\n",
    "        labels = post_proc_stardist(\n",
    "            avg_prob,       # dist_map (H, W)\n",
    "            avg_stardist,   # stardist_map (n_rays, H, W)\n",
    "            score_thresh=prob_thresh,\n",
    "            iou_thresh=nms_thresh\n",
    "        )\n",
    "        \n",
    "        detections = []\n",
    "        for prop in regionprops(labels):\n",
    "            cy, cx = prop.centroid\n",
    "            detections.append((cx, cy))\n",
    "        return detections\n",
    "    \n",
    "    except Exception as e:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_video(video_path, config=None, models=None, output_csv=None):\n",
    "    \"\"\"\n",
    "    Run full detection + tracking pipeline on a video.\n",
    "    \n",
    "    This is the main inference API.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to TIFF video file\n",
    "        config: Inference config dict (uses INFERENCE_CONFIG if None)\n",
    "        models: List of models (uses global fold_models if None)\n",
    "        output_csv: Optional path to save predictions\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: frame, x, y, track_id\n",
    "    \n",
    "    Example:\n",
    "        >>> predictions = infer_video('test.tif')\n",
    "        >>> predictions.to_csv('submission.csv', index=False)\n",
    "    \"\"\"\n",
    "    # Use defaults\n",
    "    if config is None:\n",
    "        config = INFERENCE_CONFIG\n",
    "    if models is None:\n",
    "        models = fold_models\n",
    "    \n",
    "    if len(models) == 0:\n",
    "        raise ValueError(\"No models loaded! Run model loading cell first.\")\n",
    "    \n",
    "    # Load video\n",
    "    print(f\"Loading: {video_path}\")\n",
    "    video = tifffile.imread(video_path)\n",
    "    print(f\"  Video shape: {video.shape}\")\n",
    "    \n",
    "    # Extract ROI\n",
    "    roi = config['roi']\n",
    "    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    print(f\"  ROI shape: {video_roi.shape}\")\n",
    "    \n",
    "    # Detection\n",
    "    print(f\"\\nDetecting cells (ensemble of {len(models)} models)...\")\n",
    "    all_detections = []\n",
    "    \n",
    "    for frame_idx in tqdm(range(len(video_roi)), desc=\"Detection\"):\n",
    "        # Preprocess\n",
    "        frame = preprocess_frame(video_roi[frame_idx])\n",
    "        \n",
    "        # Detect\n",
    "        detections = detect_cells_ensemble(\n",
    "            models, frame,\n",
    "            prob_thresh=config['prob_thresh'],\n",
    "            nms_thresh=config['nms_thresh'],\n",
    "            device=DEVICE\n",
    "        )\n",
    "        \n",
    "        # Convert to full image coordinates\n",
    "        for x, y in detections:\n",
    "            all_detections.append({\n",
    "                'frame': frame_idx,\n",
    "                'x': x + roi['x_min'],\n",
    "                'y': y + roi['y_min']\n",
    "            })\n",
    "    \n",
    "    detections_df = pd.DataFrame(all_detections)\n",
    "    print(f\"  Total detections: {len(detections_df)}\")\n",
    "    \n",
    "    if detections_df.empty:\n",
    "        print(\"  ⚠️ No detections found!\")\n",
    "        return pd.DataFrame(columns=['frame', 'x', 'y', 'track_id'])\n",
    "    \n",
    "    # Tracking\n",
    "    print(f\"\\nTracking (max_dist={config['track_max_dist']}px)...\")\n",
    "    tracked_df = run_laptrack(\n",
    "        detections_df,\n",
    "        max_dist=config['track_max_dist'],\n",
    "        closing_gap=config['gap_closing_frames'],\n",
    "        min_length=2\n",
    "    )\n",
    "    print(f\"  Total tracks: {tracked_df['track_id'].nunique()}\")\n",
    "    \n",
    "    # Save if requested\n",
    "    if output_csv:\n",
    "        tracked_df.to_csv(output_csv, index=False)\n",
    "        print(f\"\\n  Saved: {output_csv}\")\n",
    "    \n",
    "    return tracked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Run Inference\n",
    "\n",
    "### Option A: Use validation video (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on validation video\n",
    "VAL_TIF = repo_root / \"data\" / \"val\" / \"val.tif\"\n",
    "VAL_CSV = repo_root / \"data\" / \"val\" / \"val.csv\"\n",
    "OUTPUT_DIR = repo_root / \"results\" / \"stardist\" / \"competition\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if VAL_TIF.exists() and len(fold_models) > 0:\n",
    "    predictions = infer_video(\n",
    "        VAL_TIF,\n",
    "        output_csv=OUTPUT_DIR / \"predictions.csv\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Validation video not found or models not loaded.\")\n",
    "    print(f\"  VAL_TIF exists: {VAL_TIF.exists()}\")\n",
    "    print(f\"  Models loaded: {len(fold_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Option B: Upload your own video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to upload and process your own video\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# test_video_path = list(uploaded.keys())[0]\n",
    "# predictions = infer_video(test_video_path, output_csv=\"my_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 6. Evaluate Results\n",
    "\n",
    "Compare predictions against ground truth (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against ground truth\n",
    "if VAL_CSV.exists() and 'predictions' in dir() and len(predictions) > 0:\n",
    "    print(\"Evaluating against ground truth...\")\n",
    "    \n",
    "    # Load GT\n",
    "    gt_df = pd.read_csv(VAL_CSV)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    gt_roi = gt_df[\n",
    "        (gt_df.x >= roi['x_min']) & (gt_df.x < roi['x_max']) &\n",
    "        (gt_df.y >= roi['y_min']) & (gt_df.y < roi['y_max'])\n",
    "    ].copy()\n",
    "    \n",
    "    # Calculate HOTA metrics\n",
    "    hota_scores = hota(gt_roi, predictions, threshold=5.0)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\n  HOTA: {hota_scores['HOTA']:.4f}\")\n",
    "    print(f\"  DetA: {hota_scores['DetA']:.4f}  (detection accuracy)\")\n",
    "    print(f\"  AssA: {hota_scores['AssA']:.4f}  (association accuracy)\")\n",
    "    print(f\"\\n  Detections: {len(predictions)}\")\n",
    "    print(f\"  Tracks: {predictions['track_id'].nunique()}\")\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(\"Ground truth not available for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs ground truth\n",
    "if VAL_TIF.exists() and 'predictions' in dir() and len(predictions) > 0:\n",
    "    video = tifffile.imread(VAL_TIF)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    \n",
    "    # Load GT if available\n",
    "    gt_available = VAL_CSV.exists()\n",
    "    if gt_available:\n",
    "        gt_df = pd.read_csv(VAL_CSV)\n",
    "        gt_roi = gt_df[\n",
    "            (gt_df.x >= roi['x_min']) & (gt_df.x < roi['x_max']) &\n",
    "            (gt_df.y >= roi['y_min']) & (gt_df.y < roi['y_max'])\n",
    "        ]\n",
    "    \n",
    "    # Plot sample frames\n",
    "    sample_frames = [0, 30, 60, 90]\n",
    "    fig, axes = plt.subplots(1, len(sample_frames), figsize=(16, 4))\n",
    "    \n",
    "    for ax, fidx in zip(axes, sample_frames):\n",
    "        if fidx >= len(video_roi):\n",
    "            continue\n",
    "        \n",
    "        ax.imshow(video_roi[fidx], cmap='gray')\n",
    "        \n",
    "        # Plot GT (green circles)\n",
    "        if gt_available:\n",
    "            frame_gt = gt_roi[gt_roi.frame == fidx]\n",
    "            ax.scatter(\n",
    "                frame_gt.x - roi['x_min'],\n",
    "                frame_gt.y - roi['y_min'],\n",
    "                c='lime', s=40, marker='o', facecolors='none', \n",
    "                linewidths=1.5, label='GT'\n",
    "            )\n",
    "        \n",
    "        # Plot predictions (red crosses)\n",
    "        frame_preds = predictions[predictions.frame == fidx]\n",
    "        ax.scatter(\n",
    "            frame_preds.x - roi['x_min'],\n",
    "            frame_preds.y - roi['y_min'],\n",
    "            c='red', s=25, marker='x', linewidths=1.5, label='Predicted'\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f'Frame {fidx}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    axes[0].legend(loc='upper left')\n",
    "    plt.suptitle('Detection Results: Green=GT, Red=Predicted', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'detection_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracking (color by track_id)\n",
    "if 'predictions' in dir() and len(predictions) > 0:\n",
    "    video = tifffile.imread(VAL_TIF)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    sample_frames = [0, 30, 60, 90]\n",
    "    \n",
    "    # Get unique track IDs and assign colors\n",
    "    unique_tracks = predictions['track_id'].unique()\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_tracks)))\n",
    "    track_to_color = {t: colors[i % len(colors)] for i, t in enumerate(unique_tracks)}\n",
    "    \n",
    "    for ax, fidx in zip(axes, sample_frames):\n",
    "        if fidx >= len(video_roi):\n",
    "            continue\n",
    "        \n",
    "        ax.imshow(video_roi[fidx], cmap='gray')\n",
    "        \n",
    "        frame_preds = predictions[predictions.frame == fidx]\n",
    "        for _, row in frame_preds.iterrows():\n",
    "            color = track_to_color[row['track_id']]\n",
    "            ax.scatter(\n",
    "                row['x'] - roi['x_min'],\n",
    "                row['y'] - roi['y_min'],\n",
    "                c=[color], s=30, marker='o'\n",
    "            )\n",
    "        \n",
    "        ax.set_title(f'Frame {fidx} ({len(frame_preds)} cells)')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Tracking Results (color = track ID)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'tracking_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Export for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of exported files\n",
    "print(\"=\"*60)\n",
    "print(\"COMPETITION SUBMISSION FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(\"\\nFiles:\")\n",
    "for f in sorted(OUTPUT_DIR.glob('*')):\n",
    "    if f.is_file():\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION FORMAT\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "The predictions.csv file contains:\n",
    "  - frame: Frame index (0-based)\n",
    "  - x: Cell x-coordinate (full image)\n",
    "  - y: Cell y-coordinate (full image)  \n",
    "  - track_id: Unique track identifier\n",
    "\n",
    "To submit:\n",
    "  1. Download predictions.csv\n",
    "  2. Rename if needed\n",
    "  3. Upload to competition platform\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download predictions (Colab only)\n",
    "if IN_COLAB and (OUTPUT_DIR / 'predictions.csv').exists():\n",
    "    from google.colab import files\n",
    "    files.download(str(OUTPUT_DIR / 'predictions.csv'))\n",
    "    print(\"Download started!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
