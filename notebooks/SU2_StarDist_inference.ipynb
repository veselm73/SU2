{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UuegVH0SMXf"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/veselm73/SU2/blob/main/notebooks/SU2_StarDist_inference.ipynb)\n",
    "\n",
    "# StarDist Cell Detection & Tracking\n",
    "\n",
    "**Authors:** Matyáš Veselý, Ruslan Guliev\n",
    "\n",
    "This notebook provides inference with a pre-trained **StarDist** ensemble for cell detection in TIRF-SIM microscopy images, followed by **LapTrack** for cell tracking.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "**StarDist** is a deep learning method for object detection that predicts star-convex polygons for each object. Key features:\n",
    "\n",
    "- **Architecture**: ResNet18 encoder + StarDist decoder with 64 radial rays\n",
    "- **Training**: 5-fold cross-validation ensemble (predictions averaged)\n",
    "- **Output**: Probability map + 64 ray distances → NMS → centroid coordinates\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Encoder | ResNet18 |\n",
    "| N_Rays | 64 |\n",
    "| Epochs | 100 |\n",
    "| Augmentation | None |\n",
    "| Input Size | 256×256 (ROI crop) |\n",
    "| Normalization | Percentile (1st, 99.8th) |\n",
    "\n",
    "### K-Fold Training Results (OOF)\n",
    "\n",
    "Out-of-Fold DetA scores using fixed threshold (prob=0.5, nms=0.3):\n",
    "\n",
    "| Fold | DetA | Epochs |\n",
    "|------|------|--------|\n",
    "| 1 | 0.8261 | 100 |\n",
    "| 2 | 0.7691 | 100 |\n",
    "| 3 | 0.8259 | 100 |\n",
    "| 4 | 0.8149 | 100 |\n",
    "| 5 | 0.8286 | 100 |\n",
    "| **Mean** | **0.8129 ± 0.0224** | |\n",
    "\n",
    "### Optimized Inference Thresholds\n",
    "\n",
    "After threshold sweep on OOF predictions:\n",
    "- **prob_thresh**: 0.6\n",
    "- **nms_thresh**: 0.35\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU-s9NSBSMXj"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Fm8l1jR0SMXj"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install uv -q\n",
    "!uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121 --system -q\n",
    "!uv pip install \"numpy<2\" cellseg-models-pytorch pytorch-lightning tifffile scipy laptrack --system -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6yH_q6hSMXk",
    "outputId": "4a443a9a-ba39-48e0-99a1-a955ffe0e146"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Repository: /content/SU2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path('/content/SU2').exists():\n",
    "        !git clone https://github.com/veselm73/SU2.git /content/SU2\n",
    "    os.chdir('/content/SU2')\n",
    "    repo_root = Path('/content/SU2')\n",
    "else:\n",
    "    repo_root = Path('.').resolve()\n",
    "    if repo_root.name == 'notebooks':\n",
    "        repo_root = repo_root.parent\n",
    "\n",
    "print(f\"Repository: {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F_vOL2FxSMXl",
    "outputId": "f82eb134-3ee1-4379-ee10-5f77845170b8"
   },
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom skimage.measure import regionprops\nimport urllib.request\nimport json\nimport tifffile\nimport networkx as nx\nfrom scipy import spatial\nfrom scipy.optimize import linear_sum_assignment\nfrom laptrack import LapTrack\n\nfrom cellseg_models_pytorch.postproc.functional.stardist.stardist import post_proc_stardist\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {DEVICE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNQk64w-SMXl"
   },
   "source": [
    "## 2. Download Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_6K7PYhSMXl",
    "outputId": "b95bac54-7ed9-40a1-fcfc-8f5b6bbb665a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Weights found locally: /content/SU2/weights/100e_noaug_64rays\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"100e_noaug_64rays\"\n",
    "BASE_URL = \"https://raw.githubusercontent.com/veselm73/SU2/main\"\n",
    "\n",
    "def download_model_weights(model_name):\n",
    "    \"\"\"Download model weights from GitHub if not present.\"\"\"\n",
    "    weights_dir = repo_root / \"weights\" / model_name\n",
    "    models_dir = weights_dir / \"models\"\n",
    "\n",
    "    if models_dir.exists() and len(list(models_dir.glob(\"*.pth\"))) == 5:\n",
    "        print(f\"Weights found locally: {weights_dir}\")\n",
    "        return weights_dir\n",
    "\n",
    "    print(f\"Downloading {model_name} weights...\")\n",
    "    weights_dir.mkdir(parents=True, exist_ok=True)\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    base_url = f\"{BASE_URL}/weights/{model_name}\"\n",
    "\n",
    "    for cfg in [\"model_config.json\", \"inference_config.json\"]:\n",
    "        urllib.request.urlretrieve(f\"{base_url}/{cfg}\", weights_dir / cfg)\n",
    "\n",
    "    for fold in range(1, 6):\n",
    "        print(f\"  Downloading fold_{fold}.pth (~53MB)...\")\n",
    "        urllib.request.urlretrieve(f\"{base_url}/models/fold_{fold}.pth\", models_dir / f\"fold_{fold}.pth\")\n",
    "\n",
    "    print(\"Download complete!\")\n",
    "    return weights_dir\n",
    "\n",
    "WEIGHTS_DIR = download_model_weights(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aEryVVNSMXm"
   },
   "source": [
    "## 3. Model Definition & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "8cfc0967300f40608cad107b4f9baa9a",
      "38983730bb4544fe9da1f2186d3ea2f9",
      "993eda99dcad4bf6841dcf31475255f0",
      "497eda9508b54792826e9454898f84da",
      "c1cc22c3a4664fa193cda8a6cb53a0d6",
      "d52fc47445f04f899959d5ebf41efe98",
      "bcfff001bc9c45ccb243b0fb96fb8193",
      "eda4aaa5789d4517bc569269f3a2d1c1",
      "075a936c5d444b3d9164fdb981a20afb",
      "54083025ab684a15ad547da3e0bde177",
      "27e11a6ddcae493798f462b454af8d37"
     ]
    },
    "id": "a_GnPj8lSMXm",
    "outputId": "1c8aeb15-8ca1-4518-a433-d6bf5696460e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cfc0967300f40608cad107b4f9baa9a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 5 models: resnet18, n_rays=64\n",
      "Thresholds: prob=0.6, nms=0.35\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from cellseg_models_pytorch.models.stardist.stardist import StarDist\n",
    "\n",
    "class StarDistModel(pl.LightningModule):\n",
    "    def __init__(self, n_rays=64, encoder_name=\"resnet18\"):\n",
    "        super().__init__()\n",
    "        self.n_rays = n_rays\n",
    "        wrapper = StarDist(\n",
    "            n_nuc_classes=1, n_rays=n_rays, enc_name=encoder_name,\n",
    "            model_kwargs={\"encoder_kws\": {\"in_chans\": 1}}\n",
    "        )\n",
    "        self.model = wrapper.model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def load_ensemble(weights_dir):\n",
    "    \"\"\"Load 5-fold ensemble.\"\"\"\n",
    "    with open(weights_dir / \"model_config.json\") as f:\n",
    "        config = json.load(f)\n",
    "    with open(weights_dir / \"inference_config.json\") as f:\n",
    "        inf_config = json.load(f)\n",
    "\n",
    "    models = []\n",
    "    for fold in range(1, 6):\n",
    "        model = StarDistModel(n_rays=config['n_rays'], encoder_name=config['encoder_name'])\n",
    "        model.load_state_dict(torch.load(weights_dir / \"models\" / f\"fold_{fold}.pth\", map_location='cpu'))\n",
    "        model.eval().to(DEVICE)\n",
    "        models.append(model)\n",
    "\n",
    "    print(f\"Loaded {len(models)} models: {config['encoder_name']}, n_rays={config['n_rays']}\")\n",
    "    print(f\"Thresholds: prob={inf_config['prob_thresh']}, nms={inf_config['nms_thresh']}\")\n",
    "\n",
    "    return models, inf_config\n",
    "\n",
    "# Load ensemble\n",
    "models, INF_CONFIG = load_ensemble(WEIGHTS_DIR)\n",
    "PROB_THRESH = INF_CONFIG['prob_thresh']\n",
    "NMS_THRESH = INF_CONFIG['nms_thresh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm6Qk_u1SMXm"
   },
   "source": [
    "## 4. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GF9QO2KYSMXm",
    "outputId": "541c7291-1ccb-4479-db4a-6499b3e8c0e6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inference functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ROI configuration\n",
    "ROI = {'x_min': 256, 'x_max': 512, 'y_min': 512, 'y_max': 768}\n",
    "\n",
    "def preprocess(frame):\n",
    "    \"\"\"Percentile normalization.\"\"\"\n",
    "    frame = frame.astype(np.float32)\n",
    "    p1, p99 = np.percentile(frame, (1, 99.8))\n",
    "    return np.clip((frame - p1) / (p99 - p1 + 1e-8), 0, 1)\n",
    "\n",
    "\n",
    "def detect_single_frame(models, frame, prob_thresh, nms_thresh):\n",
    "    \"\"\"Run ensemble detection on a single frame.\"\"\"\n",
    "    x = torch.from_numpy(frame).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    all_stardist, all_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            out = model(x)['nuc']\n",
    "            all_stardist.append(out.aux_map.cpu().numpy()[0])\n",
    "            all_prob.append(torch.sigmoid(out.binary_map).cpu().numpy()[0, 0])\n",
    "\n",
    "    try:\n",
    "        labels = post_proc_stardist(\n",
    "            np.mean(all_prob, axis=0), np.mean(all_stardist, axis=0),\n",
    "            score_thresh=prob_thresh, iou_thresh=nms_thresh\n",
    "        )\n",
    "        return [(p.centroid[1], p.centroid[0]) for p in regionprops(labels)]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def run_detection(video, models, prob_thresh, nms_thresh, roi=None):\n",
    "    \"\"\"Run detection on entire video.\"\"\"\n",
    "    if roi:\n",
    "        video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    else:\n",
    "        video_roi = video\n",
    "        roi = {'x_min': 0, 'y_min': 0}\n",
    "\n",
    "    all_detections = []\n",
    "    detections_per_frame = []\n",
    "\n",
    "    for frame_idx in tqdm(range(len(video_roi)), desc=\"Detecting\"):\n",
    "        frame = preprocess(video_roi[frame_idx])\n",
    "        dets = detect_single_frame(models, frame, prob_thresh, nms_thresh)\n",
    "\n",
    "        # Store for tracking (local coords)\n",
    "        detections_per_frame.append(dets)\n",
    "\n",
    "        # Store for output (global coords)\n",
    "        for x, y in dets:\n",
    "            all_detections.append({\n",
    "                'frame': frame_idx,\n",
    "                'x': x + roi['x_min'],\n",
    "                'y': y + roi['y_min']\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(all_detections), detections_per_frame\n",
    "\n",
    "print(\"Inference functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2dDDgHySMXn"
   },
   "source": [
    "## 5. Tracking with LapTrack\n",
    "\n",
    "**LapTrack** uses Linear Assignment Problem (LAP) optimization for frame-to-frame linking with gap closing.\n",
    "\n",
    "Best configuration from benchmark (HOTA=0.9406):\n",
    "- `track_cost_cutoff`: 25 (5px squared)\n",
    "- `gap_closing_cost_cutoff`: 49 (7px squared)  \n",
    "- `gap_closing_max_frame_count`: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yy6RHzpkSMXn",
    "outputId": "53baa753-0ea4-4ffb-f575-20803f1f1ff5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LapTrack config: track=5px, gap=7px, max_frames=1\n"
     ]
    }
   ],
   "source": [
    "# Best LapTrack configuration (HOTA=0.9406 on GT benchmark)\n",
    "LAPTRACK_CONFIG = {\n",
    "    'track_cost_cutoff': 25,        # 5px squared\n",
    "    'gap_closing_cost_cutoff': 49,  # 7px squared\n",
    "    'gap_closing_max_frame_count': 1\n",
    "}\n",
    "\n",
    "def run_tracking(detections_per_frame, config=LAPTRACK_CONFIG):\n",
    "    \"\"\"Track detections using LapTrack.\"\"\"\n",
    "    if len(detections_per_frame) == 0:\n",
    "        return pd.DataFrame(columns=['frame', 'x', 'y', 'track_id'])\n",
    "\n",
    "    # Prepare coordinates\n",
    "    coords_per_frame = []\n",
    "    for dets in detections_per_frame:\n",
    "        if len(dets) > 0:\n",
    "            coords_per_frame.append(np.array([[x, y] for x, y in dets]))\n",
    "        else:\n",
    "            coords_per_frame.append(np.empty((0, 2)))\n",
    "\n",
    "    # Run tracking\n",
    "    tracker = LapTrack(\n",
    "        track_cost_cutoff=config['track_cost_cutoff'],\n",
    "        gap_closing_cost_cutoff=config['gap_closing_cost_cutoff'],\n",
    "        gap_closing_max_frame_count=config['gap_closing_max_frame_count']\n",
    "    )\n",
    "\n",
    "    graph = tracker.predict(coords_per_frame)\n",
    "\n",
    "    # Extract tracks\n",
    "    records = []\n",
    "    for track_id, component in enumerate(nx.weakly_connected_components(graph)):\n",
    "        for node in component:\n",
    "            frame_idx, det_idx = node\n",
    "            if frame_idx < len(coords_per_frame) and det_idx < len(coords_per_frame[frame_idx]):\n",
    "                x, y = coords_per_frame[frame_idx][det_idx]\n",
    "                records.append({'frame': int(frame_idx), 'x': float(x), 'y': float(y), 'track_id': int(track_id)})\n",
    "\n",
    "    if not records:\n",
    "        return pd.DataFrame(columns=['frame', 'x', 'y', 'track_id'])\n",
    "\n",
    "    return pd.DataFrame(records).sort_values(['track_id', 'frame']).reset_index(drop=True)\n",
    "\n",
    "print(f\"LapTrack config: track={np.sqrt(LAPTRACK_CONFIG['track_cost_cutoff']):.0f}px, gap={np.sqrt(LAPTRACK_CONFIG['gap_closing_cost_cutoff']):.0f}px, max_frames={LAPTRACK_CONFIG['gap_closing_max_frame_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP05ftfqSMXn"
   },
   "source": "---\n\n## 6. Load Test Data\n\n**Configure your local data path below.** The data folder should contain:\n- `val.tif` - validation video (TIFF stack)\n- `val.csv` - ground truth annotations (columns: frame, x, y, track_id)\n- `sota.csv` - SOTA predictions for comparison (columns: frame, x, y, track_id)"
  },
  {
   "cell_type": "code",
   "source": "# Upload test data files (Colab only)\n# Required files: val.tif, val.csv, sota.csv\n\nif IN_COLAB:\n    from google.colab import files\n    \n    DATA_DIR = Path(\"/content/data\")\n    DATA_DIR.mkdir(exist_ok=True)\n    \n    print(\"Upload your test data files (val.tif, val.csv, sota.csv):\")\n    uploaded = files.upload()\n    \n    for filename, content in uploaded.items():\n        filepath = DATA_DIR / filename\n        with open(filepath, 'wb') as f:\n            f.write(content)\n        print(f\"Saved: {filepath}\")\nelse:\n    # Local mode: configure your data path here\n    DATA_DIR = Path(r\"C:\\Users\\Mateusz\\SU2\\data\\test_and_sota\")  # <-- Change this for local use\n\nprint(f\"\\nData directory: {DATA_DIR}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "id": "7lN-vs-kSMXn",
    "outputId": "b8bd9550-44bb-4a1f-8e26-b8f71cbe33ef"
   },
   "outputs": [],
   "source": "# Verify data files exist\nrequired_files = [\"val.tif\", \"val.csv\", \"sota.csv\"]\nmissing = [f for f in required_files if not (DATA_DIR / f).exists()]\nif missing:\n    raise FileNotFoundError(f\"Missing files in {DATA_DIR}: {missing}\")\n\n# Load video\nVIDEO_PATH = DATA_DIR / \"val.tif\"\nvideo = tifffile.imread(VIDEO_PATH)\nprint(f\"Loaded: {VIDEO_PATH}\")\nprint(f\"Shape: {video.shape} (frames, height, width)\")\n\n# Load ground truth annotations\nGT_PATH = DATA_DIR / \"val.csv\"\ngt_df = pd.read_csv(GT_PATH)\nprint(f\"\\nGround truth: {len(gt_df)} detections, {gt_df['track_id'].nunique()} tracks\")\n\n# Load SOTA predictions\nSOTA_PATH = DATA_DIR / \"sota.csv\"\nsota_df = pd.read_csv(SOTA_PATH)\nprint(f\"SOTA predictions: {len(sota_df)} detections, {sota_df['track_id'].nunique()} tracks\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQ93FLf6SMXn"
   },
   "outputs": [],
   "source": [
    "# Run detection\n",
    "print(\"\\nRunning detection...\")\n",
    "detections_df, detections_per_frame = run_detection(\n",
    "    video, models, PROB_THRESH, NMS_THRESH, roi=ROI\n",
    ")\n",
    "print(f\"Detected {len(detections_df)} cells across {len(video)} frames\")\n",
    "print(f\"Average: {len(detections_df) / len(video):.1f} cells/frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc0Skb1HSMXn"
   },
   "outputs": [],
   "source": [
    "# Run tracking\n",
    "print(\"\\nRunning tracking...\")\n",
    "tracks_df = run_tracking(detections_per_frame)\n",
    "\n",
    "# Adjust to global coordinates\n",
    "tracks_df['x'] += ROI['x_min']\n",
    "tracks_df['y'] += ROI['y_min']\n",
    "\n",
    "print(f\"Found {tracks_df['track_id'].nunique()} tracks\")\n",
    "print(f\"Average track length: {tracks_df.groupby('track_id').size().mean():.1f} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwv9JMJpSMXn"
   },
   "outputs": [],
   "source": [
    "# Visualize sample frame\n",
    "SAMPLE_FRAME = 0\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Full frame with ROI\n",
    "ax = axes[0]\n",
    "ax.imshow(video[SAMPLE_FRAME], cmap='gray')\n",
    "rect = plt.Rectangle((ROI['x_min'], ROI['y_min']),\n",
    "                      ROI['x_max']-ROI['x_min'], ROI['y_max']-ROI['y_min'],\n",
    "                      linewidth=2, edgecolor='lime', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "ax.set_title(f'Frame {SAMPLE_FRAME} - Full (ROI in green)')\n",
    "ax.axis('off')\n",
    "\n",
    "# ROI with detections\n",
    "ax = axes[1]\n",
    "roi_frame = video[SAMPLE_FRAME, ROI['y_min']:ROI['y_max'], ROI['x_min']:ROI['x_max']]\n",
    "ax.imshow(roi_frame, cmap='gray')\n",
    "\n",
    "frame_dets = detections_df[detections_df['frame'] == SAMPLE_FRAME]\n",
    "if len(frame_dets) > 0:\n",
    "    ax.scatter(frame_dets['x'] - ROI['x_min'], frame_dets['y'] - ROI['y_min'],\n",
    "               c='red', s=50, marker='x', linewidths=1.5)\n",
    "ax.set_title(f'Frame {SAMPLE_FRAME} - ROI with Detections ({len(frame_dets)})')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEU-kFOtSMXo"
   },
   "outputs": [],
   "source": [
    "# Visualize tracks\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(video[0, ROI['y_min']:ROI['y_max'], ROI['x_min']:ROI['x_max']], cmap='gray', alpha=0.5)\n",
    "\n",
    "# Plot sample tracks\n",
    "sample_tracks = np.random.choice(tracks_df['track_id'].unique(),\n",
    "                                  size=min(30, tracks_df['track_id'].nunique()),\n",
    "                                  replace=False)\n",
    "\n",
    "for tid in sample_tracks:\n",
    "    track = tracks_df[tracks_df['track_id'] == tid].sort_values('frame')\n",
    "    ax.plot(track['x'] - ROI['x_min'], track['y'] - ROI['y_min'],\n",
    "            linewidth=1.5, alpha=0.7)\n",
    "\n",
    "ax.set_title(f'Sample Trajectories (n={len(sample_tracks)})')\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 7. Compare with SOTA\n\nCompare StarDist predictions against ground truth and SOTA method using HOTA metrics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Evaluation metric functions\ndef calculate_deta(gt_df, pred_df, match_thresh=5.0):\n    \"\"\"Calculate Detection Accuracy (DetA) using Hungarian matching.\"\"\"\n    gt_df = gt_df.copy()\n    pred_df = pred_df.copy()\n    gt_df['frame'] = gt_df['frame'].astype(int)\n    pred_df['frame'] = pred_df['frame'].astype(int)\n\n    total_tp, total_fp, total_fn = 0, 0, 0\n\n    for frame in gt_df['frame'].unique():\n        gt_frame = gt_df[gt_df['frame'] == frame][['x', 'y']].values\n        pred_frame = pred_df[pred_df['frame'] == frame][['x', 'y']].values\n\n        if len(gt_frame) == 0:\n            total_fp += len(pred_frame)\n            continue\n        if len(pred_frame) == 0:\n            total_fn += len(gt_frame)\n            continue\n\n        dist_matrix = spatial.distance.cdist(gt_frame, pred_frame)\n        row_ind, col_ind = linear_sum_assignment(dist_matrix)\n        matches = sum(dist_matrix[row_ind[i], col_ind[i]] <= match_thresh for i in range(len(row_ind)))\n\n        total_tp += matches\n        total_fp += len(pred_frame) - matches\n        total_fn += len(gt_frame) - matches\n\n    deta = total_tp / max(1, total_tp + total_fp + total_fn)\n    return deta, {'TP': total_tp, 'FP': total_fp, 'FN': total_fn}\n\n\ndef hota(gt: pd.DataFrame, tr: pd.DataFrame, threshold: float = 5) -> dict:\n    \"\"\"Calculate HOTA (Higher Order Tracking Accuracy) metric.\"\"\"\n    gt = gt.copy()\n    tr = tr.copy()\n\n    if 'track_id' not in gt.columns or 'track_id' not in tr.columns:\n        return {'HOTA': 0.0, 'AssA': 0.0, 'DetA': 0.0}\n    if gt.empty or tr.empty:\n        return {'HOTA': 0.0, 'AssA': 0.0, 'DetA': 0.0}\n\n    gt.track_id = gt.track_id.map({old: new for old, new in zip(gt.track_id.unique(), range(gt.track_id.nunique()))})\n    tr.track_id = tr.track_id.map({old: new for old, new in zip(tr.track_id.unique(), range(tr.track_id.nunique()))})\n\n    num_gt_ids = gt.track_id.nunique()\n    num_tr_ids = tr.track_id.nunique()\n\n    matches_count = np.zeros((num_gt_ids, num_tr_ids))\n    gt_id_count = np.zeros((num_gt_ids, 1))\n    tracker_id_count = np.zeros((1, num_tr_ids))\n    HOTA_TP = HOTA_FN = HOTA_FP = 0\n\n    similarities = [1 - np.clip(spatial.distance.cdist(\n        gt[gt.frame == t][['x', 'y']].values,\n        tr[tr.frame == t][['x', 'y']].values\n    ) / threshold, 0, 1) for t in range(int(gt.frame.max()) + 1)]\n\n    for t in range(int(gt.frame.max()) + 1):\n        gt_ids_t = gt[gt.frame == t].track_id.to_numpy()\n        tr_ids_t = tr[tr.frame == t].track_id.to_numpy()\n\n        gt_id_count[gt_ids_t] += 1\n        tracker_id_count[:, tr_ids_t] += 1\n\n        if len(gt_ids_t) == 0:\n            HOTA_FP += len(tr_ids_t)\n            continue\n        if len(tr_ids_t) == 0:\n            HOTA_FN += len(gt_ids_t)\n            continue\n\n        similarity = similarities[t]\n        match_rows, match_cols = linear_sum_assignment(-similarity)\n        mask = similarity[match_rows, match_cols] > 0\n        alpha_match_rows = match_rows[mask]\n        alpha_match_cols = match_cols[mask]\n        num_matches = len(alpha_match_rows)\n\n        HOTA_TP += num_matches\n        HOTA_FN += len(gt_ids_t) - num_matches\n        HOTA_FP += len(tr_ids_t) - num_matches\n\n        if num_matches > 0:\n            matches_count[gt_ids_t[alpha_match_rows], tr_ids_t[alpha_match_cols]] += 1\n\n    ass_a = matches_count / np.maximum(1, gt_id_count + tracker_id_count - matches_count)\n    AssA = np.sum(matches_count * ass_a) / np.maximum(1, HOTA_TP)\n    DetA = HOTA_TP / np.maximum(1, HOTA_TP + HOTA_FN + HOTA_FP)\n    HOTA_score = np.sqrt(DetA * AssA)\n\n    return {'HOTA': float(HOTA_score), 'AssA': float(AssA), 'DetA': float(DetA)}\n\nprint(\"Evaluation functions defined.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run comparison\nprint(\"=\"*60)\nprint(\"PERFORMANCE COMPARISON\")\nprint(\"=\"*60)\n\n# StarDist metrics\nstardist_hota = hota(gt_df, tracks_df)\nprint(f\"\\nStarDist (Ours):\")\nprint(f\"  HOTA={stardist_hota['HOTA']:.4f}, DetA={stardist_hota['DetA']:.4f}, AssA={stardist_hota['AssA']:.4f}\")\nprint(f\"  Tracks: {tracks_df['track_id'].nunique()}, Detections: {len(tracks_df)}\")\n\n# SOTA metrics\nsota_hota = hota(gt_df, sota_df)\nprint(f\"\\nSOTA:\")\nprint(f\"  HOTA={sota_hota['HOTA']:.4f}, DetA={sota_hota['DetA']:.4f}, AssA={sota_hota['AssA']:.4f}\")\nprint(f\"  Tracks: {sota_df['track_id'].nunique()}, Detections: {len(sota_df)}\")\n\n# Ground truth stats\nprint(f\"\\nGround Truth:\")\nprint(f\"  Tracks: {gt_df['track_id'].nunique()}, Detections: {len(gt_df)}\")\n\n# Comparison table\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARISON TABLE\")\nprint(\"=\"*60)\ncomparison_df = pd.DataFrame({\n    'Method': ['StarDist (Ours)', 'SOTA'],\n    'HOTA': [stardist_hota['HOTA'], sota_hota['HOTA']],\n    'DetA': [stardist_hota['DetA'], sota_hota['DetA']],\n    'AssA': [stardist_hota['AssA'], sota_hota['AssA']],\n    'Tracks': [tracks_df['track_id'].nunique(), sota_df['track_id'].nunique()]\n})\nprint(comparison_df.to_string(index=False))\n\n# Bar chart\nfig, ax = plt.subplots(figsize=(10, 6))\nmetrics = ['HOTA', 'DetA', 'AssA']\nx = np.arange(len(metrics))\nwidth = 0.35\nbars1 = ax.bar(x - width/2, [stardist_hota[m] for m in metrics], width, label='StarDist (Ours)', color='steelblue')\nbars2 = ax.bar(x + width/2, [sota_hota[m] for m in metrics], width, label='SOTA', color='coral')\nax.set_ylabel('Score')\nax.set_title('Tracking Performance Comparison')\nax.set_xticks(x)\nax.set_xticklabels(metrics)\nax.legend()\nax.set_ylim(0, 1)\nfor bar in bars1 + bars2:\n    ax.annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n                xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=9)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fp62MuHSMXo"
   },
   "outputs": [],
   "source": "# Save results to local data directory\noutput_dir = DATA_DIR\n\n# Save detections\ndet_path = output_dir / \"stardist_detections.csv\"\ndetections_df.to_csv(det_path, index=False)\nprint(f\"Detections saved: {det_path}\")\n\n# Save tracks\ntracks_path = output_dir / \"stardist_tracks.csv\"\ntracks_df.to_csv(tracks_path, index=False)\nprint(f\"Tracks saved: {tracks_path}\")\n\nprint(f\"\\nNote: Results saved to local folder (not in git repo)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2ohlJ-QSMXo"
   },
   "source": "---\n\n## Summary\n\nThis notebook:\n1. Downloads **StarDist ensemble** weights (5-fold, ResNet18, 64 rays) from GitHub\n2. Loads test data from your **local data folder** (configure `DATA_DIR` in Section 6)\n3. Runs cell detection and **LapTrack** tracking on the validation video\n4. Compares performance against **SOTA method** using HOTA, DetA, AssA metrics\n5. Saves results to your local data folder\n\n**Required files in DATA_DIR:**\n- `val.tif` - validation video\n- `val.csv` - ground truth annotations\n- `sota.csv` - SOTA predictions\n\nFor questions or issues, see: https://github.com/veselm73/SU2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "8cfc0967300f40608cad107b4f9baa9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_38983730bb4544fe9da1f2186d3ea2f9",
       "IPY_MODEL_993eda99dcad4bf6841dcf31475255f0",
       "IPY_MODEL_497eda9508b54792826e9454898f84da"
      ],
      "layout": "IPY_MODEL_c1cc22c3a4664fa193cda8a6cb53a0d6"
     }
    },
    "38983730bb4544fe9da1f2186d3ea2f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d52fc47445f04f899959d5ebf41efe98",
      "placeholder": "​",
      "style": "IPY_MODEL_bcfff001bc9c45ccb243b0fb96fb8193",
      "value": "model.safetensors: 100%"
     }
    },
    "993eda99dcad4bf6841dcf31475255f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eda4aaa5789d4517bc569269f3a2d1c1",
      "max": 46807446,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_075a936c5d444b3d9164fdb981a20afb",
      "value": 46807446
     }
    },
    "497eda9508b54792826e9454898f84da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54083025ab684a15ad547da3e0bde177",
      "placeholder": "​",
      "style": "IPY_MODEL_27e11a6ddcae493798f462b454af8d37",
      "value": " 46.8M/46.8M [00:01&lt;00:00, 2.24MB/s]"
     }
    },
    "c1cc22c3a4664fa193cda8a6cb53a0d6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d52fc47445f04f899959d5ebf41efe98": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcfff001bc9c45ccb243b0fb96fb8193": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eda4aaa5789d4517bc569269f3a2d1c1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "075a936c5d444b3d9164fdb981a20afb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54083025ab684a15ad547da3e0bde177": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27e11a6ddcae493798f462b454af8d37": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}