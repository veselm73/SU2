{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/veselm73/SU2/blob/main/notebooks/SU2_StarDist_inference.ipynb)\n\n# StarDist Cell Detection & Tracking\n\n**Authors:** Matyáš Veselý, Ruslan Guliev\n\nThis notebook provides inference with a pre-trained **StarDist** ensemble for cell detection in TIRF-SIM microscopy images, followed by **LapTrack** for cell tracking.\n\n---\n\n## Model Overview\n\n**StarDist** is a deep learning method for object detection that predicts star-convex polygons for each object. Key features:\n\n- **Architecture**: ResNet18 encoder + StarDist decoder with 64 radial rays\n- **Training**: 5-fold cross-validation ensemble (predictions averaged)\n- **Output**: Probability map + 64 ray distances → NMS → centroid coordinates\n\n### Training Configuration\n\n| Parameter | Value |\n|-----------|-------|\n| Encoder | ResNet18 |\n| N_Rays | 64 |\n| Epochs | 100 |\n| Augmentation | None |\n| Input Size | 256×256 (ROI crop) |\n| Normalization | Percentile (1st, 99.8th) |\n\n### K-Fold Training Results (OOF)\n\nOut-of-Fold DetA scores using fixed threshold (prob=0.5, nms=0.3):\n\n| Fold | DetA | Epochs |\n|------|------|--------|\n| 1 | 0.8261 | 100 |\n| 2 | 0.7691 | 100 |\n| 3 | 0.8259 | 100 |\n| 4 | 0.8149 | 100 |\n| 5 | 0.8286 | 100 |\n| **Mean** | **0.8129 ± 0.0224** | |\n\n### Optimized Inference Thresholds\n\nAfter threshold sweep on OOF predictions:\n- **prob_thresh**: 0.6\n- **nms_thresh**: 0.35\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install uv -q\n",
    "!uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121 --system -q\n",
    "!uv pip install \"numpy<2\" cellseg-models-pytorch pytorch-lightning tifffile scipy laptrack --system -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path('/content/SU2').exists():\n",
    "        !git clone https://github.com/veselm73/SU2.git /content/SU2\n",
    "    os.chdir('/content/SU2')\n",
    "    repo_root = Path('/content/SU2')\n",
    "else:\n",
    "    repo_root = Path('.').resolve()\n",
    "    if repo_root.name == 'notebooks':\n",
    "        repo_root = repo_root.parent\n",
    "\n",
    "print(f\"Repository: {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.measure import regionprops\n",
    "import urllib.request\n",
    "import json\n",
    "import tifffile\n",
    "import networkx as nx\n",
    "from laptrack import LapTrack\n",
    "\n",
    "from cellseg_models_pytorch.postproc.functional.stardist.stardist import post_proc_stardist\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"100e_noaug_64rays\"\n",
    "BASE_URL = \"https://raw.githubusercontent.com/veselm73/SU2/main\"\n",
    "\n",
    "def download_model_weights(model_name):\n",
    "    \"\"\"Download model weights from GitHub if not present.\"\"\"\n",
    "    weights_dir = repo_root / \"weights\" / model_name\n",
    "    models_dir = weights_dir / \"models\"\n",
    "    \n",
    "    if models_dir.exists() and len(list(models_dir.glob(\"*.pth\"))) == 5:\n",
    "        print(f\"Weights found locally: {weights_dir}\")\n",
    "        return weights_dir\n",
    "    \n",
    "    print(f\"Downloading {model_name} weights...\")\n",
    "    weights_dir.mkdir(parents=True, exist_ok=True)\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    base_url = f\"{BASE_URL}/weights/{model_name}\"\n",
    "    \n",
    "    for cfg in [\"model_config.json\", \"inference_config.json\"]:\n",
    "        urllib.request.urlretrieve(f\"{base_url}/{cfg}\", weights_dir / cfg)\n",
    "    \n",
    "    for fold in range(1, 6):\n",
    "        print(f\"  Downloading fold_{fold}.pth (~53MB)...\")\n",
    "        urllib.request.urlretrieve(f\"{base_url}/models/fold_{fold}.pth\", models_dir / f\"fold_{fold}.pth\")\n",
    "    \n",
    "    print(\"Download complete!\")\n",
    "    return weights_dir\n",
    "\n",
    "WEIGHTS_DIR = download_model_weights(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from cellseg_models_pytorch.models.stardist.stardist import StarDist\n",
    "\n",
    "class StarDistModel(pl.LightningModule):\n",
    "    def __init__(self, n_rays=64, encoder_name=\"resnet18\"):\n",
    "        super().__init__()\n",
    "        self.n_rays = n_rays\n",
    "        wrapper = StarDist(\n",
    "            n_nuc_classes=1, n_rays=n_rays, enc_name=encoder_name,\n",
    "            model_kwargs={\"encoder_kws\": {\"in_chans\": 1}}\n",
    "        )\n",
    "        self.model = wrapper.model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def load_ensemble(weights_dir):\n",
    "    \"\"\"Load 5-fold ensemble.\"\"\"\n",
    "    with open(weights_dir / \"model_config.json\") as f:\n",
    "        config = json.load(f)\n",
    "    with open(weights_dir / \"inference_config.json\") as f:\n",
    "        inf_config = json.load(f)\n",
    "    \n",
    "    models = []\n",
    "    for fold in range(1, 6):\n",
    "        model = StarDistModel(n_rays=config['n_rays'], encoder_name=config['encoder_name'])\n",
    "        model.load_state_dict(torch.load(weights_dir / \"models\" / f\"fold_{fold}.pth\", map_location='cpu'))\n",
    "        model.eval().to(DEVICE)\n",
    "        models.append(model)\n",
    "    \n",
    "    print(f\"Loaded {len(models)} models: {config['encoder_name']}, n_rays={config['n_rays']}\")\n",
    "    print(f\"Thresholds: prob={inf_config['prob_thresh']}, nms={inf_config['nms_thresh']}\")\n",
    "    \n",
    "    return models, inf_config\n",
    "\n",
    "# Load ensemble\n",
    "models, INF_CONFIG = load_ensemble(WEIGHTS_DIR)\n",
    "PROB_THRESH = INF_CONFIG['prob_thresh']\n",
    "NMS_THRESH = INF_CONFIG['nms_thresh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI configuration\n",
    "ROI = {'x_min': 256, 'x_max': 512, 'y_min': 512, 'y_max': 768}\n",
    "\n",
    "def preprocess(frame):\n",
    "    \"\"\"Percentile normalization.\"\"\"\n",
    "    frame = frame.astype(np.float32)\n",
    "    p1, p99 = np.percentile(frame, (1, 99.8))\n",
    "    return np.clip((frame - p1) / (p99 - p1 + 1e-8), 0, 1)\n",
    "\n",
    "\n",
    "def detect_single_frame(models, frame, prob_thresh, nms_thresh):\n",
    "    \"\"\"Run ensemble detection on a single frame.\"\"\"\n",
    "    x = torch.from_numpy(frame).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    all_stardist, all_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            out = model(x)['nuc']\n",
    "            all_stardist.append(out.aux_map.cpu().numpy()[0])\n",
    "            all_prob.append(torch.sigmoid(out.binary_map).cpu().numpy()[0, 0])\n",
    "    \n",
    "    try:\n",
    "        labels = post_proc_stardist(\n",
    "            np.mean(all_prob, axis=0), np.mean(all_stardist, axis=0),\n",
    "            score_thresh=prob_thresh, iou_thresh=nms_thresh\n",
    "        )\n",
    "        return [(p.centroid[1], p.centroid[0]) for p in regionprops(labels)]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def run_detection(video, models, prob_thresh, nms_thresh, roi=None):\n",
    "    \"\"\"Run detection on entire video.\"\"\"\n",
    "    if roi:\n",
    "        video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    else:\n",
    "        video_roi = video\n",
    "        roi = {'x_min': 0, 'y_min': 0}\n",
    "    \n",
    "    all_detections = []\n",
    "    detections_per_frame = []\n",
    "    \n",
    "    for frame_idx in tqdm(range(len(video_roi)), desc=\"Detecting\"):\n",
    "        frame = preprocess(video_roi[frame_idx])\n",
    "        dets = detect_single_frame(models, frame, prob_thresh, nms_thresh)\n",
    "        \n",
    "        # Store for tracking (local coords)\n",
    "        detections_per_frame.append(dets)\n",
    "        \n",
    "        # Store for output (global coords)\n",
    "        for x, y in dets:\n",
    "            all_detections.append({\n",
    "                'frame': frame_idx,\n",
    "                'x': x + roi['x_min'],\n",
    "                'y': y + roi['y_min']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_detections), detections_per_frame\n",
    "\n",
    "print(\"Inference functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Tracking with LapTrack\n\n**LapTrack** uses Linear Assignment Problem (LAP) optimization for frame-to-frame linking with gap closing.\n\nBest configuration from benchmark (HOTA=0.9406):\n- `track_cost_cutoff`: 25 (5px squared)\n- `gap_closing_cost_cutoff`: 49 (7px squared)  \n- `gap_closing_max_frame_count`: 1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Best LapTrack configuration (HOTA=0.9406 on GT benchmark)\nLAPTRACK_CONFIG = {\n    'track_cost_cutoff': 25,        # 5px squared\n    'gap_closing_cost_cutoff': 49,  # 7px squared\n    'gap_closing_max_frame_count': 1\n}\n\ndef run_tracking(detections_per_frame, config=LAPTRACK_CONFIG):\n    \"\"\"Track detections using LapTrack.\"\"\"\n    if len(detections_per_frame) == 0:\n        return pd.DataFrame(columns=['frame', 'x', 'y', 'track_id'])\n    \n    # Prepare coordinates\n    coords_per_frame = []\n    for dets in detections_per_frame:\n        if len(dets) > 0:\n            coords_per_frame.append(np.array([[x, y] for x, y in dets]))\n        else:\n            coords_per_frame.append(np.empty((0, 2)))\n    \n    # Run tracking\n    tracker = LapTrack(\n        track_cost_cutoff=config['track_cost_cutoff'],\n        gap_closing_cost_cutoff=config['gap_closing_cost_cutoff'],\n        gap_closing_max_frame_count=config['gap_closing_max_frame_count']\n    )\n    \n    graph = tracker.predict(coords_per_frame)\n    \n    # Extract tracks\n    records = []\n    for track_id, component in enumerate(nx.weakly_connected_components(graph)):\n        for node in component:\n            frame_idx, det_idx = node\n            if frame_idx < len(coords_per_frame) and det_idx < len(coords_per_frame[frame_idx]):\n                x, y = coords_per_frame[frame_idx][det_idx]\n                records.append({'frame': int(frame_idx), 'x': float(x), 'y': float(y), 'track_id': int(track_id)})\n    \n    if not records:\n        return pd.DataFrame(columns=['frame', 'x', 'y', 'track_id'])\n    \n    return pd.DataFrame(records).sort_values(['track_id', 'frame']).reset_index(drop=True)\n\nprint(f\"LapTrack config: track={np.sqrt(LAPTRACK_CONFIG['track_cost_cutoff']):.0f}px, gap={np.sqrt(LAPTRACK_CONFIG['gap_closing_cost_cutoff']):.0f}px, max_frames={LAPTRACK_CONFIG['gap_closing_max_frame_count']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Run Inference on Your Data\n",
    "\n",
    "Upload a `.tif` video file to run detection and tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload video\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"Upload your .tif video:\")\n",
    "    uploaded = files.upload()\n",
    "    VIDEO_PATH = list(uploaded.keys())[0]\n",
    "else:\n",
    "    VIDEO_PATH = input(\"Enter path to .tif video: \")\n",
    "\n",
    "# Load video\n",
    "video = tifffile.imread(VIDEO_PATH)\n",
    "print(f\"\\nLoaded: {VIDEO_PATH}\")\n",
    "print(f\"Shape: {video.shape} (frames, height, width)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection\n",
    "print(\"\\nRunning detection...\")\n",
    "detections_df, detections_per_frame = run_detection(\n",
    "    video, models, PROB_THRESH, NMS_THRESH, roi=ROI\n",
    ")\n",
    "print(f\"Detected {len(detections_df)} cells across {len(video)} frames\")\n",
    "print(f\"Average: {len(detections_df) / len(video):.1f} cells/frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tracking\n",
    "print(\"\\nRunning tracking...\")\n",
    "tracks_df = run_tracking(detections_per_frame)\n",
    "\n",
    "# Adjust to global coordinates\n",
    "tracks_df['x'] += ROI['x_min']\n",
    "tracks_df['y'] += ROI['y_min']\n",
    "\n",
    "print(f\"Found {tracks_df['track_id'].nunique()} tracks\")\n",
    "print(f\"Average track length: {tracks_df.groupby('track_id').size().mean():.1f} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample frame\n",
    "SAMPLE_FRAME = 0\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Full frame with ROI\n",
    "ax = axes[0]\n",
    "ax.imshow(video[SAMPLE_FRAME], cmap='gray')\n",
    "rect = plt.Rectangle((ROI['x_min'], ROI['y_min']), \n",
    "                      ROI['x_max']-ROI['x_min'], ROI['y_max']-ROI['y_min'],\n",
    "                      linewidth=2, edgecolor='lime', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "ax.set_title(f'Frame {SAMPLE_FRAME} - Full (ROI in green)')\n",
    "ax.axis('off')\n",
    "\n",
    "# ROI with detections\n",
    "ax = axes[1]\n",
    "roi_frame = video[SAMPLE_FRAME, ROI['y_min']:ROI['y_max'], ROI['x_min']:ROI['x_max']]\n",
    "ax.imshow(roi_frame, cmap='gray')\n",
    "\n",
    "frame_dets = detections_df[detections_df['frame'] == SAMPLE_FRAME]\n",
    "if len(frame_dets) > 0:\n",
    "    ax.scatter(frame_dets['x'] - ROI['x_min'], frame_dets['y'] - ROI['y_min'], \n",
    "               c='red', s=50, marker='x', linewidths=1.5)\n",
    "ax.set_title(f'Frame {SAMPLE_FRAME} - ROI with Detections ({len(frame_dets)})')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(video[0, ROI['y_min']:ROI['y_max'], ROI['x_min']:ROI['x_max']], cmap='gray', alpha=0.5)\n",
    "\n",
    "# Plot sample tracks\n",
    "sample_tracks = np.random.choice(tracks_df['track_id'].unique(), \n",
    "                                  size=min(30, tracks_df['track_id'].nunique()), \n",
    "                                  replace=False)\n",
    "\n",
    "for tid in sample_tracks:\n",
    "    track = tracks_df[tracks_df['track_id'] == tid].sort_values('frame')\n",
    "    ax.plot(track['x'] - ROI['x_min'], track['y'] - ROI['y_min'], \n",
    "            linewidth=1.5, alpha=0.7)\n",
    "\n",
    "ax.set_title(f'Sample Trajectories (n={len(sample_tracks)})')\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_base = VIDEO_PATH.replace('.tif', '')\n",
    "\n",
    "# Save detections\n",
    "det_path = f\"{output_base}_detections.csv\"\n",
    "detections_df.to_csv(det_path, index=False)\n",
    "print(f\"Detections saved: {det_path}\")\n",
    "\n",
    "# Save tracks\n",
    "tracks_path = f\"{output_base}_tracks.csv\"\n",
    "tracks_df.to_csv(tracks_path, index=False)\n",
    "print(f\"Tracks saved: {tracks_path}\")\n",
    "\n",
    "# Download in Colab\n",
    "if IN_COLAB:\n",
    "    files.download(det_path)\n",
    "    files.download(tracks_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "1. **StarDist ensemble** (5-fold, ResNet18, 64 rays) for cell detection\n",
    "2. **LapTrack** for cell tracking with optimized parameters\n",
    "3. **Output**: CSV files with detections and tracks\n",
    "\n",
    "For questions or issues, see: https://github.com/veselm73/SU2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}