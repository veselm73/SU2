{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/veselm73/SU2/blob/main/notebooks/SU2_StarDist_inference.ipynb)\n",
    "\n",
    "# Cell Detection & Tracking - Competition Results\n",
    "\n",
    "**Author:** Mateusz Vesel  \n",
    "**Task:** Detect and track cells in microscopy video\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "### 1. Detection: StarDist with 5-Fold Ensemble\n",
    "- **Architecture:** StarDist with ResNet18 encoder (32 rays)\n",
    "- **Training:** 5-Fold stratified cross-validation on 120 annotated frames + bonus data\n",
    "- **Inference:** Ensemble averaging of probability maps from all 5 folds\n",
    "- **Post-processing:** Non-maximum suppression to extract cell centroids\n",
    "\n",
    "### 2. Tracking: LapTrack\n",
    "- **Method:** Linear Assignment Problem (LAP) based tracking\n",
    "- **Features:** Frame-to-frame linking with gap closing for missed detections\n",
    "- **Parameters:** max_dist=5px, gap_closing=2 frames\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run all cells** to load pre-trained models and run inference\n",
    "2. Results are evaluated against ground truth and visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (fast with uv)\n",
    "!pip install uv -q\n",
    "!uv pip uninstall torch torchvision torchaudio --system -q 2>/dev/null || true\n",
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --system -q\n",
    "!uv pip install \"numpy<2\" cellseg-models-pytorch pytorch-lightning laptrack tifffile --system -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone/update repository\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path('/content/SU2').exists():\n",
    "        !git clone https://github.com/veselm73/SU2.git /content/SU2\n",
    "    else:\n",
    "        !cd /content/SU2 && git pull\n",
    "    os.chdir('/content/SU2')\n",
    "    repo_root = Path('/content/SU2')\n",
    "else:\n",
    "    notebook_dir = Path(os.getcwd())\n",
    "    repo_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"Repository: {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "from cellseg_models_pytorch.postproc.functional.stardist.stardist import post_proc_stardist\n",
    "\n",
    "# Import tracking and metrics\n",
    "from modules.stardist_helpers import (\n",
    "    run_laptrack,\n",
    "    hota,\n",
    "    ROI_X_MIN, ROI_X_MAX, ROI_Y_MIN, ROI_Y_MAX\n",
    ")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (must match training)\n",
    "MODEL_CONFIG = {\n",
    "    \"encoder_name\": \"resnet18\",\n",
    "    \"n_rays\": 32,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "# Inference configuration (optimized on validation set)\n",
    "INFERENCE_CONFIG = {\n",
    "    \"prob_thresh\": 0.5,       # Detection threshold\n",
    "    \"nms_thresh\": 0.3,        # NMS threshold\n",
    "    \"track_max_dist\": 5,      # Tracking: max distance (pixels)\n",
    "    \"gap_closing_frames\": 2,  # Tracking: gap closing\n",
    "    \"roi\": {\n",
    "        \"x_min\": ROI_X_MIN,   # 256\n",
    "        \"x_max\": ROI_X_MAX,   # 512\n",
    "        \"y_min\": ROI_Y_MIN,   # 512\n",
    "        \"y_max\": ROI_Y_MAX    # 768\n",
    "    }\n",
    "}\n",
    "\n",
    "# Paths\n",
    "MODELS_DIR = repo_root / \"results\" / \"stardist\" / \"competition\" / \"models\"\n",
    "VAL_TIF = repo_root / \"data\" / \"val\" / \"val.tif\"\n",
    "VAL_CSV = repo_root / \"data\" / \"val\" / \"val.csv\"\n",
    "K_FOLDS = 5\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_CONFIG['encoder_name']}, n_rays={MODEL_CONFIG['n_rays']}\")\n",
    "print(f\"  Detection: prob={INFERENCE_CONFIG['prob_thresh']}, nms={INFERENCE_CONFIG['nms_thresh']}\")\n",
    "print(f\"  Tracking: max_dist={INFERENCE_CONFIG['track_max_dist']}px, gap={INFERENCE_CONFIG['gap_closing_frames']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from cellseg_models_pytorch.models.stardist.stardist import StarDist\n",
    "import torch.nn as nn\n",
    "\n",
    "class StarDistLightning(pl.LightningModule):\n",
    "    \"\"\"StarDist model wrapper for inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_rays=32, encoder_name=\"resnet18\", dropout=0.0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_rays = n_rays\n",
    "        wrapper = StarDist(\n",
    "            n_nuc_classes=1,\n",
    "            n_rays=n_rays,\n",
    "            enc_name=encoder_name,\n",
    "            model_kwargs={\"encoder_kws\": {\"in_chans\": 1}}\n",
    "        )\n",
    "        self.model = wrapper.model\n",
    "        self.dropout = nn.Dropout2d(p=dropout) if dropout > 0 else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def load_fold_model(fold_path, config):\n",
    "    \"\"\"Load a single fold model from .pth file.\"\"\"\n",
    "    model = StarDistLightning(\n",
    "        n_rays=config['n_rays'],\n",
    "        encoder_name=config['encoder_name'],\n",
    "        dropout=config.get('dropout', 0.0)\n",
    "    )\n",
    "    state_dict = torch.load(fold_path, map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 5-fold ensemble\n",
    "print(f\"Loading models from: {MODELS_DIR}\")\n",
    "\n",
    "fold_models = []\n",
    "for fold in range(1, K_FOLDS + 1):\n",
    "    fold_path = MODELS_DIR / f\"fold_{fold}.pth\"\n",
    "    if fold_path.exists():\n",
    "        model = load_fold_model(fold_path, MODEL_CONFIG)\n",
    "        model = model.to(DEVICE)\n",
    "        fold_models.append(model)\n",
    "        print(f\"  ✓ Fold {fold}\")\n",
    "    else:\n",
    "        print(f\"  ✗ Fold {fold}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nLoaded {len(fold_models)}/{K_FOLDS} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"Percentile normalization.\"\"\"\n",
    "    frame = frame.astype(np.float32)\n",
    "    p1, p99 = np.percentile(frame, (1, 99.8))\n",
    "    frame = np.clip(frame, p1, p99)\n",
    "    frame = (frame - p1) / (p99 - p1 + 1e-8)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def detect_cells_ensemble(models, frame, prob_thresh, nms_thresh, device):\n",
    "    \"\"\"Detect cells using ensemble (average probability maps before thresholding).\"\"\"\n",
    "    x = torch.from_numpy(frame).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    all_stardist = []\n",
    "    all_prob = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            out = model(x)\n",
    "            nuc_out = out['nuc']\n",
    "            stardist_map = nuc_out.aux_map.cpu().numpy()[0]\n",
    "            prob_map = torch.sigmoid(nuc_out.binary_map).cpu().numpy()[0, 0]\n",
    "            all_stardist.append(stardist_map)\n",
    "            all_prob.append(prob_map)\n",
    "    \n",
    "    avg_stardist = np.mean(all_stardist, axis=0)\n",
    "    avg_prob = np.mean(all_prob, axis=0)\n",
    "    \n",
    "    try:\n",
    "        labels = post_proc_stardist(\n",
    "            avg_prob, avg_stardist,\n",
    "            score_thresh=prob_thresh,\n",
    "            iou_thresh=nms_thresh\n",
    "        )\n",
    "        detections = [(prop.centroid[1], prop.centroid[0]) for prop in regionprops(labels)]\n",
    "        return detections\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def infer_video(video_path, config=INFERENCE_CONFIG, models=None):\n",
    "    \"\"\"Run detection + tracking on video. Returns DataFrame with frame, x, y, track_id.\"\"\"\n",
    "    if models is None:\n",
    "        models = fold_models\n",
    "    \n",
    "    video = tifffile.imread(video_path)\n",
    "    roi = config['roi']\n",
    "    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    \n",
    "    print(f\"Video: {video.shape} -> ROI: {video_roi.shape}\")\n",
    "    print(f\"Running ensemble detection ({len(models)} models)...\")\n",
    "    \n",
    "    all_detections = []\n",
    "    for frame_idx in tqdm(range(len(video_roi))):\n",
    "        frame = preprocess_frame(video_roi[frame_idx])\n",
    "        detections = detect_cells_ensemble(\n",
    "            models, frame,\n",
    "            config['prob_thresh'], config['nms_thresh'], DEVICE\n",
    "        )\n",
    "        for x, y in detections:\n",
    "            all_detections.append({\n",
    "                'frame': frame_idx,\n",
    "                'x': x + roi['x_min'],\n",
    "                'y': y + roi['y_min']\n",
    "            })\n",
    "    \n",
    "    detections_df = pd.DataFrame(all_detections)\n",
    "    print(f\"Detections: {len(detections_df)}\")\n",
    "    \n",
    "    print(f\"Running tracking (max_dist={config['track_max_dist']}px)...\")\n",
    "    tracked_df = run_laptrack(\n",
    "        detections_df,\n",
    "        max_dist=config['track_max_dist'],\n",
    "        closing_gap=config['gap_closing_frames'],\n",
    "        min_length=2\n",
    "    )\n",
    "    print(f\"Tracks: {tracked_df['track_id'].nunique()}\")\n",
    "    \n",
    "    return tracked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Run Inference on Validation Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(fold_models) > 0 and VAL_TIF.exists():\n",
    "    predictions = infer_video(VAL_TIF)\n",
    "else:\n",
    "    print(\"Models not loaded or validation video not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Evaluation (HOTA Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAL_CSV.exists() and 'predictions' in dir() and len(predictions) > 0:\n",
    "    # Load ground truth\n",
    "    gt_df = pd.read_csv(VAL_CSV)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    gt_roi = gt_df[\n",
    "        (gt_df.x >= roi['x_min']) & (gt_df.x < roi['x_max']) &\n",
    "        (gt_df.y >= roi['y_min']) & (gt_df.y < roi['y_max'])\n",
    "    ].copy()\n",
    "    \n",
    "    # Calculate HOTA\n",
    "    hota_scores = hota(gt_roi, predictions, threshold=5.0)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\n  HOTA: {hota_scores['HOTA']:.4f}\")\n",
    "    print(f\"  DetA: {hota_scores['DetA']:.4f}\")\n",
    "    print(f\"  AssA: {hota_scores['AssA']:.4f}\")\n",
    "    print(f\"\\n  Detections: {len(predictions)}\")\n",
    "    print(f\"  Tracks: {predictions['track_id'].nunique()}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection visualization: GT vs Predictions\n",
    "if VAL_TIF.exists() and 'predictions' in dir() and len(predictions) > 0:\n",
    "    video = tifffile.imread(VAL_TIF)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    \n",
    "    gt_available = VAL_CSV.exists()\n",
    "    if gt_available:\n",
    "        gt_df = pd.read_csv(VAL_CSV)\n",
    "        gt_roi = gt_df[\n",
    "            (gt_df.x >= roi['x_min']) & (gt_df.x < roi['x_max']) &\n",
    "            (gt_df.y >= roi['y_min']) & (gt_df.y < roi['y_max'])\n",
    "        ]\n",
    "    \n",
    "    sample_frames = [0, 30, 60, 90]\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    for ax, fidx in zip(axes, sample_frames):\n",
    "        ax.imshow(video_roi[fidx], cmap='gray')\n",
    "        \n",
    "        if gt_available:\n",
    "            frame_gt = gt_roi[gt_roi.frame == fidx]\n",
    "            ax.scatter(frame_gt.x - roi['x_min'], frame_gt.y - roi['y_min'],\n",
    "                      c='lime', s=40, marker='o', facecolors='none', linewidths=1.5, label='GT')\n",
    "        \n",
    "        frame_preds = predictions[predictions.frame == fidx]\n",
    "        ax.scatter(frame_preds.x - roi['x_min'], frame_preds.y - roi['y_min'],\n",
    "                  c='red', s=25, marker='x', linewidths=1.5, label='Pred')\n",
    "        \n",
    "        ax.set_title(f'Frame {fidx}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    axes[0].legend(loc='upper left')\n",
    "    plt.suptitle('Detection: Green=GT, Red=Predicted', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking visualization: color by track_id\n",
    "if 'predictions' in dir() and len(predictions) > 0:\n",
    "    video = tifffile.imread(VAL_TIF)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    \n",
    "    sample_frames = [0, 30, 60, 90]\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    unique_tracks = predictions['track_id'].unique()\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_tracks)))\n",
    "    track_colors = {t: colors[i % len(colors)] for i, t in enumerate(unique_tracks)}\n",
    "    \n",
    "    for ax, fidx in zip(axes, sample_frames):\n",
    "        ax.imshow(video_roi[fidx], cmap='gray')\n",
    "        frame_preds = predictions[predictions.frame == fidx]\n",
    "        for _, row in frame_preds.iterrows():\n",
    "            ax.scatter(row['x'] - roi['x_min'], row['y'] - roi['y_min'],\n",
    "                      c=[track_colors[row['track_id']]], s=30, marker='o')\n",
    "        ax.set_title(f'Frame {fidx} ({len(frame_preds)} cells)')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Tracking: color = track ID', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
