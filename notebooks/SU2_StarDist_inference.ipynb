{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/veselm73/SU2/blob/main/notebooks/SU2_StarDist_inference.ipynb)\n\n# Cell Detection & Tracking - Competition Inference\n\n**Author:** Mateusz Vesel  \n**Task:** Detect and track cells in microscopy video\n\n---\n\n## Pipeline Overview\n\n### 1. Detection: StarDist with 5-Fold Ensemble\n- **Architecture:** StarDist with ResNet18 encoder\n- **Training:** 5-Fold stratified cross-validation on 120 annotated frames + bonus data\n- **Inference:** Ensemble averaging of probability maps from all 5 folds\n- **Post-processing:** Non-maximum suppression to extract cell centroids\n\n### 2. Tracking: LapTrack\n- **Method:** Linear Assignment Problem (LAP) based tracking\n- **Features:** Frame-to-frame linking with gap closing for missed detections\n\n---\n\n## Available Pre-trained Models\n\n| Model | Epochs | Augmentation | N_Rays | DetA |\n|-------|--------|--------------|--------|------|\n| `100e_noaug_32rays` | 100 | No | 32 | **0.9162** (best) |\n| `100e_noaug_64rays` | 100 | No | 64 | 0.9060 |\n| `120e_aug_32rays` | 120 | Yes | 32 | 0.8423 |\n\n---\n\n## How to Use This Notebook\n\n1. **Select a model** in the configuration cell below\n2. **Run all cells** - weights auto-download from GitHub if needed\n3. Results are evaluated against ground truth and visualized"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (fast with uv)\n",
    "!pip install uv -q\n",
    "!uv pip uninstall torch torchvision torchaudio --system -q 2>/dev/null || true\n",
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --system -q\n",
    "!uv pip install \"numpy<2\" cellseg-models-pytorch pytorch-lightning laptrack tifffile --system -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone/update repository\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not Path('/content/SU2').exists():\n",
    "        !git clone https://github.com/veselm73/SU2.git /content/SU2\n",
    "    else:\n",
    "        !cd /content/SU2 && git pull\n",
    "    os.chdir('/content/SU2')\n",
    "    repo_root = Path('/content/SU2')\n",
    "else:\n",
    "    notebook_dir = Path(os.getcwd())\n",
    "    repo_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"Repository: {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "from cellseg_models_pytorch.postproc.functional.stardist.stardist import post_proc_stardist\n",
    "\n",
    "# Import tracking and metrics\n",
    "from modules.stardist_helpers import (\n",
    "    run_laptrack,\n",
    "    hota,\n",
    "    ROI_X_MIN, ROI_X_MAX, ROI_Y_MIN, ROI_Y_MAX\n",
    ")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## 2. Model Selection & Configuration\n\n**Choose your model below.** Weights will be auto-downloaded from GitHub if not found locally."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# MODEL SELECTION\n# ============================================================\n# Available pre-trained models:\n#   - \"100e_noaug_32rays\": 100 epochs, no aug, 32 rays (DetA=0.9162) - BEST\n#   - \"100e_noaug_64rays\": 100 epochs, no aug, 64 rays (DetA=0.9060)\n#   - \"120e_aug_32rays\":   120 epochs, with aug, 32 rays (DetA=0.8423)\n\nAVAILABLE_MODELS = {\n    \"100e_noaug_32rays\": \"100 epochs, no aug, 32 rays (DetA=0.9162) - BEST\",\n    \"100e_noaug_64rays\": \"100 epochs, no aug, 64 rays (DetA=0.9060)\",\n    \"120e_aug_32rays\": \"120 epochs, with aug, 32 rays (DetA=0.8423)\"\n}\n\n# >>> CHANGE THIS TO SELECT MODEL <<<\nSELECTED_MODEL = \"100e_noaug_32rays\"\n\nprint(\"Available models:\")\nfor name, desc in AVAILABLE_MODELS.items():\n    marker = \">>>\" if name == SELECTED_MODEL else \"   \"\n    print(f\"  {marker} {name}: {desc}\")\nprint(f\"\\nSelected: {SELECTED_MODEL}\")"
  },
  {
   "cell_type": "code",
   "id": "wh9oc1ybphg",
   "source": "import urllib.request\nimport json\n\ndef download_weights_if_needed(model_name, repo_root):\n    \"\"\"Download model weights from GitHub if not present locally.\"\"\"\n    weights_dir = repo_root / \"weights\" / model_name\n    models_dir = weights_dir / \"models\"\n    \n    # Check if weights already exist\n    if models_dir.exists() and len(list(models_dir.glob(\"*.pth\"))) == 5:\n        print(f\"Weights found locally: {weights_dir}\")\n        return weights_dir\n    \n    print(f\"Downloading weights for '{model_name}' from GitHub...\")\n    weights_dir.mkdir(parents=True, exist_ok=True)\n    models_dir.mkdir(parents=True, exist_ok=True)\n    \n    base_url = f\"https://raw.githubusercontent.com/veselm73/SU2/main/weights/{model_name}\"\n    \n    # Download config files\n    for cfg in [\"model_config.json\", \"inference_config.json\"]:\n        url = f\"{base_url}/{cfg}\"\n        dest = weights_dir / cfg\n        print(f\"  Downloading {cfg}...\")\n        urllib.request.urlretrieve(url, dest)\n    \n    # Download fold weights (5 folds, ~53MB each)\n    for fold in range(1, 6):\n        url = f\"{base_url}/models/fold_{fold}.pth\"\n        dest = models_dir / f\"fold_{fold}.pth\"\n        print(f\"  Downloading fold_{fold}.pth (~53MB)...\")\n        urllib.request.urlretrieve(url, dest)\n    \n    print(\"Download complete!\")\n    return weights_dir\n\n\n# Download weights if needed\nWEIGHTS_DIR = download_weights_if_needed(SELECTED_MODEL, repo_root)\n\n# Load configuration from JSON files\nwith open(WEIGHTS_DIR / \"model_config.json\") as f:\n    MODEL_CONFIG = json.load(f)\n\nwith open(WEIGHTS_DIR / \"inference_config.json\") as f:\n    INFERENCE_CONFIG = json.load(f)\n\n# Add ROI from helpers if not in config\nif 'roi' not in INFERENCE_CONFIG:\n    INFERENCE_CONFIG['roi'] = {\n        'x_min': ROI_X_MIN, 'x_max': ROI_X_MAX,\n        'y_min': ROI_Y_MIN, 'y_max': ROI_Y_MAX\n    }\n\n# Set paths\nMODELS_DIR = WEIGHTS_DIR / \"models\"\nVAL_TIF = repo_root / \"data\" / \"val\" / \"val.tif\"\nVAL_CSV = repo_root / \"data\" / \"val\" / \"val.csv\"\nK_FOLDS = 5\n\nprint(\"\\nConfiguration loaded:\")\nprint(f\"  Model: {MODEL_CONFIG['encoder_name']}, n_rays={MODEL_CONFIG['n_rays']}\")\nprint(f\"  Detection: prob_thresh={INFERENCE_CONFIG['prob_thresh']}, nms_thresh={INFERENCE_CONFIG['nms_thresh']}\")\nif 'track_max_dist' in INFERENCE_CONFIG:\n    print(f\"  Tracking: max_dist={INFERENCE_CONFIG['track_max_dist']}px\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from cellseg_models_pytorch.models.stardist.stardist import StarDist\n",
    "import torch.nn as nn\n",
    "\n",
    "class StarDistLightning(pl.LightningModule):\n",
    "    \"\"\"StarDist model wrapper for inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_rays=32, encoder_name=\"resnet18\", dropout=0.0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_rays = n_rays\n",
    "        wrapper = StarDist(\n",
    "            n_nuc_classes=1,\n",
    "            n_rays=n_rays,\n",
    "            enc_name=encoder_name,\n",
    "            model_kwargs={\"encoder_kws\": {\"in_chans\": 1}}\n",
    "        )\n",
    "        self.model = wrapper.model\n",
    "        self.dropout = nn.Dropout2d(p=dropout) if dropout > 0 else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def load_fold_model(fold_path, config):\n",
    "    \"\"\"Load a single fold model from .pth file.\"\"\"\n",
    "    model = StarDistLightning(\n",
    "        n_rays=config['n_rays'],\n",
    "        encoder_name=config['encoder_name'],\n",
    "        dropout=config.get('dropout', 0.0)\n",
    "    )\n",
    "    state_dict = torch.load(fold_path, map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 5-fold ensemble\n",
    "print(f\"Loading models from: {MODELS_DIR}\")\n",
    "\n",
    "fold_models = []\n",
    "for fold in range(1, K_FOLDS + 1):\n",
    "    fold_path = MODELS_DIR / f\"fold_{fold}.pth\"\n",
    "    if fold_path.exists():\n",
    "        model = load_fold_model(fold_path, MODEL_CONFIG)\n",
    "        model = model.to(DEVICE)\n",
    "        fold_models.append(model)\n",
    "        print(f\"  ✓ Fold {fold}\")\n",
    "    else:\n",
    "        print(f\"  ✗ Fold {fold}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nLoaded {len(fold_models)}/{K_FOLDS} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "def preprocess_frame(frame):\n    \"\"\"Percentile normalization.\"\"\"\n    frame = frame.astype(np.float32)\n    p1, p99 = np.percentile(frame, (1, 99.8))\n    frame = np.clip(frame, p1, p99)\n    frame = (frame - p1) / (p99 - p1 + 1e-8)\n    return frame\n\n\ndef detect_cells_ensemble(models, frame, prob_thresh, nms_thresh, device):\n    \"\"\"Detect cells using ensemble (average probability maps before thresholding).\"\"\"\n    x = torch.from_numpy(frame).float().unsqueeze(0).unsqueeze(0).to(device)\n    \n    all_stardist = []\n    all_prob = []\n    \n    with torch.no_grad():\n        for model in models:\n            out = model(x)\n            nuc_out = out['nuc']\n            stardist_map = nuc_out.aux_map.cpu().numpy()[0]\n            prob_map = torch.sigmoid(nuc_out.binary_map).cpu().numpy()[0, 0]\n            all_stardist.append(stardist_map)\n            all_prob.append(prob_map)\n    \n    avg_stardist = np.mean(all_stardist, axis=0)\n    avg_prob = np.mean(all_prob, axis=0)\n    \n    try:\n        labels = post_proc_stardist(\n            avg_prob, avg_stardist,\n            score_thresh=prob_thresh,\n            iou_thresh=nms_thresh\n        )\n        detections = [(prop.centroid[1], prop.centroid[0]) for prop in regionprops(labels)]\n        return detections\n    except:\n        return []\n\n\ndef infer_video(video_path, config, models=None):\n    \"\"\"Run detection + tracking on video. Returns DataFrame with frame, x, y, track_id.\"\"\"\n    if models is None:\n        models = fold_models\n    \n    video = tifffile.imread(video_path)\n    roi = config['roi']\n    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n    \n    print(f\"Video: {video.shape} -> ROI: {video_roi.shape}\")\n    print(f\"Running ensemble detection ({len(models)} models)...\")\n    \n    all_detections = []\n    for frame_idx in tqdm(range(len(video_roi))):\n        frame = preprocess_frame(video_roi[frame_idx])\n        detections = detect_cells_ensemble(\n            models, frame,\n            config['prob_thresh'], config['nms_thresh'], DEVICE\n        )\n        for x, y in detections:\n            all_detections.append({\n                'frame': frame_idx,\n                'x': x + roi['x_min'],\n                'y': y + roi['y_min']\n            })\n    \n    detections_df = pd.DataFrame(all_detections)\n    print(f\"Detections: {len(detections_df)}\")\n    \n    # Handle different tracking config formats\n    if 'track_max_dist' in config:\n        max_dist = config['track_max_dist']\n        closing_gap = config.get('gap_closing_frames', 2)\n    elif 'track_cost_cutoff' in config:\n        # Convert squared distance to distance\n        max_dist = int(np.sqrt(config['track_cost_cutoff']))\n        closing_gap = config.get('gap_closing_max_frame_count', 1)\n    else:\n        # Default values\n        max_dist = 5\n        closing_gap = 2\n    \n    print(f\"Running tracking (max_dist={max_dist}px, gap={closing_gap})...\")\n    tracked_df = run_laptrack(\n        detections_df,\n        max_dist=max_dist,\n        closing_gap=closing_gap,\n        min_length=2\n    )\n    print(f\"Tracks: {tracked_df['track_id'].nunique()}\")\n    \n    return tracked_df"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Run Inference on Validation Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "if len(fold_models) > 0 and VAL_TIF.exists():\n    predictions = infer_video(VAL_TIF, INFERENCE_CONFIG)\nelse:\n    print(\"Models not loaded or validation video not found.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Evaluation (HOTA Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAL_CSV.exists() and 'predictions' in dir() and len(predictions) > 0:\n",
    "    # Load ground truth\n",
    "    gt_df = pd.read_csv(VAL_CSV)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    gt_roi = gt_df[\n",
    "        (gt_df.x >= roi['x_min']) & (gt_df.x < roi['x_max']) &\n",
    "        (gt_df.y >= roi['y_min']) & (gt_df.y < roi['y_max'])\n",
    "    ].copy()\n",
    "    \n",
    "    # Calculate HOTA\n",
    "    hota_scores = hota(gt_roi, predictions, threshold=5.0)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\n  HOTA: {hota_scores['HOTA']:.4f}\")\n",
    "    print(f\"  DetA: {hota_scores['DetA']:.4f}\")\n",
    "    print(f\"  AssA: {hota_scores['AssA']:.4f}\")\n",
    "    print(f\"\\n  Detections: {len(predictions)}\")\n",
    "    print(f\"  Tracks: {predictions['track_id'].nunique()}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection visualization: GT vs Predictions\n",
    "if VAL_TIF.exists() and 'predictions' in dir() and len(predictions) > 0:\n",
    "    video = tifffile.imread(VAL_TIF)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    \n",
    "    gt_available = VAL_CSV.exists()\n",
    "    if gt_available:\n",
    "        gt_df = pd.read_csv(VAL_CSV)\n",
    "        gt_roi = gt_df[\n",
    "            (gt_df.x >= roi['x_min']) & (gt_df.x < roi['x_max']) &\n",
    "            (gt_df.y >= roi['y_min']) & (gt_df.y < roi['y_max'])\n",
    "        ]\n",
    "    \n",
    "    sample_frames = [0, 30, 60, 90]\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    for ax, fidx in zip(axes, sample_frames):\n",
    "        ax.imshow(video_roi[fidx], cmap='gray')\n",
    "        \n",
    "        if gt_available:\n",
    "            frame_gt = gt_roi[gt_roi.frame == fidx]\n",
    "            ax.scatter(frame_gt.x - roi['x_min'], frame_gt.y - roi['y_min'],\n",
    "                      c='lime', s=40, marker='o', facecolors='none', linewidths=1.5, label='GT')\n",
    "        \n",
    "        frame_preds = predictions[predictions.frame == fidx]\n",
    "        ax.scatter(frame_preds.x - roi['x_min'], frame_preds.y - roi['y_min'],\n",
    "                  c='red', s=25, marker='x', linewidths=1.5, label='Pred')\n",
    "        \n",
    "        ax.set_title(f'Frame {fidx}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    axes[0].legend(loc='upper left')\n",
    "    plt.suptitle('Detection: Green=GT, Red=Predicted', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking visualization: color by track_id\n",
    "if 'predictions' in dir() and len(predictions) > 0:\n",
    "    video = tifffile.imread(VAL_TIF)\n",
    "    roi = INFERENCE_CONFIG['roi']\n",
    "    video_roi = video[:, roi['y_min']:roi['y_max'], roi['x_min']:roi['x_max']]\n",
    "    \n",
    "    sample_frames = [0, 30, 60, 90]\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    unique_tracks = predictions['track_id'].unique()\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_tracks)))\n",
    "    track_colors = {t: colors[i % len(colors)] for i, t in enumerate(unique_tracks)}\n",
    "    \n",
    "    for ax, fidx in zip(axes, sample_frames):\n",
    "        ax.imshow(video_roi[fidx], cmap='gray')\n",
    "        frame_preds = predictions[predictions.frame == fidx]\n",
    "        for _, row in frame_preds.iterrows():\n",
    "            ax.scatter(row['x'] - roi['x_min'], row['y'] - roi['y_min'],\n",
    "                      c=[track_colors[row['track_id']]], s=30, marker='o')\n",
    "        ax.set_title(f'Frame {fidx} ({len(frame_preds)} cells)')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Tracking: color = track ID', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}